\documentclass{style/llncs}

\usepackage{amsmath,amsfonts}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[mathscr]{eucal}

\newcommand{\M}{\mathscr M}
\newcommand{\C}{\mathscr C}
\newcommand{\T}{\mathscr T}
\newcommand{\F}{\mathscr F}
\renewcommand{\P}{\mathscr P}
\newcommand{\K}{\mathscr K}
\newcommand{\X}{\mathscr X}
\newcommand{\B}{\mathbb B}
\newcommand{\D}{\Delta}
\newcommand{\N}{\mathbb N}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\pair}[1]{\left\langle{#1}\right\rangle}
\newcommand{\concat}{\oplus}
\newcommand{\symb}[1]{\texttt{#1}}
\newcommand{\br}[1]{\overline{#1}}
\newcommand{\s}{S}
\newcommand{\dom}[1]{\mathop{\tn{dom}(#1)}}

\newtheorem{conj}{Conjecture}

\let\doendproof\endproof
\renewcommand\endproof{~\hfill\qed\doendproof}

\newcommand{\p}{\,\text{.}}

\newcommand{\tuple}[1]{\left\langle{#1}\right\rangle}

\newcommand{\hide}[1]{}

\newcommand{\sdr}[1]{\textcolor{blue}{\small #1\textsuperscript{[Steven]} }}
\newcommand{\pb}[1]{\textcolor{OliveGreen}{\small #1 \textsuperscript{[Peter]} }}

\newcommand{\argmin}{\mathop{\arg\min}}

% --- DELETE BEFORE SUBMISSIONS ---
\pagestyle{headings} 

\title{Three Problems for Sophistication}

\author{Peter Bloem and Steven de Rooij}

\institute{
  System and Network Engineering Group, \\University of Amsterdam, the Netherlands\\
  \email{uva@peterbloem.nl, steven.de.rooij@gmail.com}
}

\begin{document}
\maketitle

\begin{abstract}
Kolmogorov complexity is a sound measure of the amount of information in data, but it does not distinguish structural information from random noise. Kolmogorov's definition of the \emph{structure function} was the first attempt to capture this separation. Since then many variations of this idea have been proposed, for which we use \emph{sophistication} as an umbrella term. The multitude of treatments indicates that the idea is considered worth pursuing by multiple distinguished scholars, but at the same time it shows that the issue has never been satisfactorily resolved. We describe three fundamental problems with all existing proposals, showing each of them to be \emph{unsound}. Based on these results we we put forward the view that the problem is fundamental: while many nontrivial and useful structural properties of binary strings can be detected without a priori assumptions, it may be impossible to objectively quantify the sophistication.
\end{abstract}

\section{Introduction}

\sdr{Course sophistication handled sufficiently?}
\sdr{Do we really need the $\gamma$-invariance? Or can $\gamma$ be 1?}
\sdr{We need the definitions! But maybe we can simplify, do we really need the candidate set?}

Kolmogorov complexity breaks away from the tradition in statistics and information theory by allowing the analysis of \emph{individual} binary sequences rather than studying large samples from probability distributions in an assumed class. This is important because for many binary strings, such as a binary encoding of a novel, or a holiday snapshot, or the fossil record, it is impossible to determine what distribution generates such objects. Kolmogorov complexity is often seen as the first step to a statistics without assumptions.

There is, however, a nagging discrepancy between Kolmogorov complexity and what most people would call ``complex''. For example, a sequence of a million coin flips is exceedingly unlikely to have a Kolmogorov complexity of much less than one million bits, even though there is nothing complex about flipping a lot of coins---it's just hard work. For this reason, many scholars have defined alternative complexity measures in the spirit of Kolmogorov complexity, aimed at quantifying not \emph{all} information in a binary string, but only the \emph{meaningful} information in the data. This tradition started with a lecture by Kolmogorov in Talinn in 1973, but many alternative definitions of meaningful information, or \emph{sophistication},\footnotemark were proposed, see the related work section below. In this paper, we investigate three problems with these definitions, and show that they are unsatisfactory. We believe the problem is fundamental, and provide arguments why such attempts to define sophistication must fail.

\footnotetext{Most proposals coin their own name along with a specific treatment. For the sake of clarity, we have chosen to use \emph{sophistication} as an umbrella term to refer to all ideas that fit this mold.}

The Kolmogorov complexity of a binary string $x$, denoted $K(x)$, is (roughly) the length of the shortest computer program that will output $x$. This length depends on which programming language is used, but, as shown by a core result known as the Invariance Theorem, the language affects the result only by a constant, independent of $x$. This means that for sufficiently complex objects, the choice of programming language becomes irrelevant and Kolmogorov complexity becomes an \emph{objective} measure of the complexity of binary strings. We can see it as \emph{a property of the data}.

A definition of sophistication $S(x)$ in the spirit of Kolmogorov complexity should have similar robustness guarantees if it is to be interpreted as an objective property of a binary string. We are interested in a measure satisfying the following requirements:

\begin{enumerate}
\item Since it measures information, the sophistication should count the number of bits required for an effective description of the structural properties of a binary string.
\item An analogue of the Invariance Theorem should hold for sophistication: at the very least, there should be strict limits on the degree to which a change in the programming language can affect the sophistication.
\item There should be no constant $c$ such that $S(x)\le c$ for every input $x$. If sophistication is bounded, then knowing its value under one programming language provides no constraints on its value under another language, even for inputs of very high Kolmogorov complexity. Moreover, a bounded sophistication would be at odds with the intuition that there is no limit to the amount of structure that a binary string can exhibit.
\item Similarly, there should be no constant $c$ such that $K(x)-S(x)\le c$ for all $x$, because then sophistication would be equivalent to Kolmogorov complexity. 
\end{enumerate}

For most proposed definitions of sophistication, we can prove that they fail one or more of the conditions above. Vit\'anyi's definition \cite{vitanyi2004meaningful} is the one exception that we cannot \emph{prove} to conflict with our requirements, but we can show that only extremely deep strings (i.e., strings that require an enormous amount of processing to construct) yield a large sophistication, while intuitively it seems there must be objects that contain a lot of structure all of which can be described without much calculation. Moreover in our discussion section we provide arguments that suggest that it should be possible to generalise our negative results to show that Vit\'anyi's sophistication is not invariant.

A valid definition of sophistication must contend with the following issues. First, while all definitions are based on a two-part encoding of the data sequence, some researchers use Kolmogorov complexity as a \emph{building block} of their proposed code, while others look for the two parts of the code \emph{inside} the Kolmogorov complexity: when the definition of Kolmogorov complexity is opened up, it turns out to consist of two parts itself. In Section~\ref{section:indices}, we show how this second approach sets us up to suffer from to the so-called \emph{nickname problem}: the sophistication becomes highly dependent on the chosen programming language. This immediately breaks invariance, unless additional requirements are made on the language. We also show that there do exist languages (efficient enumerations of the partial recursive functions) that avoid this issue.

The second issue is that of \emph{overfitting}, which we consider in Section~\ref{section:overfitting}. Overfitting is a common problem in the statistical literature, that refers to the tendency of some model selection procedures to select a very complex model that provides a very good fit to the observed data, but does not generalise well to unseen data. For example, in a polynomial regression setting, a higher order polynomial can always provide better fit to a data set. Therefore, even if the data are in fact noisy samples from a parabola, naive model selection methods may select a much higher order polynomial that provides a worse description of the trend in the data, encoding all noise in the model parameters. The same can occur for the sophistication. We somehow need to make sure that the two-part representation that determines the sophistication does not store noise in the ``structure'' part of the code. In statistics, this is typically addressed by penalising complex models. However, in the sophistication setting, such penalties tend to break the delicate balance between structural information and noise, and lead to the opposite problem: underfitting.

Underfitting (Section~\ref{section:underfitting}) is the opposite problem, where the selected model is simple, but fails to represent part of the structure in the data. It is an especially serious problem for sophistication because the models for used in sophistication are so powerful. In particular, in any programming language, there are programs that implement an interpreter for \emph{another} language. Such interpreters, or \emph{universal models}, can be described in a constant amount of bits, and can generate the data with an input of the  the length of the Kolmogorov complexity, essentially encoding all information as noise. If complex models are penalized to avoid overfitting, then the problem becomes to make sure that simple universal models are not \emph{always} preferred for complex data. The usual workaround is to restrict the set of allowed models to exclude the universal ones. For example, the model class may be restricted to the set of all total recursive functions (only programs that halt for all inputs) rather than all partial recursive functions (all programs). While this patches the problem with universal models, it is questionable whether it adequately solves the problem of underfitting in general.

Finally, in the discussion in Section~\ref{section:conclusion} we argue that while two-part coding can yield useful insights into the structure of the data and allows us to identify some models as unlikely to capture all structure in the data, it is probably not possible to uniquely separate structure from noise and identify a \emph{single} model as ``best'': in general many models, of vastly different complexities, may be reasonable models of the data generating mechanism. Rather than doggedly trying to ``fix'' this property of algorithmic statistics, we propose embracing the idea that sometimes the data allows for multiple interpretations, and that there is no such thing as sophistication.

\subsection{Related work}
The idea of separating structure from noise originated with Kolmogorov's lecture in Tallinn, Estonia in 1973 introducing the \emph{structure function} \cite{cover1985kolmogorov}. In this theory, the two-part description of the data consists of a program that generates a finite set, and an index of the original object in the set. The length of the program then measures the structure in the data. This approach was discussed more extensively, and generalised, by Vereshchagin, G\'acs and Vit\'anyi \cite{vereshchagin2004kolmogorov,gacs2001algorithmic}. 

Koppel \cite{koppelSoph1988,koppel1991almost} also built on Kolmogorov and Cover's work to define a measure of the meaningful information in a binary string, called \emph{sophistication}, the term we adopt in this paper. Koppel changed the class of models from finite sets to total functions and used monotone functions, instead of prefix-free Kolmogorov complexity, to be able to extend the theory to infinite strings, an approach which has been abandoned by subsequent authors. We will only consider the finite version (based on regular functions) in this paper.

In 1996 Gell-Mann and Lloyd introduced \emph{effective complexity} \cite{gellmann1996information}. Working from the perspective of statistical mechanics, they arrived (largely independently, it seems) at a notion that fits the mold of sophistication.

Antunes et al. \cite{antunes2009sophistication} note that the constant cutoff used by Koppel to separate candidate two-part descriptions from those deemed to be too inefficient, is a source of instability, a result in the spirit of this paper. They propose a different definition called \emph{coarse sophistication}. As we shall see, this version behaves very differently from regular sophistication, and is closer to computational depth. Vereshchagin \cite{vereshchagin2013algorithmic} proposes an alternative solution to the instability issue. \sdr{But if his stuff works, this paper is nonsense, so we should say more about it!}
 
Adriaans' \emph{Facticity}, introduced in 2012\cite{adriaans2012facticity}, relaxes the model class to include all partial recursive functions. Adriaans also identifies the nickname problem discussed in Section~\ref{section:indices}.

\subsection{Preliminaries, definitions and desiderata}

Let $\B = \{0,1\}^*$. We deal with partial functions $\phi: \B \to \B$, which we will also call \emph{models}. Such functions are \emph{computable} if they can be computed by a Turing machine as described in \cite[Definition~1.7.1]{li1993introduction}. A function is prefix-free if its domain, $\dom{f} = \{y : f(y) \neq \infty\}$ is a prefix free set. A function $f$ is \emph{total} if $\dom{f} = \B$. A prefix-free function is \emph{complete} if every infinite binary string has a member of $\dom{f}$ as a prefix.\footnotemark 

\sdr{I think this footnote should not be a footnote and better explained. It's important}

\footnotetext{Completeness is analogous to totality: a total function corresponds to a Turing machine that halts for all its inputs, while a complete function corresponds to a Turing machine that always halts, so long as there are enough bits on the input tape.}

\begin{definition}[Numberings]
A numbering $\psi_1,\psi_2,\ldots$ is an enumeration of of the partial computable functions. We fix one canonical numbering $\phi$ that we know to be effective, that is for any $i \in \N, y\in \B$, we can effectively compute $\phi_i(y)$, where the computation diverges if $\phi_i(y) = \infty$. We call a numbering $\psi$ \emph{acceptable} if there exist total, computable functions $a, b: \N \to \N$ such that for all $i$, $\phi_i = \psi_{b(i)}$ and  $\psi_i = \phi_{a(i)}$.  We abbreviate the sequence by its symbol without the index: $\phi=\phi_1,\phi_2,\ldots$
\end{definition}

\begin{definition}[Prefix encoding function]
  A prefix encoding function $f:\B\to\B$ is an injective
  function that maps the binary strings to a prefix-free set. There
  are prefix encoding functions $g$ with the property that
  $|g(x)|\le|x|+2\log(|x|+2)|$ \cite{li1993introduction}. We fix such a function and
  denote it using a bar, $\br{x}=g(x)$.
\end{definition}

\sdr{Clearly explain what it means if a function function is total, complete, prefix. Do we need to fuss about equating the numbers and binary strings? Perhaps it's better to do the conversions explicitly in this paper}

\begin{definition}[Complexity]\belowdisplayskip=-12pt
Let $\Pi$ be a computable subset of the natural numbers such that  $\{\phi_i:i\in\Pi\}$ is the set of all computable prefix functions. \sdr{Cite textbook that this is possible.} We define the plain and prefix Kolmogorov complexity as 
\begin{align*}
C(x)&=\min\{|\bar\imath y|:\phi_i(y)=x\},\\
K(x)&=\min\{|\bar\imath y|:\phi_i(y)=x, i\in\Pi\}.\\
\end{align*}\label{definition:complexity}\vspace{-18pt}
\end{definition}
This definition of $K$ is uncommon. It is equivalent to the standard definition in \cite{li1993introduction}, but this form is more convenient for defining the $K$-complexity of non-prefix functions (see Definition~\ref{definition:model-complexity} below).

We will generalize the main proposals for sophistication into a single notation. We start with a class of models, we consider all two-part descriptions of the data using these models, we select the set of representations with the shortest code-lengths and we select the smallest model from this set. The size of the smallest model represents the sophistication of the data.

\begin{definition}[Models and model classes] A \emph{model class} is a set of models. We distinguish five important model classes:
  \begin{itemize}
  \item $\C$ is the set of all partial computable functions,
  \item $\K\subset\C$ is the set of all prefix-free
    functions,
  \item $\T\subset\C$ is the set of all total functions,
  \item $\P\subset\K$ is the set of all complete, prefix-free functions,
  \item $\F\subset\P$ contains, for every finite set $S$, a computable
    surjective function mapping the binary sequences of length
    $\lceil\log S\rceil$ onto $S$. In other words, this is the model class of finite sets.
  \end{itemize}
\end{definition}
We note that classes $\T$ and $\P$ are unusual in that they are not enumerable.
\begin{definition}[Representations and code-lengths]
A pair $(m,y)$ is a \emph{representation} for $x$ if $m$ is a model and $m(y)=x$.
Define the following code-length functions for the representations:
\begin{itemize}
\item $L(m)$ is the length of a self-delimiting description of
  the model $m$, i.e. it is the length function of a prefix code for models. 
\item $L^m(x)=L(m)+\min\{|y|:m(y)=x\}$ measures the number of bits required to describe both the model $m$ and the data $x$ using the model.
\item $L^\M(x)=\min\{L^m(x):m\in\M\}$ measures the number of bits
  required to describe $x$ using the best model in the model class.
\end{itemize}
\end{definition}
We can now define which of these representations express the data most efficiently:
\begin{definition}[Candidate set]
The \emph{candidate models} for $x$ from model class $\M$ and threshold function $\tau$ are
\[
  C^\M_\tau(x):=\{m\in\M:L^m(x)\le \tau(x) + c\}
\]
with $c$ a constant independent of $x$ which may be chosen arbitrarily. The default threshold function is $\tau^\circ(x)=L^\M(x)$. We will often omit the subscript if the default threshold function is used. 
\end{definition}

\begin{definition}[Sophistication]\belowdisplayskip=-12pt
  The model complexity, meaningful information, effective complexity, facticity or \emph{sophistication},  of a string $x$ given a model class $\M$ and threshold function $\tau$ is
  \[
  \s_\tau^{\M}(x):=\min\{L(m):m\in C^\M_\tau(x)\}\p
  \]\label{definition:sophistication}
\end{definition}
Finally, we must formalize what we mean when we say that we expect sophistication to be invariant. It may be that a less strict invariance than we have for the Kolmogorov complexity, still yields a useful sophistication.
\begin{definition}
Let $\gamma:\N\to \N$ be some nondecreasing function. Two functions $f$ and $g$ are $\gamma$-equivalent if $|f(x) - g(x)| \in O(\gamma(\max(f(x), g(x)))$.
\end{definition}
We say that a formalization of sophistication $\s_\phi$ built on acceptable numbering $\phi$ is $\gamma$-invariant to the choice of numbering is $s_\phi$ and $s_\psi$ are $\gamma$-equal for any two acceptable numberings $\phi$ and $\psi$. 
\begin{lemma}\label{lem:broken}
Given a slack function $\gamma$, if $\s_\phi(x)$ is either $\gamma$-equivalent to $0$ or to $K(x)$ for \emph{some} numbering $\phi$, then either this equivalence exists for all other numberings as well, or $\s_\phi(x)$ is not $\gamma$-invariant.
\end{lemma}
The proof follows directly from the definitions. Most lemmas in the rest of the paper will simply prove that a given variant is bounded for a specific numbering, implicitly using this lemma to show that the variant is unsound.

\section{Inefficient indices}
\label{section:indices}
We start from a simple intuition: the definition of Kolmogorov complexity already uses a description of the data in two parts; the index of a computable function and an input to that function. Can we assume that, when such a description is as short as the Kolmogorov complexity, it automatically creates a good separation into the structural part of the data and the noise? As we will show, this depends in part on the used enumeration $\phi$. 

The issue is subtle and require a careful attention that is rarely given in the literature. We will first illustrate what happens if any acceptable numbering is allowed:

\begin{lemma}
Any sophistication $\s_\phi(x)$ that gives the length of some index $i$ such that $\phi_i(y) = x$ for some $y$ cannot be both $\gamma$-invariant and unbounded for any $\gamma(n) \in o(n)$.
\end{lemma}
\begin{proof}
Choose some $\gamma(n) \in o(n)$. Assume towards contradiction that the sophistication is $\gamma$-invariant and unbounded.

Let $\phi$ be the canonical acceptable numbering. Let $s_i$ be the binary string consisting of $2^{i}$ zeroes followed by a one.

We now define two new acceptable numberings $\psi$ and $\chi$:
\[
\psi_j(x) = \begin{cases}
	\phi_i(x) \;\text{if}\; j = s_{2i}\\
	\infty\;\text{otherwise}
\end{cases}\,\,\,
\xi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s_{2i+1} \\
	\infty \;\text{otherwise}\p
\end{cases}
\]
Now choose any $x$ and assume w.l.o.g. that $\s_\psi(x) < \s_\xi(x)$. By construction of the numberings $\psi$ and $\xi$, we have $2\;\s_\psi(x)  \leq \s_\xi(x)$, which gives us:
\begin{align*}
	\s_\xi(x) - \s_\psi(x) &\geq \frac{1}{2}\, \s_\xi(x)  \in \Omega(\s_\xi(x))\p 
\end{align*}
Since, by assumption, $\s_\xi$ can be arbitrarily large and $\s_\xi(x) - \s_\psi(x) \in O(\gamma(\s_\xi(x))) \subset o(\s_\xi(x))$, we have a contradiction.
\end{proof}
In this proof, we deliberately used ridiculously inefficient index functions. Obviously, one could simply pick a more efficient index; the point is that many definitions of sophistication do not mention any such restriction. As we shall see, any inefficiency in the index will lead to a lack of invariance in sophistication.

This is a serious problem with several papers, in particular
Koppel and Atlan's \cite{koppelSoph1988,koppel1991almost} and several of its variants \cite{antunes2009sophistication,antunes2013sophistication}. 

Interestingly, using an inefficient index is no problem in the definition of Kolmogorov complexity, since we can select as a model a universal Turing machine with an efficient index at only a constant penalty. But this robustness does \emph{not} carry over to definitions of sophistication. If we are interested in separating the model and its input, we must represent the model efficiently. 

Simply put, the length of a model in an ordinary description does not represent its complexity, much like the length of a compressible string does not reflect its information content. In order to obtain a more robust measure of model complexity, we describe a natural extension of Kolmogorov complexity to functions, and show that this retains invariance.

\begin{definition}\belowdisplayskip=-12pt
  The complexity of a partial recursive function is defined by
  \begin{align*}
    C_\phi(f) &:= \min\{C(i):\phi_i=f\}\;\tn{,}\\
    K_\phi(f) &:= \min\{K(i):\phi_i=f\}\p
  \end{align*} \label{definition:model-complexity}
\end{definition}
These definitions also appear in \cite{grunwald2004shannon} and \cite{vitanyi2004meaningful}. We will show that these values are invariant up to a constant (ie. complexity is an objective property of a function):
\begin{lemma}[Invariance]
Let $\phi$ and $\psi$ be any two acceptable numberings. There exists a constant $c$ such that $\left| K_\phi(f) - K_\psi(f)\right | \leq c$ for all $f$. \label{lemma:invariance}
\end{lemma}
\begin{proof}
Let $g(i)$ be the function such that $\psi_i=\phi_{g(i)}$.
\begin{align*}
K_\phi(f) &= \min\left\{ K_\phi(i) : \phi_i= f\right\} 
\geq \min\left\{ K_\psi(i) : \phi_i= f\right\} - c\\
&= \min\left\{ K_\psi(i) : \psi_{g(i)}= f\right\} - c
\geq \min\left\{ K_\psi(i) : \psi_i= f\right\} - c' = K_\psi(f).
\end{align*}
The opposite inequality can be achieved by reversing $\phi$ and $\psi$. 
\end{proof}
This lemma shows that the complexity of a function does not depend on the choice of
enumeration by more than a constant term.

There are two ways to use this notion of model complexity for more robust attempts to define sophistication. Confusingly, both approaches are used in the literature,
\begin{enumerate}
  \item We maintain that the representation witnessing the Kolmogorov complexity determines the sophistication, but impose the additional constraint that an efficient, acceptable numbering is used. Specifically, that for every function $f$ there is an $i$ with $\phi_i=f$ and $|i|=C(f)$. We construct such an index below. This approach is taken by Adriaans \cite{adriaans2012facticity}.
  \item On the other hand, rather than distinguishing the two components of the code \emph{within} the definition of Kolmogorov complexity, we can also use Kolmogorov complexity to measure the complexity of the \emph{model} (as in Definition~\ref{definition:model-complexity}), which is then the first part of a two-part code describing the data. With this approach, the total code length is $K(f)+|y|$, where $y$ is an input to the model $f$ such that $f(y)=x$. The difference is that in this approach, we do not need any constraints on the default numbering $\phi$. This approach is used in  \cite{cover1985kolmogorov,gacs2001algorithmic,vitanyi2004meaningful,gellmann1996information}.
\end{enumerate}
To show that the two perspectives are equivalent, we must show that there exists an acceptable numbering which is efficient enough.

\begin{definition}[Faithful Numbering]\label{def:faithful}
  A numbering $\psi$ of the partial recursive
  functions is \emph{faithful} if there is a constant $c$ such that
  for all indices $i$ there is a $j$ such that $\psi_i=\psi_j$ and
  $|j|\le C(\psi_j)+c$.
\end{definition}
This definition is a refinement of the one used in \cite{adriaans2012facticity}.\footnotemark

\footnotetext{That publication defines a faithful \emph{index function}, rather than a numbering, and notes that it is not computable. Since a numbering is all that is needed, we only need to show acceptability. The incomputability of the index function is no issue.}
\begin{lemma}
  There is a faithful acceptable numbering.
\end{lemma}
\begin{proof}
Let $\tn{div} \in \N$ be an index such that $\phi_{\tn{div}}(y)=\infty$ for all $y$. Define
  \[\psi_q=\begin{cases}
    \phi_{\phi_i(p)}&\tn{if $q$ can be written as $\bar\imath p$ and $\phi_i(p)<\infty$,}\\
    \phi_{i_\tn{div}}&\tn{otherwise.}\end{cases}
  \]
  To show that $\psi$ is faithful, pick any function $f$. Then
\[\begin{split}
C(f)&=\min\{C(i):\phi_i=f\} =\min\{\min\{|\bar a b|:\phi_a(b)=i\}:\phi_i=f\} \\
& =\min\{|\bar a b|:\phi_{\phi_a(b)}=f\}
 =\min\{|\bar a b|:\psi_{\bar a b}=f\}.
\end{split}\]
This shows there is a sufficiently small $\psi$ index.

To show that $\psi$ is acceptable, let $\phi_j$ denote the identity
function. Then a $\phi$-index $i$ can be mapped to a $\psi$-index
using the computable function $r(i)=\bar\jmath i$, so that
$\psi_{r(i)}(y)=\psi_{\bar\jmath i}(y)=\phi_i(y)$. For the reverse,
define $\phi_v(\bar\imath p, y)=\phi_{\phi_i(p)}(y)$. For fixed
$\bar\imath p$, the 
$s^n_m$-theorem \cite{kleene193notation} states that we can compute the $h$
such that $\phi_h(y)=\phi_v(\bar\imath p,y)$. Let $h(\bar\imath p)$
denote this index as a function of the program; further define
$h(q)=\tn{div}$ if $q$ cannot be expressed as $\bar\imath p$. By
construction $h$ is total and computable. To check that the mapping
returns the correct function, rewrite $\phi_{h(\bar\imath
  p)}(y)=\phi_v(\bar\imath p,y)=\phi_{\phi_i(p)}(y)=\psi_{\bar\imath p}(y)$.
\end{proof}
\sdr{Explain following better: point out that it provides a slight imbalance in favour of putting stuff in the data part, which, if the model class is $\C$, already to underfitting. Refer to the underfitting section.}
Even with a faithful index, we can fail if the prefix-function used to separate the model from its input is inefficient:
\begin{lemma}
Let $L(\phi_i) = |\bar\imath|$ for a faithful index, and some prefix encoding function, and let $\s^\C$ be defined on this $L$ with the default threshold function. \sdr{Dit wringt als een gek om t zo te zeggen, moet eleganter. Volgens mij gebruiken we maar twee $L$ functies: de index met een bar erop, en $K$.}

If $\min\{i\ge i_0:|\bar\imath|-K(i)\}$ is an unbounded function of $i_0$, then $\s^\C$ is bounded for all numberings.\label{lemma:prefix-inefficiency}
\end{lemma}
\begin{proof}
Let $\bar\imath y$ be any two-part representation for the data $x$, i.e. $\phi_i(y)=x$. Then construct an alternative two-part representation $\bar vi^* y$, where $i^*$ is the shortest $\psi_u$-program for $i$ such that $\phi_v(i^* y)=\phi_{\psi_u(i^*)}(y) = \phi_i(y)=x$. We compare the lengths of these two representations. Note that $|i^*|=K^\psi(i)$. Therefore,
\[
|\bar\imath p|-|\bar v i^* y| = |\bar\imath|-|\bar v| - |i^*| = |\bar\imath|-|\bar v|-K(i).
\]
By assumption there must be an $i_0$ such that the above expression is positive for all $i>i_0$. From this $i_0$ onwards, the second representation (using $v$) will have a shorter code length, so $\bar\imath p$ cannot achieve the minimum in the definition of the sophistication. Consequently, the sophistication is bounded by $\s(x)<|\overline{\imath_0}|$ for all $x$. 
\end{proof}
This result applies to Adriaans' facticity\cite{adriaans2012facticity}. \sdr{I think we should not talk about circumventing this problem, it is too small an imbalance. Rather we should point out from the start that the difference in cost for the model $|\bar\imath|$ instead of $K(i)$ is unbounded, so even a faithful model index is still not an optimal description, and the gap is a direct consequence of the definition of Kolmogorov complexity. And that this slight imbalance can already lead to underfitting. We can then say that the model does not occur if we use the other two-part coding approach, which is why we'll adopt that other perspective in the rest of the paper.} In order to circumvent the problem, the representation should encode the model $f$ in $K(f)$ bits, relying on $K$'s own prefix encoding to delimit the model from its input.

Note that the condition for this lemma applies to all commonly used prefix encoding functions. We conjecture that it applies to all possible computable prefix encoding functions:

\begin{conjecture}
For any partial computable prefix codelength function $L$, the function $\min\{L(i)-K(i):i\ge i_0\}$
is unbounded in $i_0$.
\end{conjecture}

Lemma~\ref{lemma:prefix-inefficiency} shows that even if we have a faithful numbering, we can only use it in settings where we don't count the cost of delimiting the model from the input (such as \cite{koppelSoph1988}). In the cases where the data representations are taken as a single code word, the model-part, including delimiting information, must represent the model as efficiently as $K(f)$ does.  

In the rest of this paper, we will avoid these issues and take the second perspective: we will consider the representations as two-part descriptions $(i, y)$ with codelength $K(\phi_i) + |y|$, such that $\phi_i(y)=x$. 

\section{Overfitting: the singletons}
\label{section:overfitting}
We now come to the main issue plaguing sophistication. \sdr{Beetje bombastisch. En hierna zou ik zeggen, we first consider the simplest, most natural setting, \dots} In the naive setting, where we allow all prefix-free functions as models, and use the default threshold function, the candidate set always contains two representations that clearly do not provide a good separation into structure and content \sdr{noise?}. The first is a \emph{universal model}: a universal Turing machine which simply runs its shortest program for $x$. The second is a \emph{singleton model} a model which returns $x$ for any input.

The key problem is that both can be present in the candidate set, and the formalization of sophistication must ensure that neither are selected \sdr{well occasionally they may be selected\dots should be more precise here}. As we will show, in many settings, we can ensure that they are the \emph{only} models in the candidate set. The following lemma generalizes the principle.

\begin{lemma}\label{lemma:thecoolone}
  Let $\psi$ be an acceptable enumeration of the partial recursive functions.
  Let $\M$ be any model class, let $\X$ be any set of binary sequences and let $D:\B\to\N$ be a computable decoding function with a prefix-free domain that maps function descriptions to their indices in $\psi$, i.e. if $f=\psi_{D(p)}$ then $p$ is a $D$-description of $f$. Let $\M'=\{\psi_i:i\in\tn{range}(D)\}$. Further assume there is a constant $c$ such that
\begin{enumerate}
  \item $\forall_{f\in\M'}:\min\{|p|:\psi_{D(p)}=f\}\le K^\psi(f)+c$
  \item $\forall_{x\in\X}:L^{\M',\psi}(x)-L^{\M,\psi}(x)\le c$.
\end{enumerate}
Then there is an enumeration $\phi$ of the partial recursive functions such that $S^\phi_{\M}(x) = |\bar 0|+S^\psi_{\M'}(x)$ for all $x\in\X$.
\end{lemma}
\begin{proof}
We define the numbering $\phi$ as follows:
\[\begin{cases}
\phi_0(p) = 1^r 0 D(p) \\
\phi_{1^r0i}(p) = \psi_i(p) \\
\phi_j(\cdot) = \infty &\text{if $j$ contains no zeroes.}
\end{cases}\]
We will show that under the $\phi$-enumeration, the best representation for $x$ using a model $f\in\M'$ is always better than the best representation using some $f\not\in\M'$.

First suppose $f\in\M'$. Then
\begin{align*}
K^\phi(f) &=\min\{|\bar\jmath q|:\phi_{\phi_j(q)}=f\}
\leq\min\{|\bar0 q|:\phi_{\phi_0(q)}=f\} \\ 
&=\min\{|\bar0 q|:\phi_{1^r0 D(q)}=f\}  
=|\bar 0|+\min\{|q|:\psi_{D(q)}=f\}
\leq K^\psi(f)+c+|\bar 0|,
\end{align*}
where the last inequality uses the first assumption.

Now assume that the best model $f$ for $x$ is not in $\M'$. 
Let $i$ be the index of $f$ with the shortest description, i.e. it achieves the minimum in $K^\phi(f)=\min\{K^\phi(i):\phi_i=f\}$.
There are two possibilities. Either $i=0$, in which case we have $K^\phi(f)\ge r$ because $\phi_0$ cannot output zero and all other $\phi$-programs are at least $r$ bits long.

Otherwise, we can bound
\begin{align*}
K^\phi(f)&=K^\phi(i)=K^\phi(1^r0j)\\
&\ge K^\phi(j)-c' \ge K^\psi(j)+r+1-c'\\
&=K^\psi(f)+r+1-c'.
\end{align*}
Now choose $r=c''+\max\{K^\psi(\phi_0),c'-1\}$. Then substitution yields, for both cases, 
$K^\phi(f)\ge c''+K^\psi(f)$.

Combining the inequalities above, for any model $g\not\in\M'$, there is a model $f\in\M$ such that
\[\begin{split}
K^\phi(g)+C^g(x) &\ge K^\psi(g)+C^g(x)+c''\\
&\ge K^\phi(f)+C^f(x) -2c+c'' -|\bar0|.
\end{split}\]

By choosing $c''$ sufficiently large, we can ensure that the best representation is in $\M'$ for all $x\in\X$, which completes the proof.
\end{proof}

\sdr{Move this proof to appendix. Explain the lemma above it rather than below it.}
This is a technical lemma, due to the double effect of the numbering, which occurs in both the definition of model complexity and the definition of data complexity. The \emph{idea} behind the lemma, however, is simple. Given a subset $\M'$ of the model class, we can choose the numbering so that representations use a model in $\M'$ perform an arbitrary constant better than all other models. This lets us effectively `push' all other models out of the candidate set, so that the sophistication is determined by a model in $\M'$.

We can now show that there exist numberings for which the sophistication always selects the singleton models:

\begin{lemma}[Overfitting]
Let $\M \subseteq \K$ be a model class where for every $x\in\X$ there is a singleton model $f\in\M$ with $f(\epsilon)=x$. Then there is an enumeration $\phi$ of the prefix partial recursive functions, and a constant $c$, such that
\[
K(x)-S^{\M,\phi}(x)\le c
\]
for all $x\in\X$.
\end{lemma}
\begin{proof}
Let $\psi$ \sdr{This should probably be $\phi$} be any default enumeration of the partial recursive prefix functions. Note that since $f$ is a prefix function, if $f$ is defined for input $\epsilon$ then it cannot be defined for any other input. Pick $f,x$ with $f(\epsilon)=x$. Note that $x$ can be computed from $f$ and a fixed program, so there is a $c$ such that $K(x)\le K(f)+c$. Vice versa, given any $x$ we can construct an index of $f$, since $\psi$ is an acceptable numbering. Therefore $|K(f)-K(x)|\le c$.

We now define a computable function $D$ by $D(\bar\imath p)=j$ where $\psi_j(\epsilon) = \psi_i(p)$.  We will show that the two conditions of Lemma~\ref{lemma:thecoolone} hold for the prefix function $D$.

(1) Let $f$ be any function in the range of $D$, and $x$ its output. Then $\min\{|p|:\psi_{D(p)}=f\}=\min\{|\bar\imath q|:\psi_i(q)=x\}=K(x)\le K(f)+c$. (2) On the one hand $L^{\M',\psi}(x)\le K(f)+|\epsilon|\le K(x)+c$. On the other hand, $L^{\M,\psi}(x)$ is an effective description of $x$, so $K(x)$ is at most a constant larger. Together, these inequalities establish the second condition.

Now, by Lemma~\ref{lemma:thecoolone} there is an enumeration $\phi_1,\phi_2,\ldots$ such that $S^{\M,\phi}(x)=|\bar 0|+S^{\M',\psi}(x)$. We observed that $|K(f)-K(x)|\le c$ for any $f\in\M'$, so $S^{\M',\psi}(x)\ge K(x)+c$. This proves the lemma.
\end{proof}

In short, this lemma tells us that there are numberings for which the singleton models always determine the sophistication. This means that for any dataset, there is a numbering $\phi$ such that the sophistication is equivalent to Kolmogorov complexity, which implies that this sophistication is unsound (by Lemma~\ref{lem:broken}).

Many variants in the literature are susceptible to this problem, \sdr{Don't say ``many'', just list explicitly} including the structure function \sdr{Type error, the structure function is not a sophistication} \cite{cover1985kolmogorov,gacs2001algorithmic}. In \cite{vitanyi2004meaningful,adriaans2012facticity} this issue is avoided by not using self-delimiting representations. \sdr{Prep the balance idea so that this is understandable. We need it} Since a singleton representation places all information in the (self-delimiting) model part, singletons are automatically less efficient than representations placing some information in the input. While this side-steps the issue of the singletons and may be a sufficient measure against overfitting, it is not clear that it helps make the sophistication invariant.

Another approach may be to make the constant in the threshold function \sdr{Nice point but very hard to understand the way its done now since we didn't really rub the framework in the reader's face} dependent on the choice of enumeration. We are not aware of any approach to this effect, and there does not seem to be a simple way to determine the threshold based on the properties of the numbering.

\section{Underfitting: the universal model}
\label{section:underfitting}
On the other end of the spectrum, there exist candidate representations which place all information in the `noise part', using a universal function as the model. If such models are allowed, we can choose our numbering so that they always determine the sophistication.

\begin{lemma}[Underfitting]
Let $\M$ be a model class containing model a universal model $u$, with the property that $\exists c \forall m \in \M : L^u(x) \leq L^m(x) + c \p$. Then, there exist numberings such that $\s^\M$ is bounded.
\end{lemma}
\begin{proof}
Let $D$ be a prefix function as in Lemma~\ref{lemma:thecoolone} such that it returns the index of $u$ for the argument $\epsilon$ and $\infty$ for any other argument. That is, $\M' = \{u\}$. This construction satisfies the conditions 1 and 2 from Lemma~\ref{lemma:thecoolone}; invoking it we find that there exists an acceptable numbering for which $\s_\M(x) = \s_{\M'}(x) + c$. Since $\M'$ contains only a single model, $\s_{\M'}$ is constant.
\end{proof}
The variants that are susceptible to the this type of underfitting are facticity \cite{adriaans2012facticity} and some versions of effective complexity \cite{gellmann1996information}. The latter aims to bypass the problem by selecting carefully from the candidate set, but as we have shown, for some numberings the candidate set contains only the universal model.

\sdr{Explain that this is actually a widely recognised problem}

Most sophistication measures avoid the underfitting problem by limiting their model class, usually to either finite sets ($\F$) or total functions ($\T$ or $\P$). This eliminates the universal model, but as we shall see in the next section, this does not solve the problem for most data. 

\subsection{The problem of depth}

We have seen that some variants of sophistication sidestep the simplest examples of underfitting by limiting the model class to exclude universal models. While this eliminates the simplest proof of boundedness, it does not follow immediately that the sophistication based on a restricted model class must be unbounded. However, a well-known result in algorithmic statistics shows that there are strings for which no finite set is as good a model as the singleton set. In this section we will focus on the variants $S^\T$ and $S^T_K$ \cite{vitanyi2004meaningful}, as those are the variants most resistant to proofs of non-invariance. \sdr{Do I know this notation?}

\begin{lemma}
There are infinitely many $x$ such that $S^\F_K(x) \geq K(x)/5$. \label{lemma:structure-function-is-not-bounded}
\end{lemma}
\begin{proof}
From \cite[Proposition~I.3 (b)]{gacs2001algorithmic} we know that for sufficiently large $i$ there are strings of length less than $i$ that are not $(i/4, i/4)$-stochastic. For each $i$, let $x_i$ be the smallest binary string that is not $(i/4, i/4)$-stochastic. Since every string is $(\alpha,\alpha)$-stochastic for sufficiently large $\alpha$, this sequence contains infinitely many distinct binary strings. Now pick any $i$ that is large enough that (a) $|x_i|<i$ and (b) $K(x_i)/5 < i/4$. By definition of $(\alpha,\beta)$-stochasticity, if a string is not $(i/4,i/4)$-stochastic, then for every finite set $S$ containing $x$, either (a) $\log|S|\ge K(x_i)+i/4$, in which case $S$ cannot be a candidate representation for $x$ in $C^\F_K$, or (b) $K(S)>i/4>K(x_i)/5$. This proves the theorem.
\end{proof}

\begin{lemma}
Let $f$ be a total, computable function, such that for some $d$, $f(d) = x$ and let $k = K(f) + |d|$. Then there exists a finite set $S$ containing $x$ and a constant $c$, such that $K(S) \leq K(f) + K(|d|) + c$ and $\log |S| \leq |d|$.\label{lemma:total-to-sets}
\end{lemma}
\begin{proof}
Let $S = f\left(\{0,1\}^{|d|}\right)$. Since $f$ is total, this set can be explicity computed from a description of $f$ and a description of $|d|$, which tells us that there is a constant $c$ such that $K(S) \leq K(f) + K(|d|) + c$. 

Since $S$ is the image of a set of size $2^{|d|}$ under $f$, we have $\log |S| \leq |d|$.
\end{proof}
This lemma is a variation on \cite[Lemma~7.2]{vitanyi2004meaningful}.

We use this lemma to transport the result of Theorem~\ref{lemma:structure-function-is-not-bounded} to the model class of total functions:

\begin{theorem}
There are infinitely many $x$ with 
\begin{enumerate}
  \item $S^\T_K(x) \geq K(x)/6$, {and}\label{eq:poezenvoer}
  \item $S^\T(x) \geq K(x)/6$. \label{eq:hondevoer}
\end{enumerate}
\end{theorem}
\begin{proof}
We continue from the proof of Theorem~\ref{lemma:structure-function-is-not-bounded}. Suppose towards contradiction that either $C^\T_K(x_i)$ or $C^\T_{\tau^\circ}(x_i)$ contains a candidate $f$ with $K(f)\le i/5$. Then by Lemma~\ref{lemma:total-to-sets}, there is a set $S$ with $\log|S|\le|d|\le\tau^\circ(x)\le K(x)$, so (a) in the proof of Theorem~\ref{lemma:structure-function-is-not-bounded} above cannot be the case, and $K(S)\le K(f)+K(|d|)+c\le i/5+K(|d|)+c$, which contradicts (b) in the same proof for large enough $i$. Therefore such candidates $f$ cannot exist and all candidates must satisfy $K(f)>i/5>K(x_i)/6$.
\end{proof}
Note that we favoured simplicity of the proof over the strength of the bounds: the multiplicative constant $1/6$ can be improved with a more detailed treatment.

This tells us that for the right strings, a sophistication so defined can truly grow to a significant part of the total description length. While this shows that sophistication, if defined with particular care, is not entirely vacuous, we can shed some light on exactly which strings end up with high sophistication, using the principle of \emph{depth}. We will use the following definition:

\begin{definition}[depth]\belowdisplayskip=-12pt
Let $U$ be some universal Turing machine, so that $U(\bar\imath y) = \phi_i(y)$. Let $U^t$ be a simulation of this machine, which is allowed to run for at most $t$ steps, and returns $0$ if it has not yet finished at that point. Let $C^t_\M(x) = \min\{|\bar\imath y| : U^t(\bar\imath y) = x, \phi_i \in \M\}$.

The \emph{$c$-depth} $d^\M_c(x)$ of a string is defined as:
\[
	d^\M_c(x) = \min \left\{t : C^\M_t(x) - C^\M(x) \leq c \right\}
\] 
\end{definition}
Deep strings are those that can only be optimally compressed with a great investment of time. The notion of depth was first proposed by Bennett \cite{bennett1988logical} and later refined to variants of this form \cite{antunes2006computational}. We note that it is exceedingly unlikely that a deep string is sampled from a shallow distribution \cite{bloem2014safe,bennett1988logical}.
 
\begin{theorem}
Let $A(n)$ be the single-argument Ackermann function and $c$ some arbitrary constant. There are numberings such that for all strings with depth $d^\C_c(x) \leq A(C(x))$, $\s^\T(x)$, $s^\P(x)$ and $\s^\F$ are bounded.
\end{theorem}
\begin{proof}
Let $U(\bar\imath y)$ be some (non-prefix) universal Turing machine, and let $U^A(\bar\imath y)$ be a simulation of that machine which outputs $0$ if the number of steps taken exceeds $A(|\bar\imath y|)$. Let $u$ be the index of the function $U^A$ in the standard enumeration.

Let $D$ be a prefix function with $D(\epsilon) = u$. We can instantiate Lemma~\ref{lemma:thecoolone} with $D$, $\M' = \{\phi_u\}$ and $X = \{x : d^\C_c(x) \leq A(C(x))\}$. This tells us that there exists a numbering for which $\s^\T(x) = \s^{\M'}(x) + |\bar0| \leq c$ for all $x \in X$.
\end{proof}

This shows that while high-sophistication strings exist for variants with limited model class, the mechanics do not follow the basic intuition of sophistication. Unless we encounter data that would take longer than the lifetime of the universe (and then some) to produce, sophistication behaves exactly as it would without the restricted model class: a solution to a super-exponential problem would have no greater sophistication than a string sampled from a simple Bernoulli model. This result also suggests that the `non-stochastic' property of strings with high sophistication \cite{shen1983concept,vereshchagin2004kolmogorov} says more about depth and totality than it does about the lack of randomness in its information content.

The relation between sophistication, depth and absolutely non-stochastic strings was also investigated in \cite{antunes2013sophistication}.

\section{Conclusion}
\label{section:conclusion}
As we have seen, the idea behind sophistication has a rich history. It has been proposed many times, by many different authors. This tells us two things. Firstly, there is a strong intuition behind sophistication, shared by many respected authors, and second, it is not easy to get the definition exactly right.

So why should we consider this intuition reasonable at all? The first assumption of sophistication is that high compressibility and pure randomness are objectively `uninteresting'. A more sophisticated signal, such as a television broadcast must contain, in some objective sense, more meaningful information. Whether this intuition seems reasonable likely differs per person, but sophistication goes one step further: it also suggests that we can objectively separate the information in a string into random and structured.

To see whether this intuition is valid; imagine that we sample from the universal distribution by feeding random bits to a universal Turing machine. If this UTM is constructed in the conventional manner, the initial bits will determine the index of a partial computable function $i$, and the rest will be the input to that machine $y$. What's the characteristic model of this data? Certainly, we cannot exclude the possibility that $\phi_i$ is also a universal model. Also, we cannot exclude the possibility that $\phi_i$ is a singleton model for $x$.

This kind of perspective, with a probabilistic source for the data is strongly discouraged by the proponents of algorithmic statistics: they often argue that information theory should precede probability theory \cite{kolmogorov1983combinatorial}. Nevertheless, this sampling scenario undercuts the intuition behind sophistication: there is no reason to assume that a string with high sophistication was sampled from a complex model, so how can the complex model be intrinsic to the data, moreso than any other model that reaches the Kolmogorov complexity?\footnotemark

\footnotetext{Note that computational depth and Kolmogorov complexity do not have this problem. The properties of a distribution allow us to put bounds on both the depth and the complexity we can expect from strings produced by it.}

For this reason, we take a skeptical view of sophistication. The matter of its invariance, in particular cannot be glossed over. It should be the first question asked of any proposal. We can offer the following insight, to back up our skepticism. As we have seen, candidates within a constant of the optimal representation can occur all over the spectrum, from small to large models. A small change in construction, such as a different acceptable numbering, or a change in threshold function, can arbitrarily push models into or out of the candidate set. Whereas in Kolmogorov complexity, constant changes have constant effects on the outcome, here constant changes can cause very large changes in the outcome. While no authors have touched on the issue of invariance explicitly, it is strongly related to the issue of instability, discussed in \cite{antunes2013sophistication,vereshchagin2013algorithmic}.

The most common approach, restricting the models to be total, is problematic. As we have seen, the restriction to a model class $\M$ tends to assign high sophistication to those strings that can be compressed significantly better by models outside $\M$ than by models within it. For the class $\T$, this results in only extremely deep strings getting high sophistication. While this is an interesting property, which is worth investigating, the interpretation of an objective separation into model and noise is spurious at best. Instead of interpreting these strings as ``non-stochastic'', we consider it more natural to consider them \emph{non-typical for $\M$}.  

Finally, we note that the use of total functions has a very subtle effect on the discussion surrounding sophistication. Since the total functions are not enumerable, the value of the sophistication relies strongly on the difference between the Kolmogorov complexity and the Kolmogorov complexity bounded to total functions. This gap is exceedingly difficult to capture, because the total functions cannot be enumerated. 

This is dangerous: if such definitions cannot be proved wrong or right, they will stand as the ad-hoc official view, while the restrictions used to elude proofs of incorrectness make them very difficult to build on. This creates an artificial dead-end for a very valuable area of research.

\subsection{A world without sophistication}

It is too early to say that sophistication is an impossible ideal, but the points we have raised certainly show that it is riddled with subtle pitfalls, and inspired by questionable intuitions. We will conclude by asking what it would mean if we were to abandon this ideal entirely.

Consider the following metaphor. We are given a bitstring which can be read as a bitmap image of the painting \emph{Impression of a Sunrise}. The candidate set for this string contains various models from very generic to very specific. The theory behind sophistication suggests that we can choose one of these as the objective, intrinsic model of the data. Different models say different things: the universal models says that it is a `thing'. A more specific model might say that it is an image. Even more specific would be a painting, a Monet, or specifically the painting Impression of a sunrise. These are the models we imagine we may find in the candidate set. Can we say that the data is intrinsically more of an image than a painting? More of a Monet than a painting? We see no intuitive reason why such a distinction should be possible.    

Of course, Kolmogorov complexity does allow us to say that it is unlikely to be a Jackson Pollock painting, or a piece of music. And, if we were to get a second sample from the same source, we \emph{could} answer the question: we would receive either the same painting again, or perhaps another Monet, and we would gain information about the model. But a single sample by itself should give us no reason to choose an intrinsic model. Ultimately, this is just a metaphor, but since the notion of sophistication is driven by intuition, we feel that an intuition to the contrary should serve to sharpen the debate.

Our conclusion is that in giving up sophistication, we do not lose algorithmic statistics, but rather clean up some of its more unmanageable components. Letting go of the minimal sufficient statistic will allow us to return to the simplicity of $\K$ as model classes. This gives us a simple framework, with a one-to-one relation to Bayesian statistics and MDL. 

\subsubsection*{\ackname}

This publication was supported by the Dutch national program COMMIT and by  the Netherlands eScience center.

\bibliographystyle{plain}
\bibliography{facticity}

\appendix


\end{document}

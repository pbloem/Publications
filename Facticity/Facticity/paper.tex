\documentclass{style/llncs}

\usepackage{amsmath,amsfonts}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[mathscr]{eucal}

\newcommand{\M}{\mathscr M}
\newcommand{\C}{\mathscr C}
\newcommand{\T}{\mathscr T}
\newcommand{\F}{\mathscr F}
\renewcommand{\P}{\mathscr P}
\newcommand{\K}{\mathscr K}
\newcommand{\X}{\mathscr X}
\newcommand{\B}{\mathbb B}
\newcommand{\D}{\Delta}
\newcommand{\N}{\mathbb N}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\pair}[1]{\left\langle{#1}\right\rangle}
\newcommand{\concat}{\oplus}
\newcommand{\symb}[1]{\texttt{#1}}
\newcommand{\br}[1]{\overline{#1}}
\newcommand{\s}{\tn{soph}}
\newcommand{\num}[1]{#1_1, #1_2, \ldots}

\newtheorem{conj}{Conjecture}

\let\doendproof\endproof
\renewcommand\endproof{~\hfill\qed\doendproof}

\newcommand{\p}{\,\text{.}}

\newcommand{\tuple}[1]{\left\langle{#1}\right\rangle}

\newcommand{\hide}[1]{}

\newcommand{\sdr}[1]{\textcolor{blue}{\small #1\textsuperscript{[Steven]} }}
\newcommand{\pb}[1]{\textcolor{OliveGreen}{\small #1 \textsuperscript{[Peter]} }}

\newcommand{\argmin}{\mathop{\arg\min}}

\title{Four Problems for Sophistication}

\author{Peter Bloem and Steven de Rooij}

\institute{
  System and Network Engineering Group, \\University of Amsterdam, the Netherlands\\
  \email{uva@peterbloem.nl, steven.de.rooij@gmail.com}
}

\begin{document}
\maketitle

\begin{abstract}
Kolmogorov complexity is a sound measure of the amount information in a binary string, but it does not distinguish between the information that expresses the structural properties of the data, and noise. Kolmogorov's definition of the \emph{structure function} in his lecture in Tallinn in 1973 was the first attempt to separate meaningful information from noise, but many alternative definitions of the \emph{sophistication}, or the amount of meaningful information in the data, have subsequently been proposed. This indicates that the idea is considered worth pursuing by multiple distinguished scholars, but at the same time it shows that the issue has never been resolved completely satisfactorily.

In this paper we describe four fundamental problems that arise with all existing proposals, showing each of them to be \emph{unsound}. Based on these results we we lay down the view that this unsoundness is fundamental: while many nontrivial and useful structural properties of binary strings can be detected without a priori assumptions, it is \emph{not} possible to objectively quantify its sophistication.
\end{abstract}

\section*{Steven's playground}

Requirements for sophistication:

(1) Being an information measure, the value should count the number of bits required for an effective description of the meaningful properties of a binary object.

(2) The Invariance Theorem for Kolmogorov complexity states that a change in reference UTM can only influence the complexity up to an additive constant term. Analogously, if sophistication is to be interpreted as an objective property of the data, then there should be strict limits on the degree to which a change in the UTM can affect the sophistication.

(3) Sophistication should not be $\gamma$-equivalent to 0. While a valid information measure, the function 0 is clearly at odds with the intuition that there is no universal cap on the amount of meaningful information in the world.

(4) Sophistication should not be $\gamma$-equivalent to the Kolmgorov complexity, because then there is no point in defining a new word for the same concept. Moreover, Kolmogorov complexity is at odds with the intuition that there is no meaningful information in a random string.

\section{Introduction}

\noindent Kolmogorov complexity is an objective measure of the information content of an object. Although its definition contains several arbitrary choices, these only affect its value up to a constant additive term. This property is crucial for the measure's success: it allows us to view Kolmogorov complexity as a \emph{property of the data} rather than an arbitrary function computed on it. This, in turn, allows us to define the intrinsic entropy, or randomness, inherent in a single object expressed as a binary string, without any assumptions on the process that produced it. However, this notion of complexity does not in all respects match our intuitive idea of which objects are complex, and which objects are not.

Imagine an old-fashioned, analog television set which can be tuned to three frequencies. The first produces a blank screen, the second an ordinary TV show, and the third white noise. Judging by their Kolmogorov complexity, these  broadcasts have increasing information content. The first is easiest to describe, the second is harder but still contains a great deal of regularity, and the third cannot be compressed. However, intuitively speaking the second signal clearly conveys more meaningful information than the other two. This notion has proved far harder to capture in formal terms than the simple notion of `raw' information content. 

Over the years one solution in particular has been proposed in many different forms. We select a description for the data by two-part coding; describing a model first, and then the data in terms of the model. The size in bits, used to describe the model is taken as a measure of the useful information in our string. This idea has been proposed many times in many different guises, which shows that it is both a persistent intuition but also that it is a difficult idea to formalize in a meaningful way. For the purposes of this paper, we will refer to this concept as \emph{sophistication}\footnotemark. 

\footnotetext{As the historical overview shows, sophistication is only one name for this idea, and not the first. The alternatives, however, are more awkward, and sophistication has been used by different authors for different realizations of the same principle.}

The intuitive idea of sophistication makes one crucial demand: we can find an objectively value for the model complexity of a string, either by having a single objectively optimal two-part description, or a set of descriptions with equivalent model complexity. In the framework of Kolmogorov complexity, two observers cannot disagree by more than a constant about the information content of a dataset. In formal terms: the Kolmogorov complexity of a dataset $x$ is defined as the shortest effective description for $x$ in some Turing complete description language. This description language is usually expressed as a universal Turing machine $U(\bar\imath y) = \phi_i(y)$ where $\phi_i$ is the $i$'th partial computable function in some effective enumeration, and the $\bar\imath$ is a self-delimiting encoding of $i$. If two people disagree about the shortest effective description of $x$, we can interpret this as them using different description languages, ie. different universal Turing machines. The crucial aspect of Kolmogorov complexity, the starting theorem that makes it interesting as a function at all, is \emph{invariance}: two people using different description languages will get the same value for a string's Kolmogorov complexity, albeit up to a fixed constant. Without invariance, Kolmogorov complexity would be an arbitrary function. With invariance, we can be sure that we are talking about an intrinsic property of the data. The same consideration must be given to each proposal for formalising sophistication: is it invariant? 

There are many approaches to sophistication, but the following pattern is common to all of them. We start with a space of \emph{models}. There can be finite sets, computable probability distributions or simple functions from strings to strings. The function perspective is the most generic, all other model classes can be translated to it. The approach to sophistication is to find a representation for a dataset $x$ in two parts: a computable function $f$ and and input $y$, such that $f(y) = x$. As with Kolmogorov complexity, the shorter a description is, the better. We take the shortest description and those close to it, and consider them \emph{candidates}. How close a description must be to the optimum differs between variants, one popular choice is to use a pre-chosen constant value. We then choose a single description from the candidate set and return the description length of the model as the sophistication. Usually, the candidate with the smallest model determines the sophistication.

The implicit assumption behind sophistication is that this approach will separate the description of the string into a structural part, representing its regularities, and a stochastic part representing its non-deterministic properties. In other words, the string is cast as a random choice from a model for which it is `typical'.  

In this article we will show that most proposals are not invariant. \sdr{Ik wil dit nog steeds graag sterker gesteld hebben.} For one remaining case, we can offer no such proof, but no proof of invariance exists either. This is likely due to the complex nature of the model class chosen, and the difficulty of resolving the matter of invariance shows how complex such a sophistication measure would be to build on, even if it did prove to be invariant.

\subsection{A brief historical overview}

We first provide a brief chronological overview of the existing variants of sophistication. We will return to each in-depth in the body of the paper. 

The first suggestion that a representation of a string can be separated cleanly into an encoding of its regular and its stochastic proeperties came from Kolmogorov himself, in a talk given at a conference in Tallinn in 1973 (first committed to paper by Cover \cite{cover1985kolmogorov} and later discussed more extensively discussed by Vereshchagin and Vit\'anyi \cite{vereshchagin2004kolmogorov}). Here, he proposed what has become known as the \emph{Kolmogorov structure function}.Minimizes over two part representations for a given string $x$, consisting of some finite set $S$ (containing $x$) and the index of $x$ in $S$. The \emph{minimal sufficient statistic} is the set with the lowest complexity for which the length of the two-part representation reaches the prefix-free Kolmogorov complexity. It was first shown by Shen \cite{shen1983concept} that there are strings for which the best model cannot be small.

Koppel \cite{koppelSoph1988,koppel1991almost} built on this notion, coining the phrase \emph{sophistication}. He changed the class of models from finite sets to total functions and used monotone functions, instead of prefix-free Kolmogorov complexity. We expect that this change was made to extend the theory to infinite strings, an approach which has been abandoned by subsequent authors. We will only consider the finite version (based on regular functions) in this paper.

While Koppel expands the model class from finite sets to functions,he does require that the functions be total. This is a recurring theme: if the model class is expanded to all computable functions, the universal Turing machine $U$ itself becomes a model. If we assume that $U$ computes the function $\phi_u$ in the standard enumeration of computable functions, and that $\bar\imath y$ is a shortest description for $x$ on $U$ (ie. $U(\bar\imath y) = x)$), then $\br{u}\br{\imath}y$ is also a description for $x$ on $\bar u$. This description has a constant model size ($|\br{u}|$), ie. every $x$ has constant sophistication. This `collapse' to a universal model is the reason that most treatments of sophistication constrain the model class to exclude the universal Turing machine. This makes it more expensive to move information out of the model. As we shall see, defining sophistication quickly becomes a balancing act, forcing information one way or the other.

In 1996 Gell-Mann and Lloyd introduced `Effective complexity' \cite{gellmann1996information}. Working from the perspective of statistical mechanics, they arrived (largely independently, it seems) at a notion that fits the mold of sophistication. Effective complexity describes a binary string in terms of a probability distribution over finite strings (an `ensemble' in the terminology of Gell-Mann and Lloyd) and the entropy of the distribution. The reasoning is that, if the string is typical for the distribution, the entropy is sufficient to describe the string given the distribution, so that the length of the description of the distribution and the size of its entropy suffices as a two-part description. The two-part description which determines the effective complexity is chosen by placing a time bound (taken from the singleton description) on the computation of the string from the distribution. The hope is that this will move deep (ie. hard to compute) information to the model part, and shallow information to the data. In a later treatment\cite{gell2004nonextensive}, Gell-Mann and Lloyd select the decisive two-part representation from the candidates based on ``the average quantities judged to be important'', which seems to be a conscious choice to make the measure subjective and context-dependant (sacrificing invariance).

G\'acs, Tromp and Vit\'anyi \cite{gacs2001algorithmic}, built on the idea of the structure function to establish a framework of \emph{algorithmic statistics}, as an attempt to found a statistical theory not on probabilistic notions, but on combinatorial ones. This echoes Kolmogorov's own hope for the theory of information:
\begin{quotation}
\noindent Information theory must precede probability theory and not be based on it. By the very essence of this discipline, the foundations of information theory must have a finite combinatorial character. \cite{kolmogorov1983combinatorial}
\end{quotation}
Algorithmic statistics studies binary strings and their possible two-part representations. In a later paper \cite{gacs2001algorithmic}, Vit\'anyi specifically studies the role of the minimal sufficient statistic as an expression of the `meaningful' information of a string (taking the name sophistication from Koppel). Like Koppel, Vit\'anyi restricts the model class to total functions, eliminating the universal model.

Antunes et al. \cite{antunes2009sophistication} note that the constant cutoff used to separate candidate two-part descriptions from those deemed to be too inefficient, is a source of instability. Small changes to this constant could shift the result sophistication by arbitrary amounts. They introduce a more elaborate criterion for the cutoff, calling this variant \emph{coarse sophistication}. As we shall see, this version behaves very differently from regular sophistication, and is closer to computational depth.
 
Adriaans' \emph{Facticity}, introduced in 2012\cite{adriaans2012facticity}, relaxes the model class to include all partial recursive functions. By constructing a very specific universal Turing machine, and taking constant terms into account, the facticity is expected to become robust against the collapse to a universal model.

\pb{Vereshchagin's strongly sufficient statistic} 

\pb{Antunes' instability result}

\pb{Diagrammetje}

\subsection{Preliminaries, definitions and desiderata}

Before we start, we will define a framework into which we can fit all current variants of sophistication. The principle is always the same: we start with a class of models, we consider the space of two-part descriptions using these models, we select the set of representations with the shortest code-lengths and we select, by some method, the best model from this set. 

Let $\B = {0,1}^*$. We deal with partial functions $\phi: \B \to \B$, which we will also call \emph{models}. Such functions are \emph{computable} if they can be computed by a Turing machine as described in \cite[Definition~1.7.1]{li1993introduction}. A function is prefix-free if its domain, $D_f = \{y : f(y) \neq \infty\}$ is a prefix set. A function $f$ is \emph{total} if $D_f = \B$. A prefix-free function is \emph{complete} if every infinite binary string has a member of $D_f$ as a prefix. \footnotemark 

\footnotetext{Completeness is analogous to totality, in that a prefix-free Turing machine that does not halt corresponds to a complete prefix-free function.}

\begin{definition}[Models and model classes]
  A \emph{model} is a function $\phi:\B\to\B$. A
  \emph{model class} is a set of models. We distinguish four important
  model classes:
  \begin{itemize}
  \item $\C$ is the set of all partial computable functions,
  \item $\K\subset\C$ is the set of all prefix-free
    functions,
  \item $\T\subset\C$ is the set of all total functions,
  \item $\P\subset\K$ is the set of all complete, prefix-free functions,
  \item $\F\subset\P$ contains, for every finite set $S$, a computable
    surjective function mapping the binary sequences of length
    $\lceil\log S\rceil$ onto $S$. In other words, this is the model class of finite sets, with uniform distributions.
  \end{itemize}
\end{definition}
\begin{definition}[Numberings]
A numbering is an enumeration of $\C$, denoted as $\phi_1, \phi_2, \ldots$ or by a function $g: \N \to \C$. We fix one canonical numbering $g^\circ$, which works as an effective description, that is for any $i \in \N, y\in \B$, we can effectively compute $(g^\circ(i))(y)$. We call a numbering $g$ \emph{acceptable} if there exist total, computable functions $a, b: \N \to \N$ as that for all $i$ $g^\circ(a(i)) = h(i)$ and  $g^\circ(i) = h(b(i))$.
\end{definition}

\begin{definition}[Prefix encoding function]
  A prefix encoding function $f:\B\to\B$ is an injective
  function that maps the binary strings to a prefix-free set. There
  are prefix encoding functions $g$ with the property that
  $|g(x)|\le|x|+2\log(|x|+2)|$ \cite{li1993introduction}. We fix such a function and
  denote it by $\br{x}=g(x)$.
\end{definition}

\begin{definition}[Complexity]
Let $\phi_1,\phi_2,\ldots$ be the canonical acceptable enumeration of $\C$. Let $\Pi$ be a computable subset of the natural numbers such that  $\{\phi_i:i\in\Pi\} = \K$. Now define Kolmogorov complexity by
\begin{align*}
C(x)&=\min\{|\bar\imath| y:\phi_i(y)=x\},\\
K(x)&=\min\{|\bar\imath| y:\phi_i(y)=x, i\in\Pi\}.\\
\end{align*}
\end{definition}
This definition of $K$ is uncommon. It is functionally equivalent to the standard definition as used in \cite{li1993introduction}, but this form is more convenient for defining the $K$-complexity of non-prefix functions (see Definition~\ref{definition:model-complexity} below).

We now have the basic tools with which we set up our generic sophistication function. We first define the space of two-part representations for a given string $x$, and the codelength that each representation achieves:

\begin{definition}[Representations and code-lengths]
A pair $(m,y)$ is a \emph{representation} for $x$ if $m$ is a model and $m(y)=x$. 

Define the following code-length functions for the representations:
\begin{itemize}
\item $L(m)$ is the length of a self-delimiting description of
  the model $m$, i.e. it is the length function of a prefix code for models. Each variant of sophistication must supply such a function.
\item $L^m(x)=L(m)+\min\{|y|:m(y)=x\}$ measures the number of bits required to describe both the model $m$ and the data $x$ using the model.
\item $L^\M(x)=\min\{L^m(x):m\in\M\}$ measures the number of bits
  required to describe $x$ using the best model in the model class.
\end{itemize}
\end{definition}
We can now define which of these representations express the data most efficiently:
\begin{definition}[Candidate set]
The \emph{candidate models} for $x$ from model class $\M$ and threshold function $\tau$ are
\[
  C^\M_\tau(x):=\{m\in\M:L^m(x)\le \tau(x) + c\}
\]
with $c$ a constant independent of $x$ which may be chosen arbitrarily. The default threshold function is $\tau^\circ(x)=L^\M(x)$.
\end{definition}

\begin{definition}[Sophistication]
  The model complexity, meaningful information, effective complexity, facticity or \emph{sophistication},  of a string $x$ given a model class $\M$ and threshold function $\tau$ is
  \[
  \s_\tau^{\M}(x):=\min\{L(m):m\in C^\M_\tau(x)\}\p
  \]\label{definition:sophistication}
\end{definition}
For a function like sophistication to be meaningful, it must satisfy certain properties. We will check all currently published variants against the following minimal requirements:

\begin{description}
  \item[Invariance] Like the Kolmogorov complexity, the construction of a sophistication function makes certain ad-hoc choices, which are captured in the choice of an arbitrary universal Turing machine $U$ or an arbitrary numbering of $\C$. If we change what choice we make, the Kolmogorov complexity remains invariant up to a constant. This is the sole reason we allow ourselves to interpret the Komogorov complexioty as a property of the data, rather than simply the result of some computation on it. A similar invariance must hold for any sophistication function.
   \item[Unboundedness] Both the sophistication and the residual information (roughly the complexity minus the sophistication) must not be bounded by a constant. If the sophistication is bekow a constant for all strings it cannot have much meaning. Also, if it is always withing a constant of the Kolmogorov complexity, it cannot correspond to the notion we hope to capture with it.
   \item[Efficiency] The two-part representation $(m, y)$ that witnesses the sophistication (that is the one that reaches the minimum described by $\s^\M_\tau$) must reach the Kolmogorov complexity (or at least the difference should be insignificant). If there is an effective one-part representation that is much more efficient than the two-part representation that witnesses the sophistication, we cannot, in all honesty, claim that $(m, y)$ captures all the information in $x$.
\end{description}

Given the philosophical underpinnings of sophistication, these three requirements are the very least that we could expect. We will show that no current proposal satisfies all three. 

For all variants but one, we show that all measures that are invariant are bounded. This leads us to the following conjecture. Let $\gamma(n)$ be a function representing a \emph{granularity}: the extent to which we care about differences between functions. That is, we will call two functions $f$ and $g$ $\gamma$-equal if $|f(x) - g(x)| \in O\left(\gamma\left(f(x)\right)\right)$.

\begin{conj}
Let $\s_g$ be a sophistication function constructed as in Definition~\ref{definition:sophistication}, built on some acceptable numbering $g$, and some fixed model class $\M$ and threshold function $\tau$ (omitted from the sub/superscript for the sake of clarity). 

For any sophistication function so constructed, and any granularity $\gamma$ we have that for any two acceptable numberings $g$ and $h$, either:
\begin{itemize}
  \item there exist $\s_g$ and $\s_h$ which are not are $\gamma$-equal (ie $\s$ is not invariant),
  \item $s_g$ is $\gamma$-equal to $0$ or to the Kolmogorov complexity.
\end{itemize}
\end{conj}

In these terms, all instantiations of the Kolmogorov complexity, using different Universal Turing machines are $1$-equal. It may be unreasonable to require the sophistication to attain the same level of invariance (a log-invariant sophistication could still be useful). This conjecture allows such relaxations: if it is true, sophistication must be considered fundamentally flawed. We hope that this conjecture will prompt authors to address the matter of invariance critically, and to consider the possibility that sophistication and its ilk are an untenable proposition.

In the rest of this paper, we will analyse the existing variants one by one. We will conclude with the informal reasoning supporting this conjecture and consider its the consequences. What does this mean for the problem of meaningful information, what does it mean for algorithmic statistics and most importantly, what are the consequences for the idea of model selection in general?

\section{Inefficient indices}

We start from a simple intuition: the definition of Kolmogorov complexity already uses a description of the data in two parts; a computable function and and input. Can we assume that such a description, that is as efficient as the Kolmogorov complexity, creates a good separation into the structural part of the data and the noise?

The first problem that arises is that allowing any acceptable numbering can lead to inefficient model representations.

\begin{lemma}
Let $\phi$ be some acceptable numbering, and $\s_\phi(x)$ be any function which gives the length of some index $i$ such that $\phi_i(y) = x$ for some $y$.

No such function can be $\gamma$-invariant and unbounded for any $\gamma(n) \in o(n)$.
\end{lemma}
\begin{proof}
Choose some $\gamma(n) \in o(n)$. Assume towards contradiction that the sophistication is $\gamma$-invariant and unbounded.

Let $\num{\phi}$ be the canonical acceptable numbering. Let $s_i$ be the binary string consisting of $2^{i}$ zeroes followed by a one.

We now define two new acceptable numberings $\num{\psi}$ and $\num{\xi}$:
\[
\psi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s_{2i}\\
	\infty\;\text{otherwise}
\end{cases}
\]
and 
\[
\xi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s_{2i+1} \\
	\infty \;\text{otherwise}\p
\end{cases}
\]
Let $\s_\psi$ and $\s_\xi$ be the sophistication functions built on these enumerations. Choose any $x$ and assume w.l.o.g. that $\s_\psi(x) < \s_\xi(x)$. By construction of the numberings $\psi$ and $\xi$, we have $2\;\s_\psi(x)  \leq \s_\xi(x)$, which gives us:
\begin{align*}
	\s_\xi(x) - \s_\psi(x) &\geq \frac{1}{2}\, \s_\xi(x)  \in \Omega(\s_\xi(x))\p 
\end{align*}
Since, by assumption, $\s_\xi$ can be arbitrarily large and $\s_\xi(x) - \s_\psi(x) \in O(\gamma(\s_\xi(x))) \subset o(\s_\xi(x))$, we have a contradiction.
\end{proof}

In this proof, we deliberately use ridiculously inefficient index functions. Obviously one could simply pick a more efficient index; the point is that many definitions of sophistication to not mention any such restriction. As we shall see in the next, any inefficiency in the index will lead to a lack of invariance in sophistication.

The treatments that fall prey to this problem are Koppel and Atlan's \cite{koppelSoph1988,koppel1991almost},  and several of its variants (\cite{antunes2009sophistication,antunes2013sophistication}). 

Interestingly, using an inefficient index is no problem in the definition of Kolmogorov complexity, since we can select as a model a UTM with an efficient index at only a constant penalty (by the Invariance Theorem \cite{li1993introduction}). But this robustness property does \emph{not} carry over to definitions of sophistication, if we are interested in separating the model and its input, we must represent the model efficiently. 

Another perspective ion this problem is that the length of a model in an ordinary description does not represent its complexity, much like the length of a compressible string does not reflect its information content. In order to obtain a more robust measure of model complexity, we describe a natural extension of Kolmogorov complexity to functions, and show that this retains invariance.

\begin{definition}
  The complexity of a partial recursive function is defined by
  \begin{align*}
    C_\phi(f) &:= \min\{C(i):\phi_i=f\}\\
    K_\phi(f) &:= \min\{K(i):\phi_i=f\}.
  \end{align*} \label{definition:model-complexity}
\end{definition}
These definitions also appear in \cite{grunwald2004shannon} and \cite{vitanyi2004meaningful}. We will show that these values are invariant up to a constant (ie. complexity is an objective property of a function):
\begin{lemma}[Invariance]
Let $\phi_1, \phi_2, \ldots$ and $\psi_1, \psi_2,\ldots$ be two acceptable numberings. There exists a constant $c$ such that $\left| K^\phi(f) - K^\psi(f)\right | \leq c$ for all $f$. \label{lemma:invariance}
\end{lemma}
\begin{proof}
Let $g(i)$ be the function which which translates from $\psi$ to $\phi$.
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(i) : \phi_i= f\right\} \\
&\geq \min\left\{ K^\psi(i) : \phi_i= f\right\} - c\\
&= \min\left\{ K^\psi(i) : \psi_{g(i)}= f\right\} - c\\
&\geq \min\left\{ K^\psi(i) : \psi_i= f\right\} - c\\
&= K^\psi(f).
\end{align*}
We can reverse $\phi$ and $\psi$ to achieve the same inequality the other way around, completing the proof.
\end{proof}
This lemma shows that the complexity of a function depends on the choice of
enumeration by no more than a constant term.

There are two ways to use this notion of model complexity for more robust attempts to define sophistication, that differ in the way the two-part code is conceptualized. Both approaches are used in the literature, which is therefore sometimes confusing.
\begin{enumerate}
  \item On the one hand one can stick with Koppel's approach, but repair it by imposing the additional requirement that the used length of the indices used is equal to $C(\phi_i)$, and that the numbering is still acceptable. We construct such an index below. This approach is taken by Adriaans \cite{adriaans2012facticity}. In this case, the total code length is simply $C(x)$, and the sophistication is the length of the model index within this code. This approach is the most direct formalization of the intuition that the two part code used within the definition of Kolmogorov complexity separates structure from noise.
  \item On the other hand, rather than distinguishing the two components of the code \emph{within} the definition of Kolmogorov complexity, we can also use Kolmogorov complexity as a black box to measure the complexity of the model, which is then the first part of a two-part code describing the data. With this approach, the total code length is $K(f)+|y|$, where $y$ is an input to the model $f$ such that $f(y)=x$. The difference is that in this approach, we can construct everything on the basis of a single, unconstrained, acceptable numbering, without using additional contraints. It is also more natural in this setting to limit the model class of allowed functions. This approach is used for Kolmogorov's structure function \cite{cover1985kolmogorov}, Algorithmic Statistics \cite{gacs2001algorithmic}, Vi\'anyi's sophistication \cite{vitanyi2004meaningful} and Effective complexity \cite{gellmann1996information}.
\end{enumerate}

To show that the two approaches are equivalent with the correct restriction on the index, we must show that there exists a numbering for which the length of the index of a model is equal to its complexity, and that such a numbering is acceptable.

\begin{definition}[Faithful Numbering]\label{def:faithful}
  A numbering $\psi_1,\psi_2,\ldots$ of the partial recursive
  functions is \emph{faithful} if there is a constant $c$ such that
  for all indices $i$ there is a $j$ such that $\psi_i=\psi_j$ and
  $|j|\le C(\psi_j)+c$.
\end{definition}
This definition is a refinement of the one used in \cite{adriaans2012facticity}. That publication defines an \emph{index function}, rather than a numbering, and notes that it is not computable. Since we use numberings, we only need to show that they are acceptable, ie. we only need to be able to unpakc the efficient description, so the incomputability of the index function is no issue:
\begin{lemma}
  There is a faithful acceptable numbering.
\end{lemma}
\begin{proof}
Let $i_\tn{div}(y)$ be an index such that $\phi_{i_\tn{div}}(y)=\infty$ for all $y$. Define
  \[\psi_q=\begin{cases}
    \phi_{\phi_i(p)}&\tn{if $q$ can be written as $\bar\imath p$ and $\phi_i(p)<\infty$,}\\
    \phi_{i_\tn{div}}&\tn{otherwise.}\end{cases}
  \]
  To show that $\psi$ is faithful, pick any function $f$. Then
\[\begin{split}
C(f)&=\min\{C(i):\phi_i=f\}\\
&=\min\{\min\{|\bar a b|:\phi_a(b)=i\}:\phi_i=f\}\\
&=\min\{|\bar a b|:\phi_{\phi_a(b)}=f\}\\
&=\min\{|\bar a b|:\psi_{\bar a b}=f\}.
\end{split}\]
This shows there is a sufficiently small $\psi$ index.

To show that $\psi$ is acceptable, let $\phi_j$ denote the identity
function. Then a $\phi$-index $i$ can be mapped to a $\psi$-index
using the computable function $r(i)=\bar\jmath i$, so that
$\psi_{r(i)}(y)=\psi_{\bar\jmath i}(y)=\phi_i(y)$. For the reverse,
define $\phi_v(\bar\imath p, y)=\phi_{\phi_i(p)}(y)$. For fixed
$\bar\imath p$, the 
$s^n_m$-theorem \cite{kleene193notation} states that we can compute the $h$
such that $\phi_h(y)=\phi_v(\bar\imath p,y)$. Let $h(\bar\imath p)$
denote this index as a function of the program; further define
$h(q)=i_\tn{div}$ if $q$ cannot be expressed as $\bar\imath p$. By
construction $h$ is total and computable. To check that the mapping
returns the correct function, rewrite $\phi_{h(\bar\imath
  p)}(y)=\phi_v(\bar\imath p,y)=\phi_{\phi_i(p)}(y)=\psi_{\bar\imath p}(y)$.
\end{proof}

Unfortunately, even with a faithful index, we can fail, if the prefix-function used to separate the model from its input is inefficient:

\begin{lemma}
Let $L(\phi) = |\bar\imath|$ for a faithful index, and some prefix encoding function, and let $\s^\C$ be defined on this $L$ with the default threshold function.

If $\min\{i\ge i_0:|\bar\imath|-K(i)\}$ is an unbounded function of $i_0$, then $s^\C$ is bounded for all numberings $\phi$
\end{lemma}
\begin{proof}
Let $\bar\imath p$ be any two-part representation for the data $x$, i.e. $\phi_i(p)=x$. Then construct an alternative two-part representation $\bar vi^* p$, where $i^*$ is the shortest $\psi_u$-program for $i$ such that $\phi_v(i^* p)=\phi_{\psi_u(i^*)}(p) = \phi_i(p)=x$. We compare the lengths of these two representations. Note that $|i^*|=K^\psi(i)$. Therefore,
\[
|\bar\imath p|-|\bar v i^* p| = |\bar\imath|-|\bar v| - |i^*| = |\bar\imath|-|\bar v|-K(i).
\]
By assumption there must be an $i_0$ such that the above expression is positive for all $i>i_0$. From this $i$ onwards, the second representation (using $v$) will have a shorter code length, so $\bar\imath p$ cannot achieve the minimum in the definition of the facticity. Consequently, the facticity is bounded by $F^\phi(x)<|\overline{\imath_0}|$ for all $x$. 
\end{proof}
This result applies to Adriaans' facticity\cite{adriaans2012facticity}. In order to circumvent the problem, the UTM should should encode the model $f$ in $K(f)$ bits, relying on $K$'s own prefix encoding to delimit the model from its input. 

Note that the condition for this lemma applies to all known partial computable prefix functions. We conjecture that it applies to all:

\begin{conjecture}
For any partial computable prefix code length function $L$, the function
\[
\min\{L(i)-K(i):i\ge i_0\}
\]
is unbounded in $i_0$.
\end{conjecture}

This result shows that even if we have a faithful numbering, we can only use it in settings where we don't count the cost of delimiting the model from the input (such as \cite{koppelSoph1988}). In the cases where the data representations are taken as a single code word, the model-part must represent the model as efficient as $K(f)$ does. Instead of a numbering, we require a self-delimiting code for functions rather than a numbering. 

In the rest of this paper, we will take the second perspective: we will consider the representations as two-part descriptions $K(f)$, and $|y|$, such that $f(y)=x$. This description can be unpacked by a very specific universal Turing machine, but contrary to the construction of the Kolmogorov complexity, we are not free to choose any UTM.

\section{Underfitting: the universal model}

In the previous section, we saw a first example of \emph{underfitting}: a second UTM with a more efficient model representation became a model for all strings. Even if we chose our model-description carefully, to avoid the issues of the previous section, however, universal Turing machines are still candidate model for the data. These candidates represent the data efficiently (they reach the Kolmogorov complexity up to a constant), but they are extremely generic: taking a second sample from the model can produce a string that has nothing to do with the data. Nevertheless, these models perform well, in terms of compression, and there is no justification for excluding them from the candidate set.

The opposite problem is \emph{overfitting}. A model that can produce only $x$ can be described in as many bits as it takes to describe $x$, plus some constant amount. Again, such models perform optimally, but they are not very informative. We call this \emph{overfitting}.

As we shall see, most variants of sophistication are constructed specifically to prevent the problems of over- and underfitting.

Any sophistication function built on a model class that includes singletons, or universal models is in danger of over or underfitting. In that case, we can construct a numbering, so that a particular model, or class of models, represents all data more efficiently than any other model by an arbitrary constant. This constant boost is enough to force all other models out of the candidate set. 

The following lemma generalizes the principle. We will instantiate it for different variants of sophistication: 

\begin{lemma}\label{lemma:thecoolone}
  Let $\psi_1,\psi_2,\ldots$ be any acceptable enumeration of the partial recursive functions.
  Let $\M$ be any model class, let $\X$ be any set of binary sequences and let $D:\{0,1\}^*\to\N$ be a computable decoding function with a prefix-free domain that maps function descriptions to their indices in $\psi$, i.e. if $f=\psi_{D(p)}$ then $p$ is a $D$-description of $f$. Let $\M'=\{\psi_i:i\in\tn{range}(D)\}$. Further assume there is a constant $c$ such that
\begin{enumerate}
  \item $\forall_{f\in\M'}:\min\{|p|:\psi_{D(p)}=f\}\le K^\psi(f)+c$
  \item $\forall_{x\in\X}:L^{\M',\psi}(x)-L^{\M,\psi}(x)\le c$.
\end{enumerate}
Then there is an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions such that $S^\phi_{\M}(x) = |\bar 0|+S^\psi_{\M'}(x)$ for all $x\in\X$.
\end{lemma}
\begin{proof}
We define the numbering $\phi$ as follows:
\[\begin{cases}
\phi_0(p) = 1^r 0 D(p) \\
\phi_{1^r0i}(p) = \psi_i(p) \\
\phi_j(\cdot) = \infty &\text{if $j$ contains no zeroes.}
\end{cases}\]
We will show that under the $\phi$-enumeration, the best representation for $x$ using a model $f\in\M'$ is always better than the best representation using some $f\not\in\M'$.

First suppose $f\in\M'$. Then
\[\begin{split}
K^\phi(f)&=\min\{K^\phi(i):\phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_j(q)=i, \phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_{\phi_j(q)}=f\}\\
&\le\min\{|\bar0 q|:\phi_{\phi_0(q)}=f\}~\text{(using $j=0$)}\\
&=\min\{|\bar0 q|:\phi_{1^r0 D(q)}=f\}\\
&=|\bar 0|+\min\{|q|:\psi_{D(q)}=f\}\\
&\le K^\psi(f)+c+|\bar 0|,
\end{split}
\]
where the last inequality uses the first assumption.


Now assume that the best model $f$ for $x$ is not in $\M'$. 
Let $i$ be the index of $f$ with the shortest description, i.e. it achieves the minimum in
\[
K^\phi(f)=\min\{K^\phi(i):\phi_i=f\}.
\]
There are two possibilities. Either $i=0$, in which case we have $K^\phi(f)\ge r$ because $\phi_0$ cannot output zero and all other $\phi$-programs are at least $r$ bits long.

Otherwise, we can bound
\[\begin{split}
K^\phi(f)&=K^\phi(i)=K^\phi(1^r0j)\\
&\ge K^\phi(j)-c'\\
&\ge K^\psi(j)+r+1-c'\\
&\ge\min\{K^\psi(j):\phi_j=f\}+r+1-c'\\
&=K^\psi(f)+r+1-c'.
\end{split}
\]
Now choose $r=c''+\max\{K^\psi(\phi_0),c'-1\}$. Then substitution yields, for both cases, 
$K^\phi(f)\ge c''+K^\psi(f)$.

Combining the inequalities above, for any model $g\not\in\M'$, there is a model $f\in\M$ such that
\[\begin{split}
K^\phi(g)+C^g(x) &\ge K^\psi(g)+C^g(x)+c''\\
&\ge K^\psi(f)+C^f(x)-c+c''\\
&\ge K^\phi(f)+C^f(x) -2c+c'' -|\bar0|.
\end{split}\]

By choosing $c''$ sufficiently large, we can ensure that the best representation is in $\M'$ for all $x\in\X$, which completes the proof.
\end{proof}

This is a very technical and opaque lemma, due to the double effect of the choice of numbering. The choice of numbering affects both the model complexity and the data complexity by a constant amount. The idea behind the lemma, however, is simple. Given a subset $\M'$ of the model class, we can choose the numbering so that representations use a model in $\M'$ perform an arbitrary constant better than all other models. This lets us effectively `push' all other models out of the candidate set, so that the sophistication is determined by a model in $\M'$.

The simplest instantiation shows a the most well known problem with sophistication: the universal model (ie. a UTM) is a candidate model too. We can manipulate the numbering so that the universal model always determines the sophistication. That is, the sophistication measure \emph{underfits}.

\begin{lemma}[underfitting]
Let $\M$ be a model class containing model $u$, called a \emph{universal model} with the following property
\[
\exists c \forall m \in \M : L^u(x) \leq L^m(x) + c \p
\]  
Then any $\s^\M$ with the default threshold function is either non-invariant or unbounded.
\end{lemma}
\begin{proof}
We will construct an acceptable numbering for which $\s^\M$ is bounded. This means that if $\s^\M$
 is either bounded for all acceptable numberings, or not invariant.
 
Let $D$ be a prefix function as in Lemma~\ref{lemma:thecoolone} such that it returns the index of $u$ for the argument $\epsilon$ and $\infty$ for any other argument. That is, $\M' = \{u\}$. The assumptions 1 and 2 from Lemma~\ref{lemma:thecoolone} follow directly from this construction, so that we can instantiate the Lemma. This tells us that there exists an acceptable numbering for which $\s_\M(x) = \s_{\M'}(x) + c$. Since $\M'$ contains only a single model, $\s_{\M'}$ must be constant, completing the proof.
\end{proof}

The underfitting problem is the reason that almost all sophistication measures limit their model class, usually to either finite sets ($\F$) or total functions ($\T$ or $\K$). This eliminates the universal model, but as we shall see in Section~\ref{section:depth}, this does not solve the problem for most data. The variants that are sucseptible to the problem of underfitting are facticity \cite{adriaans2012facticity} and some versions of effective complexity \ref{gellmann1996information}. The latter aims to bypass the problem by selecting carefully from the candidate set, but as we have shown, for some numberings the candidate set contains only the universal model. 

\section{Overfitting: the singletons}

On the other end of the spectrum, we find the problem of optimally performing, highly-specific models. Most poigniantly, the singletons models. Sets which only contain $x$, probability distributions with $p(x) = 1$, or functions which produce $x$ for all arguments. Lemma~\ref{lemma:thecoolone} allows us to give these models a constant boost as well:

\begin{lemma}[overfitting]
Let $\M$ be a prefix model class where for every $x\in\X$ there is a singleton model $f\in\M$ with $f(\epsilon)=x$. Then there is an enumeration $\phi_1,\phi_2,\ldots$ of the prefix partial recursive functions, and a constant $c$, such that
\[
K(x)-S^{\M,\phi}(x)\le c
\]
for all $x\in\X$.
\end{lemma}
\begin{proof}
Let $\psi_1,\psi_2,\ldots$ be any default enumeration of the partial recursive prefix functions. Note that since $f$ is a prefix function, if $f$ is defined for input $\epsilon$ then it cannot be defined for any other input. Pick $f,x$ with $f(\epsilon)=x$. Note that $x$ can be computed from $f$ and a fixed program, so there is a $c$ such that $K(x)\le K(f)+c$. Vice versa, given any $x$ we can construct an index of $f$, since $\psi$ is an acceptable numbering. Therefore $|K(f)-K(x)|\le c$.

We now define a computable function $D$ by $D(\bar\imath p)=j$ where $\psi_j(\epsilon) = \psi_i(p)$.  We will show that the two conditions of Lemma~\ref{lem:thecoolone} hold for the prefix function $D$.

\begin{enumerate}
\item Let $f$ be any function in the range of $D$, and $x$ its output. Then $\min\{|p|:\psi_{D(p)}=f\}=\min\{|\bar\imath q|:\psi_i(q)=x\}=K(x)\le K(f)+c$.
\item On the one hand $L^{\M',\psi}(x)\le K(f)+|\epsilon|\le K(x)+c$. On the other hand, $L^{\M,\psi}(x)$ is an effective description of $x$, so $K(x)$ is at most a constant larger.
Together, these inequalities establish the second condition.
\end{enumerate}

Then by Lemma~\ref{lem:thecoolone} there is an enumeration $\phi_1,\phi_2,\ldots$ such that $S^{\M,\phi}(x)=|\bar 0|+S^{\M',\psi}(x)$. We observed that $|K(f)-K(x)|\le c$ for any $f\in\M'$, so $S^{\M',\psi}(x)\ge K(x)+c$. This proves the lemma.
\end{proof}

In short, this lemma tells us that there are numberings for which the singleton models always determine the sophistication. This means that for any dataset, the sophistication is equal to the Kolmogorov complexity. 

Many variants in the literature are susceptible to this problem, including the structure function \cite{cover1985kolmogorov,gacs2001algorithmic}. \cite{vitanyi2004meaningful} and \cite{adriaans2012facticity} circumvent this problem by using non-self-delimiting representations. Since a singleton representation places all information in the (self-delimiting) model part, singletons are automatically less efficient than representations placing some information in the input. While this side-steps the issue of the singletons, there is no suggestion that this makes the sophistication invariant.

Another solution would be to make the constant in the threshold function dependent on the choice of enumeration. We are not aware of any approach to this effect, and there does not seem to be a simple way to determine the thresholkd based on the properties of the numbering.

\section{The problem of depth}

We have seen that some variants of sophistication sidestep the simplest examples of underfitting by limiting the model class to exclude universal models. While this eliminates the simplest proof of boundedness, it does not follow immediately that the sophistication based on a limited model class must be bounded. In fact, a well-known result in algorithmic statistics shows that there exist strings for which no finite set is as good a model as the singleton set. We will show the consequences for sophistication:

\begin{theorem}
There are infinitely many $x$ such that $S^\F_K(x) \geq K(x)/5$. \label{theorem:structure-function-is-not-bounded}
\end{theorem}
\begin{proof}
From \cite[Proposition~I.3 (b)]{gacs2001algorithmic} we know that for sufficiently large $i$ there are strings of length less than $i$ that are not $(i/4, i/4)$-stochastic. For each $i$, let $x_i$ be the smallest binary string that is not $(i/4, i/4)$-stochastic. Since every string is $(\alpha,\alpha)$-stochastic for sufficiently large $\alpha$, this sequence contains infinitely many distinct binary strings. Now pick any $i$ that is large enough that (a) $|x_i|<i$ and (b) $K(x_i)/5 < i/4$. By definition of $(\alpha,\beta)$-stochasticity, if a string is not $(i/4,i/4)$-stochastic, then for every finite set $S$ containing $x$, either (a) $\log|S|\ge K(x_i)+i/4$, in which case $S$ cannot be a candidate representation for $x$ in $C^\F_K$, or (b) $K(S)>i/4>K(x_i)/5$. This proves the theorem.
\end{proof}

\begin{lemma}
Let $f$ be a total, computable function, such that for some $d$, $f(d) = x$ and let $k = K(f) + |d|$. Then there exists a finite set $S$ containing $x$ and a constant $c$, such that $K(S) \leq K(f) + K(|d|) + c$ and $\log |S| \leq |d|$.\label{lemma:total-to-sets}
\end{lemma}
\begin{proof}
Let $S = f\left(\{0,1\}^{|d|}\right)$. Since $f$ is total, this set can be explicity computed from a description of $f$ and a description of $|d|$, which tells us that there is a constant $c$ such that $K(S) \leq K(f) + K(|d|) + c$. 

Since $S$ is the image of a set of size $2^{|d|}$ under $f$, we have $\log |S| \leq |d|$.
\end{proof}
This lemma is a variation on \cite[Lemma~7.2]{vitanyi2004meaningful}.

We use this lemma to transport the result of Theorem~\ref{theorem:structure-function-is-not-bounded} to the model class of total functions:

\begin{theorem}
There are infinitely many $x$ with 
\begin{enumerate}
  \item $S^\T_K(x) \geq K(x)/6$, {and}\label{eq:poezenvoer}
  \item $S^\T_{\tau^\circ}(x) \geq K(x)/6$. \label{eq:hondevoer}
\end{enumerate}
\end{theorem}
\begin{proof}
We continue from the proof of Theorem~\ref{theorem:structure-function-is-not-bounded}. Suppose towards contradiction that either $C^\T_K(x_i)$ or $C^\T_{\tau^\circ}(x_i)$ contains a candidate $f$ with $K(f)\le i/5$. Then by Lemma~\ref{lemma:total-to-sets}, there is a set $S$ with $\log|S|\le|d|\le\tau^\circ(x)\le K(x)$, so (a) in the proof of Theorem~\ref{theorem:structure-function-is-not-bounded} above cannot be the case, and $K(S)\le K(f)+K(|d|)+c\le i/5+K(|d|)+c$, which contradicts (b) in the same proof for large enough $i$. Therefore such candidates $f$ cannot exist and all candidates must satisfy $K(f)>i/5>K(x_i)/6$.
\end{proof}
Note that we favoured simplicity of the proof over the strength of the bounds: the multiplicative constant $1/6$ can be improved with a more detailed treatment.

This tells us that for the right strings, a sophistication so defined can truly grow to a significant part of the total description length. While this shows that sophistication, if carefully defined, is not entirely vacuous, we can shed some light on exactly which strings end up with high sophistication, using the principle of depth. We will use the following definition:

\begin{definition}[depth]
Let $U$ be some universal Turing machine, so that $U(\bar\imath y) = \phi_i(y)$. Let $U^t$ be a simulation of this machine, which is allowed to run for at most $t$ steps, and returns $0$ if it has not yet finished at that point. Let $C^t_\M(x) = \min{|\bar\imath y| : U^t(\bar\imath y) = x, \phi_i \in \M}$.

The \emph{$c$-depth} $d^\M_c(x)$ of a string is defined as:
\[
	d^\M_c(x) = \min \left\{t : C^\M_t(x) - C^\M(x) \leq c \right\}
\] 
\end{definition}

Thus, deep strings are those that can only be optimally compressed with a great investment of time. The notion of depth was first proposed by Bennett \cite{} and later refined to variants of this form \cite{}.

\begin{theorem}
Let $A(n)$ be the single-argument Ackermann function and $c$ some arbitrary constant. There are numberings such that for all strings with depth $d^\C_c(x) \leq A(C(x))$, $\s^\T(x)$, $s^\P(x)$ and $\s^\F$ are bounded.
\end{theorem}
\begin{proof}
Let $U(\bar\imath y)$ be some (non-prefix) universal Turing machine, and let $U^A(\bar\imath y)$ be a simulation of that machine which outputs $0$ if the number of steps taken exceeds $A(|\bar\imath y|)$. Let $u$ be the index of the function $U^A$ in the standard enumeration.

Let $D$ be a prefix function with $D(\epsilon) = u$. We can instantiate Lemma~\ref{lemma:thecoolone} with $D$, $\M' = \{\phi_u\}$ and $X = \{x : d^\C_c(x) \leq A(C(x))\}$. This tells us that there exists a numbering for which $\s^\T(x) = \s^{\M'}(x) + |\bar0| \leq c$ for all $x \in X$.
\end{proof}

This shows that while high-sophistication strings exist for variants with limited model class, the mechanics do not follow the basic intuition of sophistication. Unless we encounter data that would take longer than the lifetime of the universe (and then some) to produce, sophistication behaves exactly as it would without the restricted model class: a solution to a superexponential problem would have no greater sophistication that a string sampled from a simple Bernoulli model.

\section{Conclusion}

As we have seen, the idea behind sophistication has a rich history. It has been proposed many times, by many different authors: sometimes building on the work of others, sometimes proposing it independently. This tells us two things. Firstly, there is a strong intuition behind the idea of sophistication, a sense that it can be both meaningful and properly defined. Secondly, it shows that it is not an easy intuition to pin down in an unambiguous manner.

So why should we consider this intuition reasonable at all? The starting point behind sophistication is that high structure and high randomness are objectively `uninteresting'. A more sophisticated signal, such as a television broadcast must contain, in some objective sense, more meaningful information. Whether this intuition seems reasonable likely differs per person, but sophistication goes one step further: it also posits an unambiguous separation into structure and randomness. Not for nothing is the sophistication also known as the \emph{algorithmic minimal sufficient statistic}.

Let us consider whether an unambiguous separation of structure and noise is valid. Imagine a prefix-free universal Turing machine, to which we feed random bits whenever it reads from its input tape. We continue doing so until it halts (or indefinitely as the case may be). In other words, we are sampling from $m(x)$, the universal distribution. What can we say about the resulting data? If the universal Turing is constructed in the conventional manner, it will first sample random bits to select a non-universal Turing machine, and then sample an output from that. What's the characteristic model of this data? Certainly, we cannot exclude the possibility of a universal model. Also, we cannot exclude the possibility that the model actually selected was a singleton model. Ultimately, selecting that model that was actually used in generating the data comes down to guessing where the model ended and the input began. In an unbounded scenario, this is an entirely arbitrary choice.

It must be said that such probabilistic views are strongly discouraged by the proponents of algorithmic statistics, mostly due to the sentiment expressed earlier in the words of Kolmogorov. Information theory, should precede probability, not the other way around. Nevertheless, this sampling scenario somewhat undercuts the intuition behind sophistication: there is no reason to assume that a string with high sophistication was sampled from a complex model. Repeated samples offer an unambiguous golden standard, seeing the same string a million times, strongly suggests a singleton model, 

For this reason, we advocate a skeptical view of sophistication. In particular the notion that it can be made invariant must be mentioned and investigated skeptically. We offer no generic proofs to the contrary, be we do offer the following insight. As we have seen, candidates within a constant of the optimal representation can occur all over the spectrum from small to large models. A small change in construction, such as a different acceptable numbering, or a change in threshold function, can arbitrarily push models into or out of the candidate set. Whereas in Kolmogorov complexity constant changes have constant effects on the outcome, here constant changes can cause very large changes in the outcome.

The most common solution, restricting the models to be total, is problematic. We have show that the behavior of the resulting function does not correspond with the intuition behind it. Additionaly, we must not that this restriction seems arbitrary. What makes the total functions better than any other model class? If it is an arbitrary choice, then the sophistication should be invariant to changing it to some other value. If it isn't, which model classes are permissable and why? Finally, we note that the use of total functions has a very subtle effect on the discussion surrounding sophistication. Since the total functions are not enumerable, the value of the sophistication relies strongly on the difference between the Komogorov complexity and the 



\subsubsection*{\ackname}

This publication was supported by the Dutch national program COMMIT and by the the Netherlands eScience center.

\bibliographystyle{plain}
\bibliography{facticity}

\appendix

\section{=======Stuff to move up or delete=======}

\section{Old structure}


\subsection{Koppel and Atlan}

We start with sophistication as originally introduced by Koppel and Atlan \cite{koppelSoph1988,koppel1991almost}. Their sophistication is built on a two-argument universal Turing Machine $U(i, y) = \phi_p(y)$. They consider a pair $(i, y)$ a representation of a string $x$ if (a) $\phi_i$ is total, and (b) $\phi_i(y)$ has $x$ as a prefix. The length of a representation is measured as $|i| + |y|$. Note that this ignores the cost of delimiting the model $i$ from the input $y$, ie. this is not the length of a single unambiguous code word for $x$. Candidates are those models with representations $c$ bits from the minimum length, where $c$ is a pre-set `significance' parameter. 

\footnotemark 

Koppel and Atlan actually provide a machine-independence result, but only in the setting of infinite strings. 

\footnotetext{The original sophistication also allows infinite binary strings in the range of the computable functions considered. We ignore this aspect for the sake of simplicity, although our conclusions still hold with such functions included.}

This construction shows one of the first pitfalls of sophistication: using an inefficient representation for the model:


\subsection{Antunes's Coarse Sophistication}

\begin{itemize}
\item Model class is $\T$
\item Description method $L(m)$ for models is unclear, but appears to be
  taken from Koppel, i.e. arbitrary but self-delimiting.
\item Sophistication is defined slightly differently: $S^\T(x)=L(m^*)$ where $m^*=\argmin_m L(m)+L^m(x)-C(x)$. Thus, there is no slack.
\end{itemize}


\subsection{Adriaans' Facticity}

\begin{itemize}
\item Model class is $\C$
\item Description method for models is $L(m)=|\bar\imath|$ where
  $\phi_i=m$. The index function is assumed to be \emph{faithful}, see
  Definition~\ref{def:faithful}
\item Threshold function is $\tau(x)=L^\M(x)$, the constant in the candidate set is $0$.
\item Facticity is actually defined as index length without prefix
  encoding:  $S^\C_0(x):=\min\{|i|:\phi_i\in C^\C_0(x)\}$. But this
  difference is not important for the discussion.
\end{itemize}




\subsection{Effective complexity}

\begin{itemize}
\item Model class is \emph{ensembles}. Equivalent to $\K$. Subsequent literature suggests that ensembles have finite support, which would mean that effective complexity reduces to the structure function.
\item Description method for models is $L(m) = K(m)$ (the minimization is not mentioned explicitly, but we'll give them the benefit of the doubt).
\item The threshold function is equal to the minimum two-part code: $\tau(x)=L^m(x)$.
\item Gell-mann and Lloyd acknowledge that representations exist with small and large models. The choice from the candidate set is made by applying a time-bound to the computation from the string to the model, but not to the decoding of the model itself. The idea is that this forces `deep' information into the model, and shallow information into the data. Presumably, the representation with the smallest model code under this constraint is chosen.   
\end{itemize} 

\pb{\cite{ay2010effective} provides a more formal definition.}
\begin{theorem}
There exists a UTM so that for all $x$, $\s_\text{EC}(x) = K(x)$, up to a constant. 
\end{theorem}

Subsequent publications suggest that all ensemeble\ldots.  

\begin{theorem}[Reduction of Effective Complexity to Structure
  Function]
  There is a constant $c$ such that, for every representation of $x$
  consisting of an ensemble $E$ with $x\in E$ with entropy $H$,
  yielding a total information of $K(E)+H$, there is a finite set $S$
  with $x\in S$ such that $K(S)\le K(E)+c$ and $\log|S|\le H$.
\end{theorem}
\begin{proof}
  First note that although the ensemble $E$ may itself be infinite, it
  is ``typical'', and therefore a valid candidate representation, for
  only a finite number of its elements, namely the set $S=\{y\in E:-\log
  P(y)\le H\} = \{y:P(y)\ge 2^{-H}\}$. Since the sum of probabilities must
  converge, this set cannot be infinite: since $1\ge\sum_{y\in S} P(y)\ge
  \sum_{y\in S}2^{-H} = |S|2^{-H}$ so $|S|\le 2^H$.
\end{proof}



\subsection{Roll your own}


\begin{itemize}
\item Model class is either $\C$ or $\K$
\item Description method for models is $L(m)=K(m)$.
\item Threshold function is $\tau(x)=L^m(x)+c$, which matches $C(x)$
  resp. $K(x)$ up to a constant.
\end{itemize}

\paragraph{TODO: instantiate for UTM}

\begin{itemize}
\item Model class is $\K$
\item Description method for models is $L(m)=K(m)$.
\item Threshold function is $\tau(x)=L^m(x)+c$, which matches $K(x)$
  up to a constant.
\end{itemize}


\subsection{Kolmogorov's Structure Function}

\begin{itemize}
\item Model class is $\F$
\item Description method for models $L(m)=K(S)$ where $S$ is the range
  of $m$. There is some ambiguity as to the definition of prefix
  complexity for finite sets; discuss and refer to Shen \cite{TODO}.
\end{itemize}

\subsection{Vit\'anyi's Sophistication / Meaningful Information}

\begin{itemize}
\item Model class is $\T$
\item Description method for models is $L(m)=K(m)$
\item Candidate set is defined differently: $C_c=\{m\in\T:|L^m(x)-K(x)|\le
  c\}$. There appears to be a problem with this definition, because
  the two-part descriptions of $x$ may be shorter than $K(x)$ by
  exploiting the fact that the models do not need to be prefix functions.
\end{itemize}

% stru fu: model class is all finite sets
% code length L(m)+L_m(x)
% acceptability criterion acc(m) = 2L(m)+L_m(x)-C(x)
% 


\hide{

\subsection{Plan}

Perhaps surprisingly, it is also reinforced by Vit\'anyi, when he writes : (From Vereshchagin and Vit\'anyi, ``Kolmogorov’s Structure Functions and Model Selection'':)
\begin{quotation}
  This expression emphasizes the two-part code nature of Kolmogorov complexity. In the example
\[x=10101010101010101010101010\]
we can encode $x$ by a small Turing machine printing a specified number of copies of the pattern `10' which computes $x$ from the program `13'.” This way, $K(x)$ is viewed as the shortest length of a two-part code for $x$, one part describing a Turing machine, or model, for the regular aspects of $x$, and the second part describing the irregular aspects of $x$ in the form of a program to be interpreted by $T$. The regular, or “valuable,” information in  is constituted by the bits in the “model” while the random or ``useless'' information of $x$ constitutes the remainder.
\end{quotation}

\subsection{Conclusion}
Stel onze conjecture is waar, is het dan helemaal nutteloos? Nee, want lengte van two-part representations is nog steeds een redelijke maat voor counterevidence against a model. No-hypercompression, $p$-values, randomness deficiency. Probleem zit in harde cutoffs zoals minimalisatie en een vaste constante waarboven representaties niet meer meedoen. Daar gaat invariantie van kapot.

Onze mening: als onze conjecture waar is, dan moet je constraints weglaten: maak het makkelijk en doe prefix functies en alle partial recursive functions, en accepteer de collapse gewoon. Maar dat betekent weer dat je een hele wolk van two-part optimale representaties hebt waar je niet tussen moet willen kiezen.


Er is een gat tussen $K(f)+|y|-K(x)$ en $\delta(x\mid f)$. De laatste is de juiste maat voor de counterevidence tegen $f$.

\subsection{Thoughts and questions}

\begin{itemize}
\item Paul argues bleurg that the randomness deficiency of a model is a measure of the amount of structure left in the noise part. I feel that it is also a measure of the amount of counterevidence against a model in the statistical sense, as follows: the randomness deficiency of $x$ given model $M$ is $\delta(x|M)=L_M(x)-K(x|M)$. Suppose that model $M$ is true. We can then use the no hypercompression inequality to rewrite:
%
\[
P_{x\sim M}(\delta(x|M)\ge c) = P_{x\sim M}(L_M(x) - K(x|M) \ge c)\le 2^{-c}
\]
Thus, if $M$ is true the probability of a large randomness deficiency is exponentially small! This amount of evidence is robust in the sense that changing the UTM around will change the amount of evidence by at most a constant. If the structure function is going to give us a \emph{cloud} of candidate models instead of just a single one, I think this is a really nice interpretation of the meaning of that cloud: we simply measure the available evidence against all individual models.  

\item Shen explains the collapse of the structure function if you use enumerating complexity rather than listing complexity. How can we most cleanly explain that this collapse does not vanish for weaker model classes?

\item Properties of representations on $K(M)$ vs $L_M(x)$ graph: diagonals identify representations with the same two-part codelength. The randomness deficiency is the distance to the minimum, which is the $K(x)$ diagonal:
\[L(x;M)=K(M)+L_M(x) =K(M)+K(x|M)+L_M(x)-K(x|M)  = K(x)+\delta(x|M).\]
 Models move horizontally by a constant when the ordering of TMs is changed. Thus the randomness deficiency and two part code are also changed by at most a constant. In contrast the \emph{minimum} can now be achieved by a very different representation.

\item ``Power and peril of MDL'': bespreken ook de ``collapse of the structure function'' als de model class te sterk is, dwz sets worden te efficient gecodeerd (wsch equivalent met Shen's enumerating complexity).

\end{itemize}

\subsection{Dead darlings}

\begin{definition}
Given an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions, and an enumeration $\psi_1,\psi_2,\ldots$ of the partial recursive prefix functions. Let $\psi_u$ be a universal element with property $\psi_u(\bar\imath p)=\psi_i(p)$. Then define $v$ by
\[
v(a p)=\begin{cases}\phi_{\psi_u(a)}(p)&\tn{if $a$ is in the domain of $\psi_u$,}\\
\infty&\tn{otherwise}.\end{cases}
\]
\end{definition}
\begin{lemma}
The function $v=\phi_v$ as defined above is partial recursive.
\end{lemma}
\begin{proof}
Let $r$ be the input of $v$. Simulate $\psi_u$ using a TM with a one-way read-only input tape. If the TM does not halt, then enter an infinite loop. Otherwise, let $r=ap$ where $a$ is the part of the input tape that has been read when the TM halts, and let $i$ be the output of the TM. Now simulate $\phi_i$ on input $p$.
\end{proof}

\begin{lemma}
Assume the prefix function $\bar\cdot$ used in the definition of complexity is such that $\bar x-x$ is monotonically increasing. Let $\psi_1,\psi_2,\ldots$ be any acceptable numbering. Then for every $c$ there is another acceptable numbering $\phi_1,\phi_2,\ldots$ such that for all partial recursive functions $f$ we have
\[
K^\phi(f)-K^\psi(f) \ge c.
\] \label{lemma:building-block}
\end{lemma}
\begin{proof}
First note that there is a $c'$ such that 
\begin{equation}
	K(1^r 0  x) + c' \geq K(x) \label{eq:prelim1}
\end{equation}
for all $x$, since we can create a program that first runs the shortest program for $1^r 0 x$ and then strips the prefix $1^r 0$ from the result.

Define $\phi_{1^r0i}(x) = \psi_{i}(x)$, with $\phi_{j}(x) = \infty$ if $j$ does not contain a zero. Intuitively, any $\phi$-program is $r+1$ bits longer than the corresponding $\psi$-program. However, the used prefix function complicates this slightly. But all we need is the bound
\begin{equation}\label{eq:prelim2}\begin{split}
K^\phi(x) &= \min \{ |\overline{1^r 0 \imath}| y : \phi_{1^r 0 i}(y) = x \}\\
&  \ge \min\{|1^r0\bar\imath y| : \phi_{1^r 0 i}(y) = x \}\qquad\tn{(using assumption on $\bar\cdot$)}\\
& = r+1 +\min\{|\bar\imath y| : \psi_i(y) = x \}\\
& = r+1+K^\psi(x).
\end{split}\end{equation}

We can now relate the complexity of $f$ under the two enumerations as follows:
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(j) : \phi_j = f \right\}& \\
       &= \min\left\{ K^\phi(1^r0i) : \phi_{1^r0i} = f \right\}& \\
       &\ge  \min\left\{ K^\phi(i) : \phi_{1^r0i} = f \right\}-c'&\text{by~\eqref{eq:prelim1}}\\
       &\geq \min\left\{ K^\psi(i)  : \phi_{1^r0i} = f \right\}+r+1-c'& \text{by \eqref{eq:prelim2}}\\
       &= \min\left\{ K^\psi(i) : \psi_{i} = f \right\} + r+1 -c'& \\
       &= K^\psi(f) + r + 1 - c'.
\end{align*}
The proof is completed by choosing $r=c+c'-1$.
\end{proof}

\begin{definition}
  An enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive
  functions is called \emph{compact} if for all $i,y$ there is a $j$
  such that $|j|\le|i y|$ and $\phi_i(y z)=\phi_j(z)$ for all $z$.
\end{definition}

\begin{lemma}
There is a compact acceptable numbering.
\end{lemma}
\begin{proof}
  Let $\phi_1,\phi_2,\ldots$ be any acceptable numbering. Let $\psi_1,\psi_2,\ldots$ be a new acceptable numbering defined by $\psi_j(x)=\phi_i(y x)$ if $j$ is a pair satisfying $j=\bar\imath y$, and $\psi_j(x)=\infty$ otherwise.  We will show that $\psi$ is both $0$-compact and acceptable. 

To show compactness, pick any index $i$ and binary string $y$. First assume $i$ is a valid pair $i=\bar a b$. Then  $\psi_i(y x) = \psi_{\bar a b}(y x)=\phi_a(b y x)$, but also $\psi_{\bar a b y}(x)=\phi_a(b y x)$, so $\psi_i(y x) = \psi_j(x)$ for $j=\bar a b y$; so $|j|=|iy|+0$. On the other hand, if $i$ is not a valid pair, we can simply use $j=i$ so that $\psi_i(y x)=\psi_j(x)=\infty$ for all $x$.

To show that $\psi_1,\psi_2,\ldots$ is acceptable, we define total
computable functions $f$ and $g$ such that $\psi_i(x)=\phi_{f(i)}(x)$
and $\phi_j(x)=\psi_{g(j)}(x)$. The easy direction is
$g(j)=\bar\jmath\epsilon$. The definition of the mapping in the other
direction $f(i)$ depends on whether $i$ is a valid pair. If it is not,
then $f(i)=h$ for some $h$ such that $\phi_h(x)=\infty$ for all $x$. On the other hand if $i=\bar\jmath y$, then $f(i)$ is the index of a partial recursive function $\phi_{f(i)}(x)$ defined by a Turing machine that simulates the machine for $\phi_j$ but with $yx$ on the input tape.
\end{proof}


\begin{lemma}
Compact numberings are sort of faithful.
\end{lemma}
\begin{proof}
First, let $v$ be the index of the partial recursive function given by $\phi_v(\bar k p x)=\phi_{\phi_k(p)}(x)$. Let $\bar k p$ be a shortest program for $i$, that is $|\bar k p| = K(i)$ and $\phi_k(p)=i$. Then $\phi_i(x)=\phi_v(\bar k \bar p x)$. We now use compactness to find an index $j$ with $\phi_v(\bar k \bar p x)=\phi_j(x)$ and length
\[|j|\le |v\bar k\bar p|\le|v|+|\bar k p|+2\log|\bar k p|=K(i)+|v|+2\log K(i),\]
as required.
\end{proof}
}

\end{document}

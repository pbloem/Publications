\documentclass{style/llncs}

\usepackage{amsmath,amsfonts}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[mathscr]{eucal}

\newcommand{\M}{\mathscr M}
\newcommand{\C}{\mathscr C}
\newcommand{\T}{\mathscr T}
\newcommand{\F}{\mathscr F}
\renewcommand{\P}{\mathscr P}
\newcommand{\K}{\mathscr K}
\newcommand{\X}{\mathscr X}
\newcommand{\B}{\mathbb B}
\newcommand{\D}{\Delta}
\newcommand{\N}{\mathbb N}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\pair}[1]{\left\langle{#1}\right\rangle}
\newcommand{\concat}{\oplus}
\newcommand{\symb}[1]{\texttt{#1}}
\newcommand{\br}[1]{\overline{#1}}
\newcommand{\s}{\tn{soph}}
\newcommand{\num}[1]{#1_1, #1_2, \ldots}

\newcommand{\phiv}{\mathbf\phi}
\newcommand{\psiv}{\mathbf\psi}

\newtheorem{conj}{Conjecture}

\let\doendproof\endproof
\renewcommand\endproof{~\hfill\qed\doendproof}

\newcommand{\p}{\,\text{.}}

\newcommand{\tuple}[1]{\left\langle{#1}\right\rangle}

\newcommand{\hide}[1]{}

\newcommand{\sdr}[1]{\textcolor{blue}{\small #1\textsuperscript{[Steven]} }}
\newcommand{\pb}[1]{\textcolor{OliveGreen}{\small #1 \textsuperscript{[Peter]} }}

\newcommand{\argmin}{\mathop{\arg\min}}

% --- DELETE BEFORE SUBMUSSIONS ---
\pagestyle{headings} 

\title{Three Problems for Sophistication}

\author{Peter Bloem and Steven de Rooij}

\institute{
  System and Network Engineering Group, \\University of Amsterdam, the Netherlands\\
  \email{uva@peterbloem.nl, steven.de.rooij@gmail.com}
}

\begin{document}
\maketitle

\begin{abstract}
Kolmogorov complexity is a sound measure of the amount of information in a binary string, but it does not distinguish between the information that expresses the structural properties of the data, and noise. Kolmogorov's definition of the \emph{structure function} in his lecture Tallinn in 1973 was the first attempt to separate meaningful information from noise, but many alternative definitions of the \emph{sophistication}, or the amount of meaningful information in the data, have subsequently been proposed. This indicates that the idea is considered worth pursuing by multiple distinguished scholars, but at the same time it shows that the issue has never been completely satisfactorily resolved.

In this paper we describe three fundamental problems that arise with all existing proposals, showing each of them to be \emph{unsound}. Based on these results we we put forward the view that the problem is fundamental: while many nontrivial and useful structural properties of binary strings can be detected without a priori assumptions, it may be impossible to objectively quantify its sophistication.
\end{abstract}

\section{Introduction}

Kolmogorov complexity breaks away from the tradition in statistics and information theory by allowing the analysis of \emph{individual} binary sequences rather than studying probability distributions. This is important because for many binary strings, such as a binary encoding of a novel, or a holiday snapshot, or the fossil record, it is impossible to determine what distribution generates such objects. There is, however, a nagging discrepancy between Kolmogorov complexity and what most people whould intuitively call ``complex''. For example, a sequence of a million coin flips is exceedingly unlikely to have a Kolmogorov complexity of much less than one million bits, even though there is nothing complex about flipping a lot of coins---it's just hard work. For this reason, many scholars have defined alternative complexity measures in the spirit of Kolmogorov complexity, aimed at quantifying not \emph{all} information in a binary string, but only the \emph{meaningful} information in the data. This tradition started with a lecture by Kolmogorov in Talinn, Estonia in 1973, but many alternative definitions of meaningful information, or \emph{sophistication}, were proposed, see the related work section below. In this paper, we investigate three problems with these definitions, and show that they are unsatisfactory. We believe the problem to be fundamental, and provide several intuitive arguments why these attempts to define sophistication must fail.

The Kolmogorov complexity of a binary string $x$ is, roughly, the length of the shortest computer program that will output $x$. This length depends on the used programming language. However, the Invariance Theorem states that the choice between any two Turing complete languages affects the Kolmogorov complexity by at most a constant term: if $K^L(x)$ denotes Kolmogorov complexity of $x$ with respect to language $L$, then $|K^L(x)-K^M(x)|<c$ for some constant $c$ that depends on $L$ and $M$, but \emph{not on $x$}. This means that for sufficiently complex objects, the choice of programming language becomes irrelevant and Kolmogorov complexity becomes an \emph{objective} measure of the complexity of binary strings. The invariance theorem allows us to fix a single reference programming language and omit it from the superscript.

A definition of sophistication $S(x)$ in the spirit of Kolmogorov complexity should have similar robustness guarantees if it is to be interpreted as an objective property of a binary string. We are interested in a measure satisfying the following requirements:

\begin{enumerate}
\item Being an information measure, the sophistication should count the number of bits required for an effective description of the structural properties of a binary string.
\item An analogue of the Invariance Theorem should hold for sophistication: at the very least, there should be strict limits on the degree to which a change in the used programming language can affect the sophistication.
\item There should be no constant $c$ such that $S(x)\le c$ for every input sequence $x$. If sophistication is bounded, then knowing its value under one programming language provides no constraints on its value under another language, even for inputs of very high Kolmogorov complexity. Moreover, a bounded sophistication would be at odds with the intuition that there is no limit to the amount structure that a binary string can exhibit.
\item Similarly, there should be no constant $c$ such that the $K(x)-S(x)\le c$ for all $x$, because then sophistication would be equivalent to Kolmogorov complexity, and the sophistication would incorporate the amount of noise in the data just like Kolmogorov complexity does.
\end{enumerate}

For most proposed definitions of sophistication, we can prove that they fail one or more of the attributes above. Vit\'anyi's definition \cite{TODO} is the one exception that we cannot \emph{prove} to conflict with our requirements, but we can show that only extremely deep strings (i.e., strings that require an enormous amount of processing to construct) yield a large sophistication, while intuitively it seems there must be objects that contain a lot of structure all of which can be described without much calculation. Moreover in our discussion section we provide arguments that suggest that it should be possible to generalise our negative results to show that Vit\'anyi's sophistication is not invariant.

A valid definition of sophistication must contend with the following issues. First, while all definitions are based on a two-part encoding of the data sequence, some researchers use Kolmogorov complexity as a \emph{building block} of their proposed code, while others look for the two parts of the code \emph{inside} the Kolmgorov complexity: when the definition of Kolmogorov complexity is opened up, it turns out to consist of two parts itself. In Section~\ref{sec:nickname}, we show how this second approach sets us up to suffer from to the so-called \emph{nickname problem}: the sophistication becomes highly dependent on the chosen programming language. This immediately breaks invariance, unless additional requirements are made on the language. We also show that there do exist languages (efficient enumerations of the partial recursive functions) that address this issue.

The second issue is that of \emph{overfitting}, which we consider in Section~\ref{sec:overfitting}. Overfitting is a common problem in the statistical literature, that refers to the tendency of some model selection procedures to select a very complex model that provides a very good fit to the observed data, but does not generalise well to unseen data. For example, in a polynomial regression setting, a higher order polynomial can always provide better fit to a data set. Therefore, even if the data are in fact noisy samples from a parabola, naive model selection methods may select a much higher order polynomial that provides a worse description of the trend in the data. The same can occur for the sophistication. We somehow need to make sure that the two-part representation that determines the sophistication does not store noise in the ``structure'' part of the code. In statistics, this is typically addressed by penalising complex models, e.g. using the BIC criterion \cite{TODO}. However, such penalisations tend to break the delicate balance between structural information and noise, and lead to the opposite problem: underfitting.

Underfitting (Section~\ref{sec:underfitting}) is the opposite problem, where the selected model is simple, but fails to represent part of the structure in the data. It is an especially serious problem for sophistication because the considered models for sophistication are so powerful. In particular, in any programming language, there are programs that implement an interpreter for \emph{another} language. Such interpreters can generate \emph{any} data sequence with only a short input, and if complex models are penalized to avoid overfitting, then the problem becomes to make sure that interpreters are not \emph{always} preferred for complex data. This problem is widely recognised, and is sometimes referred to as the ``collapse'' of the structure function \cite{}. The usual workaround is to restrict the set of considered models such that interpreters are no longer allowed. For example, the model class may be restricted to the set of all total recursive functions (only programs that never hang) rather than all partial recursive functions (all programs). While this patches the ``collapse'' problem to an extent, it is questionable whether it adequately solves the problem of underfitting in general.

Finally, in the discussion in Section~\ref{sec:discussion} we argue that while two-part coding can yield useful insights into the structure of the data and allows us to identify some models as unlikely to capture all structure in the data, it is probably not possible to uniquely separate structure from noise and identify a \emph{single} model as ``best'': in general many models, of vastly different complexities, may be reasonable models of the data generating mechanism. Rather than doggedly trying to ``fix'' this property of algorithmic statistics, we propose embracing the idea that sometimes the data allows for multiple very different interpretations, and that there is no such thing as sophistication.

\subsection{Related work}
The idea of separating structure from noise originated with Kolmogorov's lecture in Tallinn, Estonia in 1973 about the \emph{structure function},. This work was never published, but summarized by Cover \cite{cover1985kolmogorov}.  In this theory, the two-part description of the data consists of a program that generates a finite set, and an index of the original object in the set. The length of the program then measures the structure in the data. This approach was discussed more extensively, and generalised, by Vereshchagin, G\'acs and Vit\'anyi \cite{vereshchagin2004kolmogorov,gacs2001algorithmic}). 

Koppel \cite{koppelSoph1988,koppel1991almost} also built on Kolmogorov and Cover's work to define a measure of the meaningful information in a binary string, called \emph{sophistication}, the term we adopt in this paper. He changed the class of models from finite sets to total functions and used monotone functions, instead of prefix-free Kolmogorov complexity, to be able to extend the theory to infinite strings, an approach which has been abandoned by subsequent authors. We will only consider the finite version (based on regular functions) in this paper.

In 1996 Gell-Mann and Lloyd introduced `Effective complexity' \cite{gellmann1996information}. Working from the perspective of statistical mechanics, they arrived (largely independently, it seems) at a notion that fits the mold of sophistication.

Antunes et al. \cite{antunes2009sophistication} note that the constant cutoff used by Koppel to separate candidate two-part descriptions from those deemed to be too inefficient, is a source of instability, a result in the spirit of this paper. They propose a different definition called \emph{coarse sophistication}. As we shall see, this version behaves very differently from regular sophistication, and is closer to computational depth.
 
Adriaans' \emph{Facticity}, introduced in 2012\cite{adriaans2012facticity}, relaxes the model class to include all partial recursive functions. Adriaans also identifies the nickname problem discussed in Section~\ref{sec:nickname}.

\subsection{Preliminaries, definitions and desiderata}

We will generalizes the main proposals for sophistication into a single, consistent notation. The unifying principle is this: we start with a class of models, we consider the space of two-part descriptions using these models, we select the set of representations with the shortest code-lengths and we select, by some method, the best model from this set. 

Let $\B = \{0,1\}^*$. We deal with partial functions $\phi: \B \to \B$, which we will also call \emph{models}. Such functions are \emph{computable} if they can be computed by a Turing machine as described in \cite[Definition~1.7.1]{li1993introduction}. A function is prefix-free if its domain, $D_f = \{y : f(y) \neq \infty\}$ is a prefix set. A function $f$ is \emph{total} if $D_f = \B$. A prefix-free function is \emph{complete} if every infinite binary string has a member of $D_f$ as a prefix. \footnotemark 

\footnotetext{Completeness is analogous to totality: a total function corresponds to a Turing machine that halts for all its inputs, while a complete function corresponds to a Turing machine that always halts, so long as there are enough bits on the input tape.}

As noted in the introduction, the Kolmogorov complexity is invariant to the choice of description language. Formally, our description language creates a numbering of the partial computable functions.  

\begin{definition}[Numberings]
A numbering is an enumeration of $\C$, denoted as $\phi_1, \phi_2, \ldots$ or by a function $g: \N \to \C$. We fix one canonical numbering $g^\circ$, which works as an effective description, that is for any $i \in \N, y\in \B$, we can effectively compute $(g^\circ(i))(y)$. We call a numbering $g$ \emph{acceptable} if there exist total, computable functions $a, b: \N \to \N$ as that for all $i$ $g^\circ(a(i)) = h(i)$ and  $g^\circ(i) = h(b(i))$.
\end{definition}

When we say that Kolmogorov complexity is invariant, we mean that changing the canonical acceptable numbering changes the Kolmogorov complexity by at most a constant. 

\begin{definition}[Prefix encoding function]
  A prefix encoding function $f:\B\to\B$ is an injective
  function that maps the binary strings to a prefix-free set. There
  are prefix encoding functions $g$ with the property that
  $|g(x)|\le|x|+2\log(|x|+2)|$ \cite{li1993introduction}. We fix such a function and
  denote it by $\br{x}=g(x)$.
\end{definition}

\begin{definition}[Complexity]
Let $\phi_1,\phi_2,\ldots$ be the canonical acceptable enumeration of $\C$. Let $\Pi$ be a computable subset of the natural numbers such that  $\{\phi_i:i\in\Pi\}$ corresponfs to the set of all computable prefix functions. Now define Kolmogorov complexity by
\begin{align*}
C(x)&=\min\{|\bar\imath y|:\phi_i(y)=x\},\\
K(x)&=\min\{|\bar\imath y|:\phi_i(y)=x, i\in\Pi\}.\\
\end{align*}
\end{definition}
This definition of $K$ is uncommon. It is equivalent to the standard definition as used in \cite{li1993introduction}, up to a constant, but this form is more convenient for defining the $K$-complexity of non-prefix functions (see Definition~\ref{definition:model-complexity} below).

We now have the basic tools with which we set up our generic sophistication function. We start with the class of allowed models:

\begin{definition}[Models and model classes] A \emph{model class} is a set of models. We distinguish four important model classes:
  \begin{itemize}
  \item $\C$ is the set of all partial computable functions,
  \item $\K\subset\C$ is the set of all prefix-free
    functions,
  \item $\T\subset\C$ is the set of all total functions,
  \item $\P\subset\K$ is the set of all complete, prefix-free functions,
  \item $\F\subset\P$ contains, for every finite set $S$, a computable
    surjective function mapping the binary sequences of length
    $\lceil\log S\rceil$ onto $S$. In other words, this is the model class of finite sets, with uniform distributions.
  \end{itemize}
\end{definition}

We note that the model classes $\T$ and $\P$ are unusual in that they are not enumerable.

\begin{definition}[Representations and code-lengths]
A pair $(m,y)$ is a \emph{representation} for $x$ if $m$ is a model and $m(y)=x$. 

Define the following code-length functions for the representations:
\begin{itemize}
\item $L(m)$ is the length of a self-delimiting description of
  the model $m$, i.e. it is the length function of a prefix code for models. Each variant of sophistication must supply such a function.
\item $L^m(x)=L(m)+\min\{|y|:m(y)=x\}$ measures the number of bits required to describe both the model $m$ and the data $x$ using the model.
\item $L^\M(x)=\min\{L^m(x):m\in\M\}$ measures the number of bits
  required to describe $x$ using the best model in the model class.
\end{itemize}
\end{definition}
We can now define which of these representations express the data most efficiently:
\begin{definition}[Candidate set]
The \emph{candidate models} for $x$ from model class $\M$ and threshold function $\tau$ are
\[
  C^\M_\tau(x):=\{m\in\M:L^m(x)\le \tau(x) + c\}
\]
with $c$ a constant independent of $x$ which may be chosen arbitrarily. The default threshold function is $\tau^\circ(x)=L^\M(x)$. We will often omit the subscript if the default threshold function is used. 
\end{definition}

\begin{definition}[Sophistication]
  The model complexity, meaningful information, effective complexity, facticity or \emph{sophistication},  of a string $x$ given a model class $\M$ and threshold function $\tau$ is
  \[
  \s_\tau^{\M}(x):=\min\{L(m):m\in C^\M_\tau(x)\}\p
  \]\label{definition:sophistication}
\end{definition}

Finally, we must formalize what we mean when we say that we desire sophistication to be invariant. For the Kolmogorov complexity, we know that a change in acceptable numbering causes no more than a constant change. It may be that a less stringent invariance (for instance a $\log$ term) can still yield a worthwhile definition of sophistication. To allow such distinctions, we generalize the notion of invariance:

\begin{definition}
Let $\gamma:\N\to \N$ be some nondecreasing function. Two functions $f$ and $g$ are $\gamma$-equivalent if $|f(x) - g(x)| \in O(\gamma(\max(f(x), g(x)))$.
\end{definition}

We say that a formalization of sophistication $s_\phi$ built on acceptable numbering $\phi$ is $\gamma$-invariant to the choice of numbering is $s_\phi$ and $s_\psi$ are $\gamma$-equal for any two acceptable numberings $\phi$ and $\psi$. Thus, by this definition, Kolmogorov complexity is $1$-invariant.

\section{Inefficient indices}

We start from a simple intuition: the definition of Kolmogorov complexity already uses a description of the data in two parts; a computable function and and input. Can we assume that when such a description is as short as the Kolmogorov complexity, it automatically creates a good separation into the structural part of the data and the noise? As we will show, the representation of the structural part plays a fundamental part in answering this question. If it is in any way inefficient, the resulting sophistication function does not behave as expected.

The issues can be very subtle and require a careful attention that is not always afforded in the literature. We will first illustrate what happens if any acceptable numbering is allowed:

\begin{lemma}
Let $\phi$ be some acceptable numbering, and $\s_\phi(x)$ be any function which gives the length of some index $i$ such that $\phi_i(y) = x$ for some $y$.

No such function can be both $\gamma$-invariant and unbounded for any $\gamma(n) \in o(n)$.
\end{lemma}
\begin{proof}
Choose some $\gamma(n) \in o(n)$. Assume towards contradiction that the sophistication is $\gamma$-invariant and unbounded.

Let $\num{\phi}$ be the canonical acceptable numbering. Let $s_i$ be the binary string consisting of $2^{i}$ zeroes followed by a one.

We now define two new acceptable numberings $\num{\psi}$ and $\num{\xi}$:
\[
\psi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s_{2i}\\
	\infty\;\text{otherwise}
\end{cases}
\]
and 
\[
\xi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s_{2i+1} \\
	\infty \;\text{otherwise}\p
\end{cases}
\]
Let $\s_\psi$ and $\s_\xi$ be the sophistication functions built on these enumerations. Choose any $x$ and assume w.l.o.g. that $\s_\psi(x) < \s_\xi(x)$. By construction of the numberings $\psi$ and $\xi$, we have $2\;\s_\psi(x)  \leq \s_\xi(x)$, which gives us:
\begin{align*}
	\s_\xi(x) - \s_\psi(x) &\geq \frac{1}{2}\, \s_\xi(x)  \in \Omega(\s_\xi(x))\p 
\end{align*}
Since, by assumption, $\s_\xi$ can be arbitrarily large and $\s_\xi(x) - \s_\psi(x) \in O(\gamma(\s_\xi(x))) \subset o(\s_\xi(x))$, we have a contradiction.
\end{proof}

In this proof, we deliberately used ridiculously inefficient index functions. Obviously, one could simply pick a more efficient index; the point is that many definitions of sophistication do not mention any such restriction. As we shall see, any inefficiency in the index will lead to a lack of invariance in sophistication.

The treatments that fall prey to this problem are Koppel and Atlan's \cite{koppelSoph1988,koppel1991almost},  and several of its variants (\cite{antunes2009sophistication,antunes2013sophistication}). 

Interestingly, using an inefficient index is no problem in the definition of Kolmogorov complexity, since we can select as a model a universal Turing machine with an efficient index at only a constant penalty (by the Invariance Theorem \cite{li1993introduction}). But this robustness property does \emph{not} carry over to definitions of sophistication. If we are interested in separating the model and its input, we must represent the model efficiently. 

Another perspective on this problem is that the length of a model in an ordinary description does not represent its complexity, much like the length of a compressible string does not reflect its information content. In order to obtain a more robust measure of model complexity, we describe a natural extension of Kolmogorov complexity to functions, and show that this retains invariance.

\begin{definition}
  The complexity of a partial recursive function is defined by
  \begin{align*}
    C_\phi(f) &:= \min\{C(i):\phi_i=f\}\\
    K_\phi(f) &:= \min\{K(i):\phi_i=f\}.
  \end{align*} \label{definition:model-complexity}
\end{definition}
These definitions also appear in \cite{grunwald2004shannon} and \cite{vitanyi2004meaningful}. We will show that these values are invariant up to a constant (ie. complexity is an objective property of a function):
\begin{lemma}[Invariance]
Let $\phi_1, \phi_2, \ldots$ and $\psi_1, \psi_2,\ldots$ be two acceptable numberings. There exists a constant $c$ such that $\left| K^\phi(f) - K^\psi(f)\right | \leq c$ for all $f$. \label{lemma:invariance}
\end{lemma}
\begin{proof}
Let $g(i)$ be the function which which translates from $\psi$ to $\phi$.
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(i) : \phi_i= f\right\} \\
&\geq \min\left\{ K^\psi(i) : \phi_i= f\right\} - c\\
&= \min\left\{ K^\psi(i) : \psi_{g(i)}= f\right\} - c\\
&\geq \min\left\{ K^\psi(i) : \psi_i= f\right\} - c\\
&= K^\psi(f).
\end{align*}
We can reverse $\phi$ and $\psi$ to achieve the same inequality the other way around, completing the proof.
\end{proof}
This lemma shows that the complexity of a function does not depend on the choice of
enumeration by more than a constant term.

There are two ways to use this notion of model complexity for more robust attempts to define sophistication. Confusingly, both approaches are used in the literature,
\begin{enumerate}
  \item One can stick with the approach of using the model used by the representation that witnesses the Kolmogorov complexity, but repair it by imposing the additional constraint that an efficient numbering is used. Specifically, that the length of the indices satisfies $|i| = C(\phi_i)$, and that the numbering is still acceptable. We construct such an index below. This approach is taken by Adriaans \cite{adriaans2012facticity}. In this case, the total code length is simply $C(x)$, and the sophistication is the length of the model index within this code. This approach is the most direct formalization of the intuition that the two part code used within the definition of Kolmogorov complexity separates structure from noise.
  \item On the other hand, rather than distinguishing the two components of the code \emph{within} the definition of Kolmogorov complexity, we can also use Kolmogorov complexity as a black box to measure the complexity of the \emph{model}, which is then the first part of a two-part code describing the data. With this approach, the total code length is $K(f)+|y|$, where $y$ is an input to the model $f$ such that $f(y)=x$. The difference is that in this approach, we can construct everything on the basis of a single acceptable numbering, without using additional contraints. It is also more natural in this setting to limit the model class of allowed functions. This approach is used for Kolmogorov's structure function \cite{cover1985kolmogorov}, Algorithmic Statistics \cite{gacs2001algorithmic}, Vi\'anyi's sophistication \cite{vitanyi2004meaningful} and Effective complexity \cite{gellmann1996information}.
\end{enumerate}

To show that the two approaches are equivalent with the correct restriction on the index, we must show that there exists an acceptable numbering for which the length of the index of a model is equal to its complexity.

\begin{definition}[Faithful Numbering]\label{def:faithful}
  A numbering $\psi_1,\psi_2,\ldots$ of the partial recursive
  functions is \emph{faithful} if there is a constant $c$ such that
  for all indices $i$ there is a $j$ such that $\psi_i=\psi_j$ and
  $|j|\le C(\psi_j)+c$.
\end{definition}
This definition is a refinement of the one used in \cite{adriaans2012facticity}. That publication defines an \emph{index function}, rather than a numbering, and notes that it is not computable. Since a numbering is all that is needed, we only need to show acceptability. The incomputability of the index function is no issue:
\begin{lemma}
  There is a faithful acceptable numbering.
\end{lemma}
\begin{proof}
Let $\tn{div} \in \N$ be an index such that $\phi_{\tn{div}}(y)=\infty$ for all $y$. Define
  \[\psi_q=\begin{cases}
    \phi_{\phi_i(p)}&\tn{if $q$ can be written as $\bar\imath p$ and $\phi_i(p)<\infty$,}\\
    \phi_{i_\tn{div}}&\tn{otherwise.}\end{cases}
  \]
  To show that $\psi$ is faithful, pick any function $f$. Then
\[\begin{split}
C(f)&=\min\{C(i):\phi_i=f\}\\
&=\min\{\min\{|\bar a b|:\phi_a(b)=i\}:\phi_i=f\}\\
&=\min\{|\bar a b|:\phi_{\phi_a(b)}=f\}\\
&=\min\{|\bar a b|:\psi_{\bar a b}=f\}.
\end{split}\]
This shows there is a sufficiently small $\psi$ index.

To show that $\psi$ is acceptable, let $\phi_j$ denote the identity
function. Then a $\phi$-index $i$ can be mapped to a $\psi$-index
using the computable function $r(i)=\bar\jmath i$, so that
$\psi_{r(i)}(y)=\psi_{\bar\jmath i}(y)=\phi_i(y)$. For the reverse,
define $\phi_v(\bar\imath p, y)=\phi_{\phi_i(p)}(y)$. For fixed
$\bar\imath p$, the 
$s^n_m$-theorem \cite{kleene193notation} states that we can compute the $h$
such that $\phi_h(y)=\phi_v(\bar\imath p,y)$. Let $h(\bar\imath p)$
denote this index as a function of the program; further define
$h(q)=i_\tn{div}$ if $q$ cannot be expressed as $\bar\imath p$. By
construction $h$ is total and computable. To check that the mapping
returns the correct function, rewrite $\phi_{h(\bar\imath
  p)}(y)=\phi_v(\bar\imath p,y)=\phi_{\phi_i(p)}(y)=\psi_{\bar\imath p}(y)$.
\end{proof}

Unfortunately, even with a faithful index, we can fail, if the prefix-function used to separate the model from its input is inefficient:

\begin{lemma}
Let $L(\phi_i) = |\bar\imath|$ for a faithful index, and some prefix encoding function, and let $\s^\C$ be defined on this $L$ with the default threshold function.

If $\min\{i\ge i_0:|\bar\imath|-K(i)\}$ is an unbounded function of $i_0$, then $\s^\C$ is bounded for all numberings $\phi$.\label{lemma:prefix-inefficiency}
\end{lemma}
\begin{proof}
Let $\bar\imath y$ be any two-part representation for the data $x$, i.e. $\phi_i(y)=x$. Then construct an alternative two-part representation $\bar vi^* y$, where $i^*$ is the shortest $\psi_u$-program for $i$ such that $\phi_v(i^* y)=\phi_{\psi_u(i^*)}(y) = \phi_i(y)=x$. We compare the lengths of these two representations. Note that $|i^*|=K^\psi(i)$. Therefore,
\[
|\bar\imath p|-|\bar v i^* y| = |\bar\imath|-|\bar v| - |i^*| = |\bar\imath|-|\bar v|-K(i).
\]
By assumption there must be an $i_0$ such that the above expression is positive for all $i>i_0$. From this $i_0$ onwards, the second representation (using $v$) will have a shorter code length, so $\bar\imath p$ cannot achieve the minimum in the definition of the sophistication. Consequently, the sophistication is bounded by $\s(x)<|\overline{\imath_0}|$ for all $x$. 
\end{proof}
This result applies to Adriaans' facticity\cite{adriaans2012facticity}. In order to circumvent the problem, the UTM should should encode the model $f$ in $K(f)$ bits, relying on $K$'s own prefix encoding to delimit the model from its input. 

Note that the condition for this lemma applies to all commonly used prefix encoding functions. We conjecture that it applies to all possible computable prefix encoding functions:

\begin{conjecture}
For any partial computable prefix codelength function $L$, the function
\[
\min\{L(i)-K(i):i\ge i_0\}
\]
is unbounded in $i_0$.
\end{conjecture}

Lemma~\ref{lemma:prefix-inefficiency} shows that even if we have a faithful numbering, we can only use it in settings where we don't count the cost of delimiting the model from the input (such as \cite{koppelSoph1988}). In the cases where the data representations are taken as a single code word, the model-part, including delimiting information, must represent the model as efficiently as $K(f)$ does.  

In the rest of this paper, we will avoid these issues and take the second perspective: we will consider the representations as two-part descriptions $(f, y)$ with codelength $K(f) + |y|$, such that $f(y)=x$. 

\section{Overfitting: the singletons}

We now come to the main issue plaguing sophistication. In the naive setting, where we allow all prefix-free functions as models, and use the default threshold function, the candidate set always contains two representations that clearly do not provide a good separation into structure and content. The first is a universal model: a universal Turing machine which simply runs its shortest program for $x$. The second is a \emph{singleton model} a model which returns $x$ for any input. In the first representation all information about $x$ is stored in the second part of the two-part description. In the second representation all information is stored in the model. The model complexity suggested by these representations is either constant, or equal to the Kolmogorov complexity.

The key problem is that both can be present in the candidate set, and the formalization of sophistication must ensure that neither are selected. As we will show, in many settings, we can ensure that they are the \emph{only} models in the candidate set. The principle we use is that the threshold on the candidate set is determined by a constant, and a change in numbering affects model size by a constant. Thus, We can choose our numbering  so that all but a certain class of models take a constant penalty, pushing them out of the candidate set. Since we use this trick repeatedly we have generalized it into an (admittedly complex) lemma.

\begin{lemma}\label{lemma:thecoolone}
  Let $\psi_1,\psi_2,\ldots$ be any acceptable enumeration of the partial recursive functions.
  Let $\M$ be any model class, let $\X$ be any set of binary sequences and let $D:\{0,1\}^*\to\N$ be a computable decoding function with a prefix-free domain that maps function descriptions to their indices in $\psi$, i.e. if $f=\psi_{D(p)}$ then $p$ is a $D$-description of $f$. Let $\M'=\{\psi_i:i\in\tn{range}(D)\}$. Further assume there is a constant $c$ such that
\begin{enumerate}
  \item $\forall_{f\in\M'}:\min\{|p|:\psi_{D(p)}=f\}\le K^\psi(f)+c$
  \item $\forall_{x\in\X}:L^{\M',\psi}(x)-L^{\M,\psi}(x)\le c$.
\end{enumerate}
Then there is an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions such that $S^\phi_{\M}(x) = |\bar 0|+S^\psi_{\M'}(x)$ for all $x\in\X$.
\end{lemma}
\begin{proof}
We define the numbering $\phi$ as follows:
\[\begin{cases}
\phi_0(p) = 1^r 0 D(p) \\
\phi_{1^r0i}(p) = \psi_i(p) \\
\phi_j(\cdot) = \infty &\text{if $j$ contains no zeroes.}
\end{cases}\]
We will show that under the $\phi$-enumeration, the best representation for $x$ using a model $f\in\M'$ is always better than the best representation using some $f\not\in\M'$.

First suppose $f\in\M'$. Then
\[\begin{split}
K^\phi(f)&=\min\{K^\phi(i):\phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_j(q)=i, \phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_{\phi_j(q)}=f\}\\
&\le\min\{|\bar0 q|:\phi_{\phi_0(q)}=f\}~\text{(using $j=0$)}\\
&=\min\{|\bar0 q|:\phi_{1^r0 D(q)}=f\}\\
&=|\bar 0|+\min\{|q|:\psi_{D(q)}=f\}\\
&\le K^\psi(f)+c+|\bar 0|,
\end{split}
\]
where the last inequality uses the first assumption.


Now assume that the best model $f$ for $x$ is not in $\M'$. 
Let $i$ be the index of $f$ with the shortest description, i.e. it achieves the minimum in
\[
K^\phi(f)=\min\{K^\phi(i):\phi_i=f\}.
\]
There are two possibilities. Either $i=0$, in which case we have $K^\phi(f)\ge r$ because $\phi_0$ cannot output zero and all other $\phi$-programs are at least $r$ bits long.

Otherwise, we can bound
\[\begin{split}
K^\phi(f)&=K^\phi(i)=K^\phi(1^r0j)\\
&\ge K^\phi(j)-c'\\
&\ge K^\psi(j)+r+1-c'\\
&\ge\min\{K^\psi(j):\phi_j=f\}+r+1-c'\\
&=K^\psi(f)+r+1-c'.
\end{split}
\]
Now choose $r=c''+\max\{K^\psi(\phi_0),c'-1\}$. Then substitution yields, for both cases, 
$K^\phi(f)\ge c''+K^\psi(f)$.

Combining the inequalities above, for any model $g\not\in\M'$, there is a model $f\in\M$ such that
\[\begin{split}
K^\phi(g)+C^g(x) &\ge K^\psi(g)+C^g(x)+c''\\
&\ge K^\psi(f)+C^f(x)-c+c''\\
&\ge K^\phi(f)+C^f(x) -2c+c'' -|\bar0|.
\end{split}\]

By choosing $c''$ sufficiently large, we can ensure that the best representation is in $\M'$ for all $x\in\X$, which completes the proof.
\end{proof}

This is a technical and opaque lemma and proof, due to the double effect of the numbering, which occurs in both the definition of model complexity and the definition of data complexity. The \emph{idea} behind the lemma, however, is simple. Given a subset $\M'$ of the model class, we can choose the numbering so that representations use a model in $\M'$ perform an arbitrary constant better than all other models. This lets us effectively `push' all other models out of the candidate set, so that the sophistication is determined by a model in $\M'$.

We can now show that there exist numberings for which the sophistication always selects the singleton models:

\begin{lemma}[overfitting]
Let $\M \subseteq \K$ be a model class where for every $x\in\X$ there is a singleton model $f\in\M$ with $f(\epsilon)=x$. Then there is an enumeration $\phi_1,\phi_2,\ldots$ of the prefix partial recursive functions, and a constant $c$, such that
\[
K(x)-S^{\M,\phi}(x)\le c
\]
for all $x\in\X$.
\end{lemma}
\begin{proof}
Let $\psi_1,\psi_2,\ldots$ be any default enumeration of the partial recursive prefix functions. Note that since $f$ is a prefix function, if $f$ is defined for input $\epsilon$ then it cannot be defined for any other input. Pick $f,x$ with $f(\epsilon)=x$. Note that $x$ can be computed from $f$ and a fixed program, so there is a $c$ such that $K(x)\le K(f)+c$. Vice versa, given any $x$ we can construct an index of $f$, since $\psi$ is an acceptable numbering. Therefore $|K(f)-K(x)|\le c$.

We now define a computable function $D$ by $D(\bar\imath p)=j$ where $\psi_j(\epsilon) = \psi_i(p)$.  We will show that the two conditions of Lemma~\ref{lemma:thecoolone} hold for the prefix function $D$.

\begin{enumerate}
\item Let $f$ be any function in the range of $D$, and $x$ its output. Then $\min\{|p|:\psi_{D(p)}=f\}=\min\{|\bar\imath q|:\psi_i(q)=x\}=K(x)\le K(f)+c$.
\item On the one hand $L^{\M',\psi}(x)\le K(f)+|\epsilon|\le K(x)+c$. On the other hand, $L^{\M,\psi}(x)$ is an effective description of $x$, so $K(x)$ is at most a constant larger.
Together, these inequalities establish the second condition.
\end{enumerate}

Then by Lemma~\ref{lemma:thecoolone} there is an enumeration $\phi_1,\phi_2,\ldots$ such that $S^{\M,\phi}(x)=|\bar 0|+S^{\M',\psi}(x)$. We observed that $|K(f)-K(x)|\le c$ for any $f\in\M'$, so $S^{\M',\psi}(x)\ge K(x)+c$. This proves the lemma.
\end{proof}

In short, this lemma tells us that there are numberings for which the singleton models always determine the sophistication. This means that for any dataset, the sophistication is equal to the Kolmogorov complexity. 

Many variants in the literature are susceptible to this problem, including the structure function \cite{cover1985kolmogorov,gacs2001algorithmic}. \cite{vitanyi2004meaningful} and \cite{adriaans2012facticity} void this issue by using non-self-delimiting representations. Since a singleton representation places all information in the (self-delimiting) model part, singletons are automatically less efficient than representations placing some information in the input. While this side-steps the issue of the singletons, there is no suggestion that this makes the sophistication invariant.

Another approach may be to make the constant in the threshold function dependent on the choice of enumeration. We are not aware of any approach to this effect, and there does not seem to be a simple way to determine the threshold based on the properties of the numbering. We are not aware of such considerations being mentioned anywhere in the literature.

\section{Underfitting: the universal model}

On the other end of the spectrum, there exist candidate representations which place all information in the `noise part', using a universal function as the model. We can easily show that if such models exist, we can choose our numbering so that they are always chosen.

\begin{lemma}[underfitting]
Let $\M$ be a model class containing model $u$, called a \emph{universal model} with the following property
\[
\exists c \forall m \in \M : L^u(x) \leq L^m(x) + c \p
\]  
Then any $\s^\M$ with the default threshold function is either non-invariant or unbounded.
\end{lemma}
\begin{proof}
We will construct an acceptable numbering for which $\s^\M$ is bounded. This means that if $\s^\M$
 is either bounded for all acceptable numberings, or not invariant.
 
Let $D$ be a prefix function as in Lemma~\ref{lemma:thecoolone} such that it returns the index of $u$ for the argument $\epsilon$ and $\infty$ for any other argument. That is, $\M' = \{u\}$. The assumptions 1 and 2 from Lemma~\ref{lemma:thecoolone} follow directly from this construction, so that we can instantiate the Lemma. This tells us that there exists an acceptable numbering for which $\s_\M(x) = \s_{\M'}(x) + c$. Since $\M'$ contains only a single model, $\s_{\M'}$ must be constant, completing the proof.
\end{proof}

Most sophistication measures avoid the underfitting problem by limiting their model class, usually to either finite sets ($\F$) or total functions ($\T$ or $\K$). This eliminates the universal model, but as we shall see in the next section, this does not solve the problem for most data. The variants that are susceptible to the problem of underfitting are facticity \cite{adriaans2012facticity} and some versions of effective complexity \ref{gellmann1996information}. The latter aims to bypass the problem by selecting carefully from the candidate set, but as we have shown, for some numberings the candidate set contains only the universal model.

\subsection{The problem of depth}

We have seen that some variants of sophistication sidestep the simplest examples of underfitting by limiting the model class to exclude universal models. While this eliminates the simplest proof of boundedness, it does not follow immediately that the sophistication based on a limited model class must be bounded. In fact, a well-known result in algorithmic statistics shows that there exist strings for which no finite set is as good a model as the singleton set. We will show the consequences for sophistication:

\begin{theorem}
There are infinitely many $x$ such that $S^\F_K(x) \geq K(x)/5$. \label{theorem:structure-function-is-not-bounded}
\end{theorem}
\begin{proof}
From \cite[Proposition~I.3 (b)]{gacs2001algorithmic} we know that for sufficiently large $i$ there are strings of length less than $i$ that are not $(i/4, i/4)$-stochastic. For each $i$, let $x_i$ be the smallest binary string that is not $(i/4, i/4)$-stochastic. Since every string is $(\alpha,\alpha)$-stochastic for sufficiently large $\alpha$, this sequence contains infinitely many distinct binary strings. Now pick any $i$ that is large enough that (a) $|x_i|<i$ and (b) $K(x_i)/5 < i/4$. By definition of $(\alpha,\beta)$-stochasticity, if a string is not $(i/4,i/4)$-stochastic, then for every finite set $S$ containing $x$, either (a) $\log|S|\ge K(x_i)+i/4$, in which case $S$ cannot be a candidate representation for $x$ in $C^\F_K$, or (b) $K(S)>i/4>K(x_i)/5$. This proves the theorem.
\end{proof}

\begin{lemma}
Let $f$ be a total, computable function, such that for some $d$, $f(d) = x$ and let $k = K(f) + |d|$. Then there exists a finite set $S$ containing $x$ and a constant $c$, such that $K(S) \leq K(f) + K(|d|) + c$ and $\log |S| \leq |d|$.\label{lemma:total-to-sets}
\end{lemma}
\begin{proof}
Let $S = f\left(\{0,1\}^{|d|}\right)$. Since $f$ is total, this set can be explicity computed from a description of $f$ and a description of $|d|$, which tells us that there is a constant $c$ such that $K(S) \leq K(f) + K(|d|) + c$. 

Since $S$ is the image of a set of size $2^{|d|}$ under $f$, we have $\log |S| \leq |d|$.
\end{proof}
This lemma is a variation on \cite[Lemma~7.2]{vitanyi2004meaningful}.

We use this lemma to transport the result of Theorem~\ref{theorem:structure-function-is-not-bounded} to the model class of total functions:

\begin{theorem}
There are infinitely many $x$ with 
\begin{enumerate}
  \item $S^\T_K(x) \geq K(x)/6$, {and}\label{eq:poezenvoer}
  \item $S^\T_{\tau^\circ}(x) \geq K(x)/6$. \label{eq:hondevoer}
\end{enumerate}
\end{theorem}
\begin{proof}
We continue from the proof of Theorem~\ref{theorem:structure-function-is-not-bounded}. Suppose towards contradiction that either $C^\T_K(x_i)$ or $C^\T_{\tau^\circ}(x_i)$ contains a candidate $f$ with $K(f)\le i/5$. Then by Lemma~\ref{lemma:total-to-sets}, there is a set $S$ with $\log|S|\le|d|\le\tau^\circ(x)\le K(x)$, so (a) in the proof of Theorem~\ref{theorem:structure-function-is-not-bounded} above cannot be the case, and $K(S)\le K(f)+K(|d|)+c\le i/5+K(|d|)+c$, which contradicts (b) in the same proof for large enough $i$. Therefore such candidates $f$ cannot exist and all candidates must satisfy $K(f)>i/5>K(x_i)/6$.
\end{proof}
Note that we favoured simplicity of the proof over the strength of the bounds: the multiplicative constant $1/6$ can be improved with a more detailed treatment.

This tells us that for the right strings, a sophistication so defined can truly grow to a significant part of the total description length. While this shows that sophistication, if defined with particular care, is not entirely vacuous, we can shed some light on exactly which strings end up with high sophistication, using the principle of depth. We will use the following definition:

\begin{definition}[depth]
Let $U$ be some universal Turing machine, so that $U(\bar\imath y) = \phi_i(y)$. Let $U^t$ be a simulation of this machine, which is allowed to run for at most $t$ steps, and returns $0$ if it has not yet finished at that point. Let $C^t_\M(x) = \min{|\bar\imath y| : U^t(\bar\imath y) = x, \phi_i \in \M}$.

The \emph{$c$-depth} $d^\M_c(x)$ of a string is defined as:
\[
	d^\M_c(x) = \min \left\{t : C^\M_t(x) - C^\M(x) \leq c \right\}
\] 
\end{definition}

Thus, deep strings are those that can only be optimally compressed with a great investment of time. The notion of depth was first proposed by Bennett \cite{bennett1988logical} and later refined to variants of this form \cite{antunes2006computational}. It is important to note that it is exceedingly unlikely to sample deep string from a non-deep distribution. We note that it is exceedingly unlikely that a deep string is sampled from a shallow distribution \cite{bloem2014safe,bennett1988logical}.
 
\begin{theorem}
Let $A(n)$ be the single-argument Ackermann function and $c$ some arbitrary constant. There are numberings such that for all strings with depth $d^\C_c(x) \leq A(C(x))$, $\s^\T(x)$, $s^\P(x)$ and $\s^\F$ are bounded.
\end{theorem}
\begin{proof}
Let $U(\bar\imath y)$ be some (non-prefix) universal Turing machine, and let $U^A(\bar\imath y)$ be a simulation of that machine which outputs $0$ if the number of steps taken exceeds $A(|\bar\imath y|)$. Let $u$ be the index of the function $U^A$ in the standard enumeration.

Let $D$ be a prefix function with $D(\epsilon) = u$. We can instantiate Lemma~\ref{lemma:thecoolone} with $D$, $\M' = \{\phi_u\}$ and $X = \{x : d^\C_c(x) \leq A(C(x))\}$. This tells us that there exists a numbering for which $\s^\T(x) = \s^{\M'}(x) + |\bar0| \leq c$ for all $x \in X$.
\end{proof}

This shows that while high-sophistication strings exist for variants with limited model class, the mechanics do not follow the basic intuition of sophistication. Unless we encounter data that would take longer than the lifetime of the universe (and then some) to produce, sophistication behaves exactly as it would without the restricted model class: a solution to a super-exponential problem would have no greater sophistication than a string sampled from a simple Bernoulli model. This result also suggests that the `non-stochastic' property of strings with high sophistication\cite{shen1983concept,vereshchagin2004kolmogorov} says more about depth and totality than it does about the lack of randomness in its information content.

The relation between sophistication, depth and absolutely non-stochastic strings was also investigated in \cite{antunes2013sophistication}.

\section{Conclusion}

As we have seen, the idea behind sophistication has a rich history. It has been proposed many times, by many different authors. This tells us two things. Firstly, there is a strong intuition behind sophistication, shared by many respected authors, and second, it is not easy to get the definition exactly right.

So why should we consider this intuition reasonable at all? The first assumption of sophistication is that high structure and high randomness are objectively `uninteresting'. A more sophisticated signal, such as a television broadcast must contain, in some objective sense, more meaningful information. Whether this intuition seems reasonable likely differs per person, but sophistication goes one step further: it also suggests that we can objectively separate the information in a string into random and structured.

To see whether this assumption is valid; imagine a prefix-free universal Turing machine, to which we feed random bits whenever it reads from its input tape. We continue doing so until it halts (or indefinitely if it does not). In other words, we are sampling from $m(x)$, the universal distribution. What can we say about the resulting data? If the universal Turing machine is constructed in the conventional manner, the initial bits will determine the index of a partial computable function $i$, and the rest will be the input to that machine $y$. What's the characteristic model of this data? Certainly, we cannot exclude the possibility that $\phi_i$ is also a universal model. Also, we cannot exclude the possibility that $\phi_i$ is a singleton model for $x$. Ultimately, choosing the specific model that was actually used in generating the data comes down to guessing where the model ended and the input began.

This kind of perspective, with a probabilistic source for the data is strongly discouraged by the proponents of algorithmic statistics: they often argue that information theory should precede probability theory \cite{kolmogorov1983combinatorial}. Nevertheless, this sampling scenario undercuts the intuition behind sophistication: there is no reason to assume that a string with high sophistication was sampled from a complex model, so how can the complex model be intrinsic to the data, moreso than any other model that reaches the Kolmogorov complexity?

For this reason, we take a skeptical view of sophistication. The matter of its invariance, in particular cannot be glossed over. It should be the first question asked of any proposal. We can offer the following insight, to back up our skepticism. As we have seen, candidates within a constant of the optimal representation can occur all over the spectrum, from small to large models. A small change in construction, such as a different acceptable numbering, or a change in threshold function, can arbitrarily push models into or out of the candidate set. Whereas in Kolmogorov complexity constant changes have constant effects on the outcome, here constant changes can cause very large changes in the outcome. While no authors have touched on the issue of invariance explicitly, it is strongly related to the issue of instability, discussed in \cite{antunes2013sophistication,vereshchagin2013algorithmic}.

The most common approach, restricting the models to be total, is problematic. We have shown that the behavior of the resulting function does not correspond with the intuition behind it. We can also ask what justification there is for this restriction? What makes the total functions better than any other model class? If it is an arbitrary choice, then the sophistication should be invariant to changing it to some other reasonable class. If it isn't, which model classes are permissable and why? 

As we have seen, the restriction to a model class $\M$ tends to assign high sophistication to those strings that can be compressed significantly better by models outside $\M$ than by models within it. For the class $\T$, this results in extremely deep strings getting high sophistication. While this is an interesting property, which is worth investigating, the interpretation of an objective separation into model and noise is spurious at best. Instead of interpreting these strings as ``non-stochastic'', we consider it more natural to consider them \emph{non-typical for $\M$}.  

Finally, we note that the use of total functions has a very subtle effect on the discussion surrounding sophistication. Since the total functions are not enumerable, the value of the sophistication relies strongly on the difference between the Kolmogorov complexity and the Kolmogorov complexity bounded to total functions. This gap is exceedingly difficult to capture, because the total functions cannot be enumerated. 

This is dangerous: if such definitions cannot be proved wrong or right, they will stand as the ad-hoc official view, while the restrictions used to elude proofs of incorrectness make them very difficult to build on. This creates an artificial dead-end for what is at heart a useful and valuable topic of research.

\subsection{A world without sophistication}

It is too early to say that sophistication is an impossible ideal, but the points we have raised certainly show that it is riddled with subtle pitfalls, and inspired by questionable intuitions. We will conclude by asking what it would mean if we were to abandon this ideal entirely.

Of course the definition of Kolmogorov complexity still stands, and the space of two-part descriptions, ordered by strength of compression is still meaningful. If we give up on the ability to distinguish between representations that are close to the minimum, and consider them equivalent, we lose the ability to select a model, but we can still eliminate all those representation that are far from the Kolmogorov complexity. This is analogous to hypothesis testing in classical statistics. We can eliminate models but not confirm them. Even if we knew the Kolmogorov complexity of our data, there would still be equivalent representations we could not objectively  choose between. Given our choice of numbering we may hav an absolute favorite, but objectively speaking, we must accept all explanations that are `close enough' to the Kolmogorov complexity.

Consider the following metaphor. We are given a bitstring which can be read as a high-resolution bitmap image of the painting \emph{Impression of a Sunrise}. The candidate set for this painting contains various models from very generic to very specific. The theory behind sophistication suggests that we can choose one of these as the objective, intrinsic model of the data. Different models say different things: the universal models says that it is a `thing'. A more specfic model might say that it is an image. Even more specific would be a painting, a Monet, or specifically the painting Impression of a sunrise. These are the models we imagine we may find in the candidate set. Can we say that the data is intrinsically more of an image than a painting? More of a Monet than a painting? We see no intuitive reason why such a distinction should be possible.    

Of course, Kolmogorov complexity does allow us to say that it is unlikely to be a Jackson Pollock painting, or a piece of music. And, if we were to get a second sample from the same source, we \emph{could} answer the question: we would receive either the same painting again, another Monet, the same painting again, or simply another `thing', and we would gain information about the model. But a single sample by itself should give us no reason to choose an intrinsic model. Ultimately, this is just a metaphor, but since the notion of sophistication is driven by intuition, we feel that an intuition to the contrary should serve to sharpen the debate.

We note that these model-selection problems are not specific to Algorithmic statistics. They can largely be directly translated to the MDL and Bayesian frameworks. While proponents in these fields have never made explicit claims of asymptotically objective model selection, it is a sobering thought that when the model class is extended to all computable distributions, model selection on a single sample becomes arbitrary. (Among the candidate set)

Our conclusion is that in giving up sophistication, we do not lose algorithmic statistics, but rather clean up some of its more unmanageable components. Letting go of the minimal sufficient statistic will allow us to return to the simplicity of $\K$ as model classes. This gives us a simple framework, with a one-to-one relation to Bayesian statistics and MDL. 

We are left with the original question that sophistication was designed to answer. How can we distinguish pure randomness from intuitively interesting information. We note that the idea of depth is built on a more solid fundament. For instance, contrary to sophistication, encountering a string with high sophistication does imply, with high probability, that the source was of high depth also. Unfortunaly, depth is not measured in bits, and so moves us away from the idea that the useful complexity of a stirng should be measured in bits. It remains to be seen whether this is a fundamental issue, or whether the interesting information in a string can be objectively expressed in the same vein as it plain information content. 

\subsubsection*{\ackname}

This publication was supported by the Dutch national program COMMIT and by  the Netherlands eScience center.

\bibliographystyle{plain}
\bibliography{facticity}

\appendix


\end{document}

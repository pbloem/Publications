\documentclass{style/llncs}

\usepackage{amsmath,amsfonts,fullpage}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{conjecture}[theorem]{Conjecture}
%\newtheorem{definition}{Definition}

\newcommand{\M}{\mathcal M}
\newcommand{\C}{\mathcal C}
\newcommand{\T}{\mathcal T}
\newcommand{\F}{\mathcal F}
\renewcommand{\P}{\mathcal P}
\newcommand{\K}{\mathcal K}
\newcommand{\X}{\mathcal X}
\newcommand{\B}{\mathbb B}
\newcommand{\D}{\Delta}
\newcommand{\N}{\mathbb N}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\pair}[1]{\left\langle{#1}\right\rangle}
\newcommand{\concat}{\oplus}
\newcommand{\symb}[1]{\texttt{#1}}

\newcommand{\p}{\quad\text{.}}

\newcommand{\tuple}[1]{\left\langle{#1}\right\rangle}


\newcommand{\sdr}[1]{\textcolor{blue}{\small #1\textsuperscript{[Steven]} }}
\newcommand{\pb}[1]{\textcolor{OliveGreen}{\small #1 \textsuperscript{[Peter]} }}

\newcommand{\argmin}{\mathop{\arg\min}}

\title{A Critical Review of Sophistication and Model Selection in Algorithmic Statistics}
\author{Peter Bloem and Steven de Rooij}

\begin{document}
\maketitle

\begin{abstract}
\noindent Kolmogorov complexity is a sounds expression of the information of the information content of a binary string, but it leaves open the issue of \emph{meaningful information}. The strings with maximal information content are not the ones we would regard as interesting, or meaningful. 
One attempt to formalize a notion of meaningful information is sophistication. It is a simple idea with many attempted formalizations: describe the string efficiently with a two-part coding approach, and take the complexity of the model as a measure of the amount of meaningful information.
We review the variety of attempts to formalize this approach and highlight their shortcomings. We provide a critical perspective: we question whether any such formalization can truly be successful as an objective notion of meaningful information. A true property of the data, like Kolmogorov complexity. We show that no formalization currently proposed in the literature satisfies this demand. All sophistication functions are either bounded by a constant value, or are not invariant to the choice of universal Turing machine used in their construction.
\end{abstract}
\section{Introduction}

\noindent Kolmogorov complexity provides an objective measurement of the information content of a string: it is a property of the data, which does not depend on ad-hoc choices in its definition. Unfortunateldoes not answer the question of meaningful or interesting data. The strings that have minimal and maximal Kolmogorov complexity are the least interesting in an intuitive sense. The strings that contain useful information are somewhere in between.

Consider the following, illustrative example: an old-fashioned, analog television set which can be tuned to three frequencies. The first shows a blank screen, the second a regular program, and the third white noise. By the Kolmogorov complexity, these  broadcasts have increasing information content. The first is easiest to describe, the second contains a great deal of regularity, and the third cannot be compressed. However, in terms of meaningful information, information that we find interesting, the second signal is by far the richest. This notion has proved far harder to capture in formal terms than the simple notion of `raw' information content. One of the most frequently explored ideas is \emph{sophistication}.

Sophistication select a model for the data by two-part coding, and to take the complexity of the model as a measure of the meaningful information. The assumption is that both simple and random data have small models, and the more interesting a dataset is, the more complex its model will be. A key assumption of the approach of sophistication is that we can perform objective model selection for a given string. That is: there is a function that, given a string, selects a single unique model that can be used to compress this string optimally. Moreover, any elements of this function that are chosen ad hoc can be changed without significantly affecting the outcome.

In the case of the Kolmogorov complexity, this holds true; we can change the universal Turing machine used in its definition and the complexities returned will not change by more than a constant. This is known as \emph{invariance}. For this reason, we view the Kolmogorov complexity as an intrinsic property of the data, rather than simply a function computed on it. If a formalization of sophistication is ever to be held in the same regard as the Kolmogorov complexity, it must be subject to a similar principle of invariance. 

The unifying principle over all the notions we discuss in this paper is simple. We start with an object expressed as a finite binary string. We represent this object, hoping to compress it, in two parts, the first is a model, and the second an `input' to this model. How the two parts are encoded, and what the class of models is are details that differ between variants. We know, however, that for succesful representations, those that capture all structure in the string, the length of the two-part code should approach the Kolmogorov complexity. We take the set of all \emph{sufficient statistics}---those representations that are `close enough' to the Kolmogorov complexity---and call the one with the lowest model complexity the \emph{minimal sufficient statistic}. The size of the minimal sufficient statistic is the sophistication.

The intuition is that both very simple and very random data have small, simple programs: one with a small input and one with a large, random input. The strings with much structure and many interesting properties are those for which the best model is large. Two assumptions are crucial to this view:
\begin{itemize}
  \item This principle separates the structure and the noise into distinct parts. In the optimal representation the model contains only `structural information' and the noise contains only the `uninteresting' information.
  \item This principle is invariant. If we change, for instance, the universal Turing machine used for our representations, the minimal sufficient statistic will not change significantly.
\end{itemize} 

\subsection{Sophistication, Minimal Sufficient Statistic, Effective Complexity and Facticity}

We will first briefly discuss the different variants of sophistication in a historical context, to give the reader a simple overview, without discussing too much technical detail. In the main body of the paper, we will define each variant rigorously, and in non-chronological order.

The first suggestion that a representation of a string can be separated cleanly into a structural part, containing a description of its properties, and a noise part, containing the residual information, came from Kolmogorov himself, in a talk given at a conference in Tallinn in 1973 (first committed to paper by Cover \cite{todo} and later more extensively discussed by Vitanyi \cite{todo}). Here, he proposed what has become known as the \emph{Kolmogorov structure function}. Briefly, this function operates on a space of two part representations for a given string $x$, consisting of some finite set $S$ (containing $x$) and the index of $x$ in $S$. The \emph{minimal sufficient statistic}, it was proposed, is the set with the lowest complexity for which the length of the two-part representation reaches the prefix-free Kolmogorov complexity. \pb{Did Komogorov already talk about the MSS?}

Koppel \cite{} built on this notion, coining the phrase \emph{sophistication}. He changed the class of `models' from finite sets to total functions and used Levin's monotone complexity, instead of prefix-free Kolmogorov complexity. One interesting difference is that Koppel takes the description of the data under a variant of Kolmogorov complexity, which consists of a description of a model and that model's input, and takes the size of the model part as the sophistication. In contrast Kolmogorov's minimal sufficient statistic is built on a second, two-part, representation in addition to the Kolmogorov complexity's one-part representation. This difference in perspective is subtle but relevant to the discussion ahead.

Separately, G\'acs, Tromp and Vit\'anyi \cite{}, built on the idea of the structure function to establish a framework of \emph{algorithmic statistics}, in the hope of founding a statistical theory not on probabilistic notions, but on combinatorical ones. This echoes Kolmogorov's own hope for the theory of information:
\begin{quotation}
\noindent Information theory must precede probability theory and not be based on it. By the very essence of this discipline, the foundations of information theory must have a finite combinatorial character
\end{quotation} 







\pb{emphasize the different aims}
\pb{effective complexity}

\section{Definitions}


All numberings should be acceptable!

\begin{definition}[Models and model classes]
  A \emph{model} is a function $f:\{0,1\}^*\to\{0,1\}^*$. A
  \emph{model class} is a set of models. We distinguish four important
  model classes:
  \begin{itemize}
  \item $\C$ is the set of all partial recursive functions,
  \item $\K\subset\C$ is the set of all partial recursive prefix
    functions,
  \item $\T\subset\C$ is the set of all total recursive functions,
  \item $\P\subset\K$ is the set of all prefix functions such that for
    every infinite binary string the function is defined for a
    prefix. This is the natural analogue of totality in the prefix
  \item $\F\subset\P$ contains, for each finite set $S$, a computable
    surjective function maping the binary sequences of length
    $\lceil\log S\rceil$ onto $S$.
  \end{itemize}
\end{definition}

\begin{definition}[Prefix encoding function]
  A prefix encoding function $f:\{0,1\}^*\to\{0,1\}^*$ is an injective
  function that maps the binary strings to a prefix-free set. There
  are prefix encoding functions $g$ with the property that
  $|g(x)|\le|x|+2\log(|x|+2)|$ \cite{TODO}. We fix such a function and
  denote it by $\bar x=g(x)$.
\end{definition}

\begin{definition}[Complexity]
  Let $\phi_1,\phi_2,\ldots$ be a fixed acceptable enumeration of all partial
  recursive functions, e.g. the enumeration described in \cite{TODO}.
  Let $\Pi$ be a computable subset of the natural numbers such that
  $\{\phi_i:i\in\Pi\}$ contains all computable prefix functions. Now
  define Kolmogorov complexity by
\begin{align*}
C(x)&=\min\{|\bar\imath| y:\phi_i(y)=x\},\\
K(x)&=\min\{|\bar\imath| y:\phi_i(y)=x, i\in\Pi\}.\\
\end{align*}
\end{definition}
This definition of $K$ is equivalent to the definition in the
textbook~\cite{TODO}, but it is more convenient for defining the
$K$-complexity of non-prefix functions:

\begin{definition}[Representations]
  A pair $(m,y)$ is a \emph{representation} for $x$ if $m$ is a model
  and $m(y)=x$.
\end{definition}

\begin{definition}[Code length functions]
Define the following code length functions for the representations:
\begin{itemize}
\item $L(m)$ is the length of a self-delimiting description of
  the model $m$, i.e. it is the length function of  a prefix code,
\item $L^m(x)=L(m)+\min\{|y|:m(y)=x\}$ measures the number of bits required to describe both the model $m$ and
  the data $x$ using the model,
\item $L^\M(x)=\min\{L^m(x):m\in\M\}$ measures the number of bits
  required to describe $x$ using the best model in the model class.
\end{itemize}
\end{definition}
  
\begin{definition}[Candidate set]
  The \emph{candidate models} for $x$ from model class $\M$ and
  threshold function $\tau$ are
  \[
  C^\M_\tau(x):=\{m\in\M:L^m(x)\le \tau(x) + c\}
  \]
  with $c$ a constant independent of $x$ which may be chosen arbitrarily. The default threshold function is $\tau^\circ(x)=L^\M(x)$.
\end{definition}

\begin{definition}[Sophistication]
  The model complexity, sophistication, or facticity, of a string $x$
  given a model class $\M$ and threshold function $\tau$ is defined by
  \[
  S^\M_\tau(x):=\min\{L(m):m\in C^\M_\tau(x)\}.
  \]
\end{definition}

\section{Measures of meaningful information}

\subsection{Koppel Sophistication}

\begin{itemize}
\item Model class is $\T$
\item Threshold function $\tau(x)=L^\M(x)+c$
\item Description method for models is required to be prefix-free, but
  can otherwise be chosen arbitrarily.
\item Koppel uses monotone complexity and discusses infinite strings,
  TODO figure out if and how this affects our results
\end{itemize}

This does not work because of the nickname problem! To fix this:

\begin{definition}
  The complexity of a partial recursive function is defined by
  \begin{align*}
    C(f) &:= \min\{C(i):\phi_i=f\}\\
    K(f) &:= \min\{K(i):\phi_i=f\}.
  \end{align*}
\end{definition}

These definitions make sense because:

\begin{lemma}[Invariance]
Let $\phi_1, \phi_2, \ldots$ and $\psi_1, \psi_2,\ldots$ be two acceptable numberings. There exists a constant $c$ such that $\left| K^\phi(f) - K^\psi(f)\right | \leq c$ for all $f$. \label{lemma:invariance}
\end{lemma}
\begin{proof}
Let $g(i)$ be the function which which translates from $\psi$ to $\phi$.
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(i) : \phi_i= f\right\} \\
&\geq \min\left\{ K^\psi(i) : \phi_i= f\right\} - c\\
&= \min\left\{ K^\psi(i) : \psi_{g(i)}= f\right\} - c\\
&\geq \min\left\{ K^\psi(i) : \psi_i= f\right\} - c\\
&= K^\psi(f).
\end{align*}
We can reverse $\phi$ and $\psi$ without loss of generality to achieve the same inequality the other way around, completing the proof.
\end{proof}

Thus, the complexity of a function depends on the choice of
enumeration by no more than a constant term. Note that this implies
that for any enumeration $\psi_1,\psi_2,\ldots$, the complexity $K(\psi_i)$
is less than the literal description length of the index
$|\bar\imath|$ up to a constant. For some numberings, there is a gap
between these two description methods; in those cases, the index is an
inefficient representation of the corresponding function. However it
is possible to construct numberings that are \emph{faithful} in the
sense that no such gap exists:

\begin{definition}[Faithful Numbering]\label{def:faithful}
  A numbering $\psi_1,\psi_2,\ldots$ of the partial recursive
  functions is \emph{faithful} if there is a constant $c$ such that
  for all indices $i$ there is a $j$ such that $\psi_i=\psi_j$ and
  $|j|\le C(\psi_j)+c$.
\end{definition}

\begin{lemma}
  There is a faithful acceptable numbering.
\end{lemma}
\begin{proof}
Let $i_\tn{div}(y)$ be an index such that $\phi_{i_\tn{div}}(y)=\infty$ for all $y$. Define
  \[\psi_q=\begin{cases}
    \phi_{\phi_i(p)}&\tn{if $q$ can be written as $\bar\imath p$ and $\phi_i(p)<\infty$,}\\
    \phi_{i_\tn{div}}&\tn{otherwise.}\end{cases}
  \]
  To show that $\psi$ is faithful, pick any function $f$. Then
\[\begin{split}
C(f)&=\min\{C(i):\phi_i=f\}\\
&=\min\{\min\{|\bar a b|:\phi_a(b)=i\}:\phi_i=f\}\\
&=\min\{|\bar a b|:\phi_{\phi_a(b)}=f\}\\
&=\min\{|\bar a b|:\psi_{\bar a b}=f\}.
\end{split}\]
This shows there is a sufficiently small $\psi$ index.

To show that $\psi$ is acceptable, let $\phi_j$ denote the identity
function. Then a $\phi$-index $i$ can be mapped to a $\psi$-index
using the computable function $r(i)=\bar\jmath i$, so that
$\psi_{r(i)}(y)=\psi_{\bar\jmath i}(y)=\phi_i(y)$. For the reverse,
define $\phi_v(\bar\imath p, y)=\phi_{\phi_i(p)}(y)$. For fixed
$\bar\imath p$, the 
$s^n_m$-theorem (see~\cite{TODO}) states that we can compute the $h$
such that $\phi_h(y)=\phi_v(\bar\imath p,y)$. Let $h(\bar\imath p)$
denote this index as a function of the program; further define
$h(q)=i_\tn{div}$ if $q$ cannot be expressed as $\bar\imath p$. By
construction $h$ is total and computable. To check that the mapping
returns the correct function, rewrite $\phi_{h(\bar\imath
  p)}(y)=\phi_v(\bar\imath p,y)=\phi_{\phi_i(p)}(y)=\psi_{\bar\imath p}(y)$.
\end{proof}

\subsection{Antunes's Coarse Sophistication}

\begin{itemize}
\item Model class is $\T$
\item Description method $L(m)$ for models is unclear, but appears to be
  taken from Koppel, i.e. arbitrary but self-delimiting.
\item Sophistication is defined slightly differently: $S^\T(x)=L(m^*)$ where $m^*=\argmin_m L(m)+L^m(x)-C(x)$. Thus, there is no slack.
\end{itemize}


\subsection{Adriaans' Facticity}

\begin{itemize}
\item Model class is $\C$
\item Description method for models is $L(m)=|\bar\imath|$ where
  $\phi_i=m$. The index function is assumed to be \emph{faithful}, see
  Definition~\ref{def:faithful}
\item Threshold function is $\tau(x)=L^\M(x)$, the constant in the candidate set is $0$.
\item Facticity is actually defined as index length without prefix
  encoding:  $S^\C_0(x):=\min\{|i|:\phi_i\in C^\C_0(x)\}$. But this
  difference is not important for the discussion.
\end{itemize}


\begin{lemma}
  If $\min\{i\ge i_0:|\bar\imath|-K(i)\}$ is an unbounded function of $i_0$, then facticity is bounded.
\end{lemma}
\begin{proof}
Let $\bar\imath p$ be any two-part representation for the data $x$, i.e. $\phi_i(p)=x$. Then construct an alternative two-part representation $\bar vi^* p$, where $i^*$ is the shortest $\psi_u$-program for $i$ such that $\phi_v(i^* p)=\phi_{\psi_u(i^*)}(p) = \phi_i(p)=x$. We compare the lengths of these two representations. Note that $|i^*|=K^\psi(i)$. Therefore,
\[
|\bar\imath p|-|\bar v i^* p| = |\bar\imath|-|\bar v| - |i^*| = |\bar\imath|-|\bar v|-K(i).
\]
By assumption there must be an $i_0$ such that the above expression is positive for all $i>i_0$. From this $i$ onwards, the second representation (using $v$) will have a shorter code length, so $\bar\imath p$ cannot achieve the minimum in the definition of the facticity. Consequently, the facticity is bounded by $F^\phi(x)<|\overline{\imath_0}|$ for all $x$. 
\end{proof}
Note that the condition for this lemma applies to all known partial recursive prefix functions:

\begin{conjecture}
For any partial recursive prefix code length function $L$, the function
\[
\min\{L(i)-K(i):i\ge i_0\}
\]
is unbounded in $i_0$.
\end{conjecture}

\subsection{Effective complexity}

\begin{itemize}
\item Model class is \emph{ensembles}. Equivalent to $\K$.
\item Description method for models is $L(m) = K(m)$ (the minimization is not mentioned explicitly, but we'll give them the benefit of the doubt).
\item The threshold function is equal to the minimum two-part code: $\tau(x)=L^m(x)$.
\item Gell-mann and Lloyd acknowledge that representations exist with small and large models. The choice from the candidate set is made by applying a time-bound to the computation from the string to the model, but not to the decoding of the model itself. The idea is that this forces `deep' information into the model, and shallow information into the data. Presumably, the representation with the smallest model code under this constraint is chosen.   
\end{itemize} 
\pb{We can still manipulate the UTM so that all non-singleton representations are pushed out of the candidate set. Since the model side is allowed unbounded decoding time, this invalidates effective complexity.}

\subsection{Roll your own}


\begin{itemize}
\item Model class is either $\C$ or $\K$
\item Description method for models is $L(m)=K(m)$.
\item Threshold function is $\tau(x)=L^m(x)+c$, which matches $C(x)$
  resp. $K(x)$ up to a constant.
\end{itemize}


\begin{lemma}\label{lem:thecoolone}
  Let $\psi_1,\psi_2,\ldots$ be any acceptable enumeration of the partial recursive functions.
  Let $\M$ be any model class, let $\X$ be any set of binary sequences and let $D:\{0,1\}^*\to\N$ be a computable decoding function with a prefix-free domain that maps function descriptions to their indices in $\psi$, i.e. if $f=\psi_{D(p)}$ then $p$ is a $D$-description of $f$. Let $\M'=\{\psi_i:i\in\tn{range}(D)\}$. Further assume there is a constant $c$ such that
\begin{enumerate}
  \item $\forall_{f\in\M'}:\min\{|p|:\psi_{D(p)}=f\}\le K^\psi(f)+c$
  \item $\forall_{x\in\X}:L^{\M',\psi}(x)-L^{\M,\psi}(x)\le c$.
\end{enumerate}
Then is an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions such that $S^\phi_{\M}(x) = |\bar 0|+S^\psi_{\M'}(x)$ for all $x\in\X$.
\end{lemma}
\begin{proof}
We define the numbering $\phi$ as follows:
\[\begin{cases}
\phi_0(p) = 1^r 0 D(p) \\
\phi_{1^r0i}(p) = \psi_i(p) \\
\phi_j(\cdot) = \infty &\text{if $j$ contains no zeroes.}
\end{cases}\]
We will show that under the $\phi$-enumeration, the best representation for $x$ using a model $f\in\M'$ is always better than the best representation using some $f\not\in\M'$.

First suppose $f\in\M'$. Then
\[\begin{split}
K^\phi(f)&=\min\{K^\phi(i):\phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_j(q)=i, \phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_{\phi_j(q)}=f\}\\
&\le\min\{|\bar0 q|:\phi_{\phi_0(q)}=f\}~\text{(using $j=0$)}\\
&=\min\{|\bar0 q|:\phi_{1^r0 D(q)}=f\}\\
&=|\bar 0|+\min\{|q|:\psi_{D(q)}=f\}\\
&\le K^\psi(f)+c+|\bar 0|,
\end{split}
\]
where the last inequality uses the first assumption.


Now assume that the best model $f$ for $x$ is not in $\M'$. 
Let $i$ be the index of $f$ with the shortest description, i.e. it achieves the minimum in
\[
K^\phi(f)=\min\{K^\phi(i):\phi_i=f\}.
\]
There are two possibilities. Either $i=0$, in which case we have $K^\phi(f)\ge r$ because $\phi_0$ cannot output zero and all other $\phi$-programs are at least $r$ bits long.

Otherwise, we can bound
\[\begin{split}
K^\phi(f)&=K^\phi(i)=K^\phi(1^r0j)\\
&\ge K^\phi(j)-c'\\
&\ge K^\psi(j)+r+1-c'\\
&\ge\min\{K^\psi(j):\phi_j=f\}+r+1-c'\\
&=K^\psi(f)+r+1-c'.
\end{split}
\]
Now choose $r=c''+\max\{K^\psi(\phi_0),c'-1\}$. Then substitution yields, for both cases, 
$K^\phi(f)\ge c''+K^\psi(f)$.

Combining the inequalities above, for any model $g\not\in\M'$, there is a model $f\in\M$ such that
\[\begin{split}
K^\phi(g)+C^g(x) &\ge K^\psi(g)+C^g(x)+c''\\
&\ge K^\psi(f)+C^f(x)-c+c''\\
&\ge K^\phi(f)+C^f(x) -2c+c'' -|\bar0|.
\end{split}\]

By choosing $c''$ sufficiently large, we can ensure that the best representation is in $\M'$ for all $x\in\X$, which completes the proof.
\end{proof}

\paragraph{TODO: instantiate for UTM}

\begin{itemize}
\item Model class is $\K$
\item Description method for models is $L(m)=K(m)$.
\item Threshold function is $\tau(x)=L^m(x)+c$, which matches $K(x)$
  up to a constant.
\end{itemize}


\begin{lemma}
Let $\M$ be a prefix model class where for every $x\in\X$ there is a singleton model $f\in\M$ with $f(\epsilon)=x$. Then there is an enumeration $\phi_1,\phi_2,\ldots$ of the prefix partial recursive functions, and a constant $c$, such that
\[
K(x)-S^{\M,\phi}(x)\le c
\]
for all $x\in\X$.
\end{lemma}
\begin{proof}
Let $\psi_1,\psi_2,\ldots$ be any default enumeration of the partial recursive prefix functions. Note that since $f$ is a prefix function, if $f$ is defined for input $\epsilon$ then it cannot be defined for any other input. Pick $f,x$ with $f(\epsilon)=x$. Note that $x$ can be computed from $f$ and a fixed program, so there is a $c$ such that $K(x)\le K(f)+c$. Vice versa, given any $x$ we can construct an index of $f$, since $\psi$ is an acceptable numbering. Therefore $|K(f)-K(x)|\le c$.

We now define a computable function $D$ by $D(\bar\imath p)=j$ where $\psi_j(\epsilon) = \psi_i(p)$.  We will show that the two conditions of Lemma~\ref{lem:thecoolone} hold for the prefix function $D$.

\begin{enumerate}
\item Let $f$ be any function in the range of $D$, and $x$ its output. Then $\min\{|p|:\psi_{D(p)}=f\}=\min\{|\bar\imath q|:\psi_i(q)=x\}=K(x)\le K(f)+c$.
\item On the one hand $L^{\M',\psi}(x)\le K(f)+|\epsilon|\le K(x)+c$. On the other hand, $L^{\M,\psi}(x)$ is an effective description of $x$, so $K(x)$ is at most a constant larger.
Together, these inequalities establish the second condition.
\end{enumerate}

Then by Lemma~\ref{lem:thecoolone} there is an enumeration $\phi_1,\phi_2,\ldots$ such that $S^{\M,\phi}(x)=|\bar 0|+S^{\M',\psi}(x)$. We observed that $|K(f)-K(x)|\le c$ for any $f\in\M'$, so $S^{\M',\psi}(x)\ge K(x)+c$. This proves the lemma.
\end{proof}

\subsection{Kolmogorov's Structure Function}

\begin{itemize}
\item Model class is $\F$
\item Description method for models $L(m)=K(S)$ where $S$ is the range
  of $m$. There is some ambiguity as to the definition of prefix
  complexity for finite sets; discuss and refer to Shen \cite{TODO}.
\end{itemize}

\subsection{Vit\'anyi's Sophistication / Meaningful Information}

\begin{itemize}
\item Model class is $\T$
\item Description method for models is $L(m)=K(m)$
\item Candidate set is defined differently: $C_c=\{m\in\T:|L^m(x)-K(x)|\le
  c\}$. There appears to be a problem with this definition, because
  the two-part descriptions of $x$ may be shorter than $K(x)$ by
  exploiting the fact that the models do not need to be prefix functions.
\end{itemize}

% stru fu: model class is all finite sets
% code length L(m)+L_m(x)
% acceptability criterion acc(m) = 2L(m)+L_m(x)-C(x)
% 


\section{Paul's sophistication (total, non-prefix) is \emph{not}  bounded}

\begin{lemma}
Let $f$ be a total, computable function, such that for some $d$, $f(d) = x$ and let $k = K(f) + |d|$. Then there exists a finite set $S$ containing $x$ and a constant $c$, such that $K(S) \leq K(f) + K(|d|) + c$ and $\log |S| \leq |d|$.\label{lemma:total-to-sets}
\end{lemma}
\begin{proof}
Let $S = f\left(\{0,1\}^{|d|}\right)$. Since $f$ is total, this set can be explicity computed from a description of $f$ and a description of $|d|$, which tells us that there is a constant $c$ such that $K(S) \leq K(f) + K(|d|) + c$. 

Since $S$ is the image of a set of size $2^{|d|}$ under $f$, we have $\log |S| \leq |d|$.
\end{proof}
This lemma is a variation on \cite[Lemma~7.2]{vitanyi2004meaningful}.

\begin{theorem}
There are infinitely many $x$ with 
\begin{enumerate}
  \item $S^\F_K(x) \geq K(x)/5$,\label{eq:item1}
  \item $S^\T_K(x) \geq K(x)/6$, {and}\label{eq:poezenvoer}
  \item $S^\T_{\tau^\circ}(x) \geq K(x)/6$. \label{eq:hondevoer}
\end{enumerate}
\end{theorem}
\begin{proof}
From \cite[Proposition~I.3 (b)]{gacs2001algorithmic} we know that for sufficiently large $i$ there are strings of length less than $i$ that are not $(i/4, i/4)$-stochastic. For each $i$, let $x_i$ be the smallest binary string that is not $(i/4, i/4)$-stochastic. Since every string is $(\alpha,\alpha)$-stochastic for sufficiently large $\alpha$, this sequence contains infinitely many distinct binary strings. Now pick any $i$ that is large enough that (a) $|x_i|<i$ and (b) $K(x_i)/5 < i/4$. By definition of $(\alpha,\beta)$-stochasticity, if a string is not $(i/4,i/4)$-stochastic, then for every finite set $S$ containing $x$, either (a) $\log|S|\ge K(x_i)+i/4$, in which case $S$ cannot be a candidate representation for $x$ in $C^\F_K$, or (b) $K(S)>i/4>K(x_i)/5$. This proves Item~\ref{eq:item1}. 

For Items~\ref{eq:poezenvoer} and~\ref{eq:hondevoer}, suppose towards contradiction that either $C^\T_K(x)$ or $C^T_{\tau^\circ}(x)$ contains a candidate $f$ with $K(f)\le i/5$. Then by Lemma~\ref{lemma:total-to-sets}, there is a set $S$ with $\log|S|\le|d|\le\le\tau^\circ(x)\le K(x)$, so (a) above cannot be the case, and $K(S)\le K(f)+K(|d|)+c\le i/5+K(|d|)+c$, which contradicts (b) for large enough $i$. Therefore such candidates $f$ cannot exist and all candidates must satisfy $K(f)>i/5>K(x_i)/6$.
\end{proof}
Note that we favoured simplicity of the proof over the strength of the bounds: the constants can be improved.

\section{Notes}

Do we have the citation ``Paul M. B. Vit\'anyi: Meaningful Information. IEEE Transactions on Information Theory 52(10): 4617-4626 (2006)''?

(From Koppel, Complexity, Depth and Sophistication:)

``Sophistication is a generalization of the "H-function" or "minimal sufficient statistic" of Cover and Kolmogoroff, using the motonic complexity of Levin: is Cover the original source for the idea of a ``minimal sufficient statistic''? Also Koppel does indeed use monotonic complexity as in Levin. We need to know at least a little about the difference between monotonic and regular KC: is it less than a logarithmic additive term? If so, we can conveniently sweep it under the rug.

\section{Plan}

Idee van Sophistication: gebruik two-part coding om een scheiding te maken tussen structuur van de data (modelinformatie) en de ruis.


Perhaps surprisingly, it is also reinforced by Vit\'anyi, when he writes : (From Vereshchagin and Vit\'anyi, ``Kolmogorov’s Structure Functions and Model Selection'':)
\begin{quotation}
  This expression emphasizes the two-part code nature of Kolmogorov complexity. In the example
\[x=10101010101010101010101010\]
we can encode $x$ by a small Turing machine printing a specified number of copies of the pattern `10' which computes $x$ from the program `13'.” This way, $K(x)$ is viewed as the shortest length of a two-part code for $x$, one part describing a Turing machine, or model, for the regular aspects of $x$, and the second part describing the irregular aspects of $x$ in the form of a program to be interpreted by $T$. The regular, or “valuable,” information in  is constituted by the bits in the “model” while the random or ``useless'' information of $x$ constitutes the remainder.
\end{quotation}



Adriaans takes this problem seriously and attempts to address it by imposing additional restrictions on the ordering of TMs implied by the used UTM \cite{adriaans2012facticity}. We are skeptical that such measures can effectively eliminate this problem without resorting to orderings of the TMs that are not admissible [TODO ref].



Inherent property of the data: 
\begin{itemize}
\item Should be a two-part effective description of $x$
\item Invariant: $\exists c:\forall x:|S^1(x)-S^2(x)|\le c$
\item Both structure and noise are unbounded
\end{itemize}

Conjecture: er is geen functie van de data die aan bovenstaande eisen voldoet.



\subsection{Review}
Onderscheid op basis van drie properties:
\begin{itemize}
\item Model class: total / partial. Reden voor total: bij partial rec. is de UTM in het model, collapse, citaten. Nadeel: arbitraire beperking op de beschrijvingskracht, dus minder universeel
\item Models are prefix functions / not prefix functions: viz $C$ vs $K$. Als wel prefix, dan gaat alles makkelijker kapot. Als niet prefix dan wordt de balans verstoord, het wordt moeilijk als de balans ook de andere kant op wordt verstoord door de modelklasse te beperken: die twee handicaps zijn moeilijk te vergelijken.
\item Model length is prefix encoded index length / $K(f)$: compact numberings, bespiegeling over of het gat tussen $|\bar\imath|$ en $K(\phi_i)$ unbounded is. Wat is nu eigenlijk de two-part code precies, is dat $K$ als white box of is $K$ gebruikt voor het eerste deel? Ook beetje vreemd om in de eerste visie modelklassen te introduceren.
\end{itemize}

Bestaande aanpakken:
\begin{itemize}

\item Facticity: partial / niet prefix: bounded

\item Paul's sophistication: Sufficient statistic: alles wat $K$-complexiteit haalt tot op slack $c$. Minimal sufficient statistic minimaliseert daarover de modelgrootte. Model bevat totale prefixfuncties. Noise is bounded.

\item total / niet prefix, o.a. coarse sophistication. We geloven invariantie niet. Voor alle shallow strings bounded (nog niet bewezen, maar werkt door alles total te maken met vaste grote maar computable time bound)

\end{itemize}

\subsection{Conclusion}
Stel onze conjecture is waar, is het dan helemaal nutteloos? Nee, want lengte van two-part representations is nog steeds een redelijke maat voor counterevidence against a model. No-hypercompression, $p$-values, randomness deficiency. Probleem zit in harde cutoffs zoals minimalisatie en een vaste constante waarboven representaties niet meer meedoen. Daar gaat invariantie van kapot.

Onze mening: als onze conjecture waar is, dan moet je constraints weglaten: maak het makkelijk en doe prefix functies en alle partial recursive functions, en accepteer de collapse gewoon. Maar dat betekent weer dat je een hele wolk van two-part optimale representaties hebt waar je niet tussen moet willen kiezen.

Er is een gat tussen $K(f)+|y|-K(x)$ en $\delta(x\mid f)$. De laatste is de juiste maat voor de counterevidence tegen $f$.

\section{Thoughts and questions}

\begin{itemize}
\item Paul argues bleurg that the randomness deficiency of a model is a measure of the amount of structure left in the noise part. I feel that it is also a measure of the amount of counterevidence against a model in the statistical sense, as follows: the randomness deficiency of $x$ given model $M$ is $\delta(x|M)=L_M(x)-K(x|M)$. Suppose that model $M$ is true. We can then use the no hypercompression inequality to rewrite:
%
\[
P_{x\sim M}(\delta(x|M)\ge c) = P_{x\sim M}(L_M(x) - K(x|M) \ge c)\le 2^{-c}
\]
Thus, if $M$ is true the probability of a large randomness deficiency is exponentially small! This amount of evidence is robust in the sense that changing the UTM around will change the amount of evidence by at most a constant. If the structure function is going to give us a \emph{cloud} of candidate models instead of just a single one, I think this is a really nice interpretation of the meaning of that cloud: we simply measure the available evidence against all individual models.  

\item Shen explains the collapse of the structure function if you use enumerating complexity rather than listing complexity. How can we most cleanly explain that this collapse does not vanish for weaker model classes?

\item Properties of representations on $K(M)$ vs $L_M(x)$ graph: diagonals identify representations with the same two-part codelength. The randomness deficiency is the distance to the minimum, which is the $K(x)$ diagonal:
\[L(x;M)=K(M)+L_M(x) =K(M)+K(x|M)+L_M(x)-K(x|M)  = K(x)+\delta(x|M).\]
 Models move horizontally by a constant when the ordering of TMs is changed. Thus the randomness deficiency and two part code are also changed by at most a constant. In contrast the \emph{minimum} can now be achieved by a very different representation.

\item ``Power and peril of MDL'': bespreken ook de ``collapse of the structure function'' als de model class te sterk is, dwz sets worden te efficient gecodeerd (wsch equivalent met Shen's enumerating complexity).

\end{itemize}

\appendix
\section{Dead darlings}

\begin{definition}
Given an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions, and an enumeration $\psi_1,\psi_2,\ldots$ of the partial recursive prefix functions. Let $\psi_u$ be a universal element with property $\psi_u(\bar\imath p)=\psi_i(p)$. Then define $v$ by
\[
v(a p)=\begin{cases}\phi_{\psi_u(a)}(p)&\tn{if $a$ is in the domain of $\psi_u$,}\\
\infty&\tn{otherwise}.\end{cases}
\]
\end{definition}
\begin{lemma}
The function $v=\phi_v$ as defined above is partial recursive.
\end{lemma}
\begin{proof}
Let $r$ be the input of $v$. Simulate $\psi_u$ using a TM with a one-way read-only input tape. If the TM does not halt, then enter an infinite loop. Otherwise, let $r=ap$ where $a$ is the part of the input tape that has been read when the TM halts, and let $i$ be the output of the TM. Now simulate $\phi_i$ on input $p$.
\end{proof}

\begin{lemma}
Assume the prefix function $\bar\cdot$ used in the definition of complexity is such that $\bar x-x$ is monotonically increasing. Let $\psi_1,\psi_2,\ldots$ be any acceptable numbering. Then for every $c$ there is another acceptable numbering $\phi_1,\phi_2,\ldots$ such that for all partial recursive functions $f$ we have
\[
K^\phi(f)-K^\psi(f) \ge c.
\] \label{lemma:building-block}
\end{lemma}
\begin{proof}
First note that there is a $c'$ such that 
\begin{equation}
	K(1^r 0  x) + c' \geq K(x) \label{eq:prelim1}
\end{equation}
for all $x$, since we can create a program that first runs the shortest program for $1^r 0 x$ and then strips the prefix $1^r 0$ from the result.

Define $\phi_{1^r0i}(x) = \psi_{i}(x)$, with $\phi_{j}(x) = \infty$ if $j$ does not contain a zero. Intuitively, any $\phi$-program is $r+1$ bits longer than the corresponding $\psi$-program. However, the used prefix function complicates this slightly. But all we need is the bound
\begin{equation}\label{eq:prelim2}\begin{split}
K^\phi(x) &= \min \{ |\overline{1^r 0 \imath}| y : \phi_{1^r 0 i}(y) = x \}\\
&  \ge \min\{|1^r0\bar\imath y| : \phi_{1^r 0 i}(y) = x \}\qquad\tn{(using assumption on $\bar\cdot$)}\\
& = r+1 +\min\{|\bar\imath y| : \psi_i(y) = x \}\\
& = r+1+K^\psi(x).
\end{split}\end{equation}

We can now relate the complexity of $f$ under the two enumerations as follows:
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(j) : \phi_j = f \right\}& \\
       &= \min\left\{ K^\phi(1^r0i) : \phi_{1^r0i} = f \right\}& \\
       &\ge  \min\left\{ K^\phi(i) : \phi_{1^r0i} = f \right\}-c'&\text{by~\eqref{eq:prelim1}}\\
       &\geq \min\left\{ K^\psi(i)  : \phi_{1^r0i} = f \right\}+r+1-c'& \text{by \eqref{eq:prelim2}}\\
       &= \min\left\{ K^\psi(i) : \psi_{i} = f \right\} + r+1 -c'& \\
       &= K^\psi(f) + r + 1 - c'.
\end{align*}
The proof is completed by choosing $r=c+c'-1$.
\end{proof}

\begin{definition}
  An enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive
  functions is called \emph{compact} if for all $i,y$ there is a $j$
  such that $|j|\le|i y|$ and $\phi_i(y z)=\phi_j(z)$ for all $z$.
\end{definition}

\begin{lemma}
There is a compact acceptable numbering.
\end{lemma}
\begin{proof}
  Let $\phi_1,\phi_2,\ldots$ be any acceptable numbering. Let $\psi_1,\psi_2,\ldots$ be a new acceptable numbering defined by $\psi_j(x)=\phi_i(y x)$ if $j$ is a pair satisfying $j=\bar\imath y$, and $\psi_j(x)=\infty$ otherwise.  We will show that $\psi$ is both $0$-compact and acceptable. 

To show compactness, pick any index $i$ and binary string $y$. First assume $i$ is a valid pair $i=\bar a b$. Then  $\psi_i(y x) = \psi_{\bar a b}(y x)=\phi_a(b y x)$, but also $\psi_{\bar a b y}(x)=\phi_a(b y x)$, so $\psi_i(y x) = \psi_j(x)$ for $j=\bar a b y$; so $|j|=|iy|+0$. On the other hand, if $i$ is not a valid pair, we can simply use $j=i$ so that $\psi_i(y x)=\psi_j(x)=\infty$ for all $x$.

To show that $\psi_1,\psi_2,\ldots$ is acceptable, we define total
computable functions $f$ and $g$ such that $\psi_i(x)=\phi_{f(i)}(x)$
and $\phi_j(x)=\psi_{g(j)}(x)$. The easy direction is
$g(j)=\bar\jmath\epsilon$. The definition of the mapping in the other
direction $f(i)$ depends on whether $i$ is a valid pair. If it is not,
then $f(i)=h$ for some $h$ such that $\phi_h(x)=\infty$ for all $x$. On the other hand if $i=\bar\jmath y$, then $f(i)$ is the index of a partial recursive function $\phi_{f(i)}(x)$ defined by a Turing machine that simulates the machine for $\phi_j$ but with $yx$ on the input tape.
\end{proof}


\begin{lemma}
Compact numberings are sort of faithful.
\end{lemma}
\begin{proof}
First, let $v$ be the index of the partial recursive function given by $\phi_v(\bar k p x)=\phi_{\phi_k(p)}(x)$. Let $\bar k p$ be a shortest program for $i$, that is $|\bar k p| = K(i)$ and $\phi_k(p)=i$. Then $\phi_i(x)=\phi_v(\bar k \bar p x)$. We now use compactness to find an index $j$ with $\phi_v(\bar k \bar p x)=\phi_j(x)$ and length
\[|j|\le |v\bar k\bar p|\le|v|+|\bar k p|+2\log|\bar k p|=K(i)+|v|+2\log K(i),\]
as required.
\end{proof}


\bibliographystyle{plain}
\bibliography{facticity}

\end{document}

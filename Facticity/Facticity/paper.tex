\documentclass{style/llncs}

\usepackage{amsmath,amsfonts}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[mathscr]{eucal}

\newcommand{\M}{\mathscr M}
\newcommand{\C}{\mathscr C}
\newcommand{\T}{\mathscr T}
\newcommand{\F}{\mathscr F}
\renewcommand{\P}{\mathscr P}
\newcommand{\K}{\mathscr K}
\newcommand{\X}{\mathscr X}
\newcommand{\B}{\mathbb B}
\newcommand{\D}{\Delta}
\newcommand{\N}{\mathbb N}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\pair}[1]{\left\langle{#1}\right\rangle}
\newcommand{\concat}{\oplus}
\newcommand{\symb}[1]{\texttt{#1}}
\newcommand{\br}[1]{\overline{#1}}
\newcommand{\s}{\tn{soph}}
\newcommand{\num}[1]{#1_1, #1_2, \ldots}

\newtheorem{conj}{Conjecture}

\let\doendproof\endproof
\renewcommand\endproof{~\hfill\qed\doendproof}

\newcommand{\p}{\,\text{.}}

\newcommand{\tuple}[1]{\left\langle{#1}\right\rangle}

\newcommand{\hide}[1]{}

\newcommand{\sdr}[1]{\textcolor{blue}{\small #1\textsuperscript{[Steven]} }}
\newcommand{\pb}[1]{\textcolor{OliveGreen}{\small #1 \textsuperscript{[Peter]} }}

\newcommand{\argmin}{\mathop{\arg\min}}

\title{A Critical Review of Sophistication and Algorithmic Model Selection}
\author{Peter Bloem and Steven de Rooij}

\institute{
  System and Network Engineering Group, \\University of Amsterdam, the Netherlands\\
  \email{uva@peterbloem.nl, steven.de.rooij@gmail.com}
}

% in introductie?
%
% model, model class
% two-part code: sophistication is length of first part
% kolmogorov complexity?
% KC = two part vs KC in two part
% collapse, restrictions on model class (eg total instead of partial recursive, see section XX)
% conjecture

\begin{document}
\maketitle

\begin{abstract}
Kolmogorov complexity is a sound expression of the information content of a binary string, but it clashes with our intuitive notions of information: the strings with maximal information content are not the ones we would regard as interesting, or meaningful. One attempt to capture this idea of `useful' information is \emph{sophistication}. It is a simple idea with many attempted formalizations: describe the string efficiently with a two-part coding approach, and take the complexity of the model as a measure of the amount of interesting information. Kolmogorov complexity is invariant to the ad-hoc choices in its construction: change them, and the result will not change more than a constant. We review the history of sophistication critically, asking whether invariance exists for the proposed sophistication functions as well. In most cases, we can show that it doesn't. We conjecture that no formalization of sophistication can be invariant, and we briefly review the consequences of this conjecture.
\end{abstract}

\section{Introduction}

\noindent Kolmogorov complexity is an objective measure of the information content of an object. Although its definition contains several arbitrary choices, these only affect its value up to a constant additive term. This property is crucial for the measure's success: it allows us to view Kolmogorov complexity as a \emph{property of the data} rather than an arbitrary function computed on it. This, in turn, allows us to define the intrinsic entropy, or randomness, inherent in a single object expressed as a binary string, without any assumptions on the process that produced it. However, this notion of complexity does not in all respects match our intuitive idea of which objects are complex, and which objects are not.

Imagine an old-fashioned, analog television set which can be tuned to three frequencies. The first produces a blank screen, the second an ordinary TV show, and the third white noise. Judging by their Kolmogorov complexity, these  broadcasts have increasing information content. The first is easiest to describe, the second is harder but still contains a great deal of regularity, and the third cannot be compressed. However, intuitively speaking the second signal clearly conveys more meaningful information than the other two. This notion has proved far harder to capture in formal terms than the simple notion of `raw' information content. 

Over the years one solution in particular has been proposed in many different forms. We select a description for the data by two-part coding; describing a model first, and then the data in terms of the model. The size in bits, used to describe the model is taken as a measure of the useful information in our string. While this idea has been proposed many times in many different guises, we will, for the purposes of this paper refer to it as \emph{sophistication}\footnotemark. 

\footnotetext{As the historical overview shows, sophistication is only one name for this idea, and not the first. The alternatives, however, are more awkward, and sophistication has been used by different authors for different realizations of the same principle.}

The intuitive idea of sophistication makes one crucial demand: we can find an objectively value for the model complexity of a string, either by having a single objectively optimal two-part description, or a set of descriptions with equivalent model complexity. In the framework of Kolmogorov complexity, two observers cannot disagree by more than a constant about the information content of a dataset. In formal terms: the Kolmogorov complexity of a dataset $x$ is defined as the shortest effective description for $x$ in some Turing complete description language. This description language is usually expressed as a universal Turing machine $U(\bar\imath y) = \phi_i(y)$ where $\phi_i$ is the $i$'th partial computable function in some effective enumeration, and the $\bar\imath$ is a self-delimiting encoding of $i$. If two people disagree about the shortest effective description of $x$, we can interpret this as them using different description languages, ie. different universal Turing machines. The crucial aspect of Kolmogorov complexity, the starting theorem that makes it interesting as a function at all, is \emph{invariance}: two people using different description languages will get the same value for a string's Kolmogorov complexity, albeit up to a fixed constant. Without invariance, Kolmogorov complexity would be an arbitrary function. With invariance, we can be sure that we are talking about an intrinsic property of the data. The same consideration must be given to each proposal for formalising sophistication: is it invariant? 

There are many approaches to sophistication, but the following pattern is common to all of them. We start with a space of \emph{models}. There can be finite sets, computable probability distributions or simple functions from strings to strings. The function perspective is the most generic, all other model classes can be translated to it. The approach to sophistication is to find a representation for a dataset $x$ in two parts: a computable function $f$ and and input $y$, such that $f(y) = x$. As with Kolmogorov complexity, the shorter a description is, the better. We take the shortest description and those close to it, and consider them \emph{candidates}. How close a description must be to the optimum differs between variants, it is often a pre-determined constant value. We then choose a single description from the candidate set and return its \emph{model complexity} as the sophistication. The method for determining model complexity varies. In almost all variants, the candidate with the smallest model complexity determines the sophistication. 

The implicit assumption behind sophistication is that this approach will separate the description of the string into a structural part, representing its regularities, and a stochastic part representing its non-deterministic properties. In other words, the string is cast as a random choice from a model for which it is `typical'.  

In this article we will show that most proposals are not invariant. For one remaining case, we can offer no such proof, but no proof of invariance exists either. This is likely due to the complex nature of the model class chosen, and the difficulty of resolving the matter of invariance shows how complex such a sophistication measure would be to build on, even if it did prove to be invariant.

\subsection{A brief historical overview}

We first provide a chronological overview of the existing variants of sophistication. In the body of the paper, we will define each variant rigorously, and in non-chronological order.

The first suggestion that a representation of a string can be separated cleanly into an encoding of its regular and its stochastic proeperties came from Kolmogorov himself, in a talk given at a conference in Tallinn in 1973 (first committed to paper by Cover \cite{cover1985kolmogorov} and later discussed more extensively discussed by Vereshchagin and Vit\'anyi \cite{vereshchagin2004kolmogorov}). Here, he proposed what has become known as the \emph{Kolmogorov structure function}. Briefly, this function operates on a space of two part representations for a given string $x$, consisting of some finite set $S$ (containing $x$) and the index of $x$ in $S$. The \emph{minimal sufficient statistic} is the set with the lowest complexity for which the length of the two-part representation reaches the prefix-free Kolmogorov complexity. It was first shown by Shen \cite{shen1983concept} that there are strings for which the best model cannot be small.

Koppel \cite{koppelSoph1988,koppel1991almost} built on this notion, coining the phrase \emph{sophistication}. He changed the class of `models' from finite sets to total functions and used monotone functions, instead of prefix-free Kolmogorov complexity. We expect that this change was made to extend the theory to infinite strings, an approach which has been abandoned by subsequent authors. We will only consider the finite version (based on regular functions) in this paper.

While Koppel expands the model class from finite sets to functions,he does require that the functions be total. This is a recurring theme: if the model class is expanded to all computable functions, the universal Turing machine $U$ itself becomes a model. If we assume that $U$ computes the function $\phi_u$ in the standard enumeration of computable functions, and that $\bar\imath y$ is a shortest description for $x$ on $U$ (ie. $U(\bar\imath y) = x)$), then $\br{u}\br{\imath}y$ is also a description for $x$ on $\bar u$. This description has a constant model size ($|\br{u}|$), ie. every $x$ has constant sophistication. This `collapse' to a universal model is the reason that most treatments of sophistication constrain the model class to exclude the universal Turing machine. This makes it more expensive to move information out of the model. As we shall see, defining sophistication quickly becomes a balancing act, forcing information one way or the other.

In 1996 Gell-Mann and Lloyd introduced `Effective complexity' \cite{gellmann1996information}. Working from the perspective of statistical mechanics, they arrived (largely independently, it seems) at a notion that fits the mold of sophistication. Effective complexity describes a binary string in terms of a probability distribution over finite strings (an `ensemble' in the terminology of Gell-Mann and Lloyd) and the entropy of the distribution. The reasoning is that, if the string is typical for the distribution, the entropy is sufficient to describe the string given the distribution, so that the length of the description of the distribution and the size of its entropy suffices as a two-part description. The two-part description which determines the effective complexity is chosen by placing a time bound (taken from the singleton description) on the computation of the string from the distribution. The hope is that this will move deep (ie. hard to compute) information to the model part, and shallow information to the data. In a later treatment\cite{gell2004nonextensive}, Gell-Mann and Lloyd select the decisive two-part representation from the candidates based on ``the average quantities judged to be important'', which seems to be a conscious choice to make the measure subjective and context-dependant (sacrificing invariance).

G\'acs, Tromp and Vit\'anyi \cite{gacs2001algorithmic}, built on the idea of the structure function to establish a framework of \emph{algorithmic statistics}, as an attempt to found a statistical theory not on probabilistic notions, but on combinatorial ones. This echoes Kolmogorov's own hope for the theory of information:
\begin{quotation}
\noindent Information theory must precede probability theory and not be based on it. By the very essence of this discipline, the foundations of information theory must have a finite combinatorial character. \cite{kolmogorov1983combinatorial}
\end{quotation}
Algorithmic statistics studies binary strings and their possible two-part representations. In a later paper \cite{gacs2001algorithmic}, Vit\'anyi specifically studies the role of the minimal sufficient statistic as an expression of the `meaningful' information of a string (taking the name sophistication from Koppel). Like Koppel, Vit\'anyi restricts the model class to total functions, eliminating the universal model.

Antunes et al. \cite{antunes2009sophistication} note that the constant cutoff used to separate candidate two-part descriptions from those deemed to be too inefficient, is a source of instability. Small changes to this constant could shift the result sophistication by arbitrary amounts. They introduce a more elaborate criterion for the cutoff, calling this variant \emph{coarse sophistication}. As we shall see, this version behaves very differently from regular sophistication, and is closer to computational depth.
 
Adriaans' \emph{Facticity}, introduced in 2012\cite{adriaans2012facticity}, relaxes the model class to include all partial recursive functions. By constructing a very specific universal Turing machine, and taking constant terms into account, the facticity is expected to become robust against the collapse to a universal model.

\pb{Vereshchagin's strongly sufficient statistic} 

\pb{Antunes' instability result}

\pb{Diagrammetje}

\section{Preliminaries, definitions and desiderata}

Before we start, we will define a framework into which we can fit all current variants of sophistication. The principle is always the same: we start with a class of models, we consider the space of two-part descriptions using these models, we select the set of representations with the shortest code-lengths and we select, by some method, the best model from this set. 

Let $\B = {0,1}^*$. We deal with partial functions $\phi: \B \to \B$, which we will also call \emph{models}. Such functions are \emph{computable} if they can be computed by a Turing machine as described in \cite[Definition~1.7.1]{li1993introduction}. A function is prefix-free if its domain, $D_f = \{y : f(y) \neq \infty\}$ is a prefix set. A function $f$ is \emph{total} if $D_f = \B$. A prefix-free function is \emph{complete} if every infinite binary string has a member of $D_f$ as a prefix. \footnotemark 

\footnotetext{Completeness is analogous to totality, in that a prefix-free Turing machine that does not halt corresponds to a complete prefix-free function.}

\begin{definition}[Models and model classes]
  A \emph{model} is a function $\phi:\B\to\B$. A
  \emph{model class} is a set of models. We distinguish four important
  model classes:
  \begin{itemize}
  \item $\C$ is the set of all partial computable functions,
  \item $\K\subset\C$ is the set of all prefix-free
    functions,
  \item $\T\subset\C$ is the set of all total functions,
  \item $\P\subset\K$ is the set of all complete, prefix-free functions,
  \item $\F\subset\P$ contains, for every finite set $S$, a computable
    surjective function mapping the binary sequences of length
    $\lceil\log S\rceil$ onto $S$. In other words, this is the model class of finite sets, with uniform distributions.
  \end{itemize}
\end{definition}
\begin{definition}[Numberings]
A numbering is an enumeration of $\C$, denoted as $\phi_1, \phi_2, \ldots$ or by a function $g: \N \to \C$. We fix one canonical numbering $g^\circ$, which works as an effective description, that is for any $i \in \N, y\in \B$, we can effectively compute $(g^\circ(i))(y)$. We call a numbering $g$ \emph{acceptable} if there exist total, computable functions $a, b: \N \to \N$ as that for all $i$ $g^\circ(a(i)) = h(i)$ and  $g^\circ(i) = h(b(i))$.
\end{definition}

\begin{definition}[Prefix encoding function]
  A prefix encoding function $f:\B\to\B$ is an injective
  function that maps the binary strings to a prefix-free set. There
  are prefix encoding functions $g$ with the property that
  $|g(x)|\le|x|+2\log(|x|+2)|$ \cite{li1993introduction}. We fix such a function and
  denote it by $\br{x}=g(x)$.
\end{definition}

\begin{definition}[Complexity]
Let $\phi_1,\phi_2,\ldots$ be the canonical acceptable enumeration of $\C$. Let $\Pi$ be a computable subset of the natural numbers such that  $\{\phi_i:i\in\Pi\} = \K$. Now define Kolmogorov complexity by
\begin{align*}
C(x)&=\min\{|\bar\imath| y:\phi_i(y)=x\},\\
K(x)&=\min\{|\bar\imath| y:\phi_i(y)=x, i\in\Pi\}.\\
\end{align*}
\end{definition}
This definition of $K$ is uncommon. It is functionally equivalent to the standard definition as used in \cite{li1993introduction}, but this form is more convenient for defining the $K$-complexity of non-prefix functions (see Definition~\ref{definition:model-complexity} below).

We now have the basic tools with which we set up our generic sophistication function. We first define the space of two-part representations for a given string $x$, and the codelength that each representation achieves:

\begin{definition}[Representations and code-lengths]
A pair $(m,y)$ is a \emph{representation} for $x$ if $m$ is a model and $m(y)=x$. 

Define the following code-length functions for the representations:
\begin{itemize}
\item $L(m)$ is the length of a self-delimiting description of
  the model $m$, i.e. it is the length function of a prefix code for models. Each variant of sophistication must supply such a function.
\item $L^m(x)=L(m)+\min\{|y|:m(y)=x\}$ measures the number of bits required to describe both the model $m$ and the data $x$ using the model.
\item $L^\M(x)=\min\{L^m(x):m\in\M\}$ measures the number of bits
  required to describe $x$ using the best model in the model class.
\end{itemize}
\end{definition}
We can now define which of these representations express the data most efficiently:
\begin{definition}[Candidate set]
The \emph{candidate models} for $x$ from model class $\M$ and threshold function $\tau$ are
\[
  C^\M_\tau(x):=\{m\in\M:L^m(x)\le \tau(x) + c\}
\]
with $c$ a constant independent of $x$ which may be chosen arbitrarily. The default threshold function is $\tau^\circ(x)=L^\M(x)$.
\end{definition}

\begin{definition}[Sophistication]
  The model complexity, meaningful information, effective complexity, facticity or \emph{sophistication},  of a string $x$ given a model class $\M$ and threshold function $\tau$ is
  \[
  \s_\tau^{\M}(x):=\min\{L(m):m\in C^\M_\tau(x)\}\p
  \]\label{definition:sophistication}
\end{definition}
For a function like sophistication to be meaningful, it must satisfy certain properties. We will check all currently published variants against the following minimal requirements:

\begin{description}
  \item[Invariance] Like the Kolmogorov complexity, the construction of a sophistication function makes certain ad-hoc choices, which are captured in the choice of an arbitrary universal Turing machine $U$ or an arbitrary numbering of $\C$. If we change what choice we make, the Kolmogorov complexity remains invariant up to a constant. This is the sole reason we allow ourselves to interpret the Komogorov complexioty as a property of the data, rather than simply the result of some computation on it. A similar invariance must hold for any sophistication function.
   \item[Unboundedness] Both the sophistication and the residual information (roughly the complexity minus the sophistication) must not be bounded by a constant. If the sophistication is bekow a constant for all strings it cannot have much meaning. Also, if it is always withing a constant of the Kolmogorov complexity, it cannot correspond to the notion we hope to capture with it.
   \item[Efficiency] The two-part representation $(m, y)$ that witnesses the sophistication (that is the one that reaches the minimum described by $\s^\M_\tau$) must reach the Kolmogorov complexity (or at least the difference should be insignificant). If there is an effective one-part representation that is much more efficient than the two-part representation that witnesses the sophistication, we cannot, in all honesty, claim that $(m, y)$ captures all the information in $x$.
\end{description}

Given the philosophical underpinnings of sophistication, these three requirements are the very least that we could expect. We will show that no current proposal satisfies all three. 

For all variants but one, we show that all measures that are invariant are bounded. This leads us to the following conjecture. Let $\gamma(n)$ be a function representing a \emph{granularity}: the extent to which we care about differences between functions. That is, we will call two functions $f$ and $g$ $\gamma$-equal if $|f(x) - g(x)| \in O\left(\gamma\left(f(x)\right)\right)$.

\begin{conj}
Let $\s_g$ be a sophistication function constructed as in Definition~\ref{definition:sophistication}, built on some acceptable numbering $g$, and some fixed model class $\M$ and threshold function $\tau$ (omitted from the sub/superscript for the sake of clarity). 

For any sophistication function so constructed, and any granularity $\gamma$ we have that for any two acceptable numberings $g$ and $h$, either:
\begin{itemize}
  \item there exist $\s_g$ and $\s_h$ which are not are $\gamma$-equal (ie $\s$ is not invariant),
  \item $s_g$ is $\gamma$-equal to $0$ or to the Kolmogorov complexity.
\end{itemize}
\end{conj}

In these terms, all instantiations of the Kolmogorov complexity, using different Universal Turing machines are $1$-equal. It may be unreasonable to require the sophistication to attain the same level of invariance (a log-invariant sophistication could still be useful). This conjecture allows such relaxations: if it is true, sophistication must be considered fundamentally flawed. We hope that this conjecture will prompt authors to address the matter of invariance critically, and to consider the possibility that sophistication and its ilk are an untenable proposition.

In the rest of this paper, we will analyse the existing variants one by one. We will conclude with the informal reasoning supporting this conjecture and consider its the consequences. What does this mean for the problem of meaningful information, what does it mean for algorithmic statistics and most importantly, what are the consequences for the idea of model selection in general?

\section{Measures of meaningful information}

\subsection{Koppel's Sophistication}


\begin{itemize}
\item Model class is $\T$
\item Threshold function $\tau(x)=L^\M(x)+c$
\item Description method for models is required to be prefix-free, but
  can otherwise be chosen arbitrarily.
\item Koppel uses monotone complexity, and monotone functions and discusses 
  infinite strings, TODO figure out if and how this affects our results.
  \item Koppel has a `machine-independence result', but not one that does what we want.
\end{itemize}

We start with sophistication as originally introduced by Koppel and Atlan \cite{koppelSoph1988,koppel1991almost}. Their sophistication is built on a two-argument universal Turing Machine $U(i, y) = \phi_p(y)$. They consider a pair $(i, y)$ a representation of a string $x$ if (a) $\phi_i$ is total, and (b) $\phi_i(y)$ has $x$ as a prefix.\footnotemark 

\footnotetext{The original sophistication also allows infinite binary strings in the range of the computable functions considered. We ignore this aspect for the sake of simplicity, although our conclusions still hold with such functions included.}

This construction shows one of the first pitfalls of sophistication: using an inefficient representation for the model. We show that unless 
\begin{lemma}
Koppel and Atlan's sophistication is not both $\gamma$-invariant and unbounded for any $\gamma(n) \in o(n)$.
\end{lemma}
\begin{proof}
Assume that the sophistication is $\gamma$-invariant, for some $\gamma(n) \in o(n)$. We will show that this implies that it is bounded.

Let $\num{\phi}$ be the canonical acceptable numbering. Choose $\alpha < 1$ so that $\gamma(i) < \alpha i$ for sufficiently large $i$. Let $s: \N \to \B$ be a computable function such that $|s(i)| = \left\lceil(1-\alpha)^{-i}\right\rceil$.

We now define two new acceptable numberings $\num{\psi}$ and $\num{\xi}$:
\[
\psi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s(2i) \\
	\infty\;\text{otherwise}
\end{cases}
\]
and 
\[
\xi_j(x) = 
\begin{cases}
	\phi_i(x) \;\text{if}\; j = s(2i+1) \\
	\infty \;\text{otherwise}\p
\end{cases}
\]
Let $\s_\psi$ and $\s_\xi$ be the sophistication functions built on these enumerations. Choose any $x$ and assume w.l.o.g. that $\s_\psi(x) < \s_\xi(x)$. By construction of the numberings $\psi$ and $\xi$, we have $\s_\psi(x)/ (1-\alpha)  \leq \s_\xi(x) + 1$, which gives us:
\begin{align*}
	\s_\xi(x) - \s_\psi(x) &\geq \alpha\, \s_\xi(x) - 1\\
	& \in \Omega(\s_\xi(x)) - 1\\
\end{align*}
while, by our original assumption $\s_\xi(x) - \s_\psi(x) \in O(\gamma(\s_\xi(x))) \subset o(\s_\xi(x))$, which contradicts the bound above.
\end{proof}

The main trick we use in this proof is that we can inflate the indexes arbitrarily. In short, the representation of the models is highly inefficient. For the definition of Kolmogorov complexity, this is not a problem, since we can switch to another universal machine at only a constant cost, but for the definition of sophistication, we must make sure that both parts of our two-part code are efficient.

We define the complexity of a given model as its shortest effective description: a description that can be used to compute the function for a given input: 

\begin{definition}
  The complexity of a partial recursive function is defined by
  \begin{align*}
    C(f) &:= \min\{C(i):\phi_i=f\}\\
    K(f) &:= \min\{K(i):\phi_i=f\}.
  \end{align*} \label{definition:model-complexity}
\end{definition}
These definitions also appear in \cite{} and \cite{}, but are not discussed in depth. We will show that these properties are still invariant up to a constant (ie. complexity is a property of a function):
\begin{lemma}[Invariance]
Let $\phi_1, \phi_2, \ldots$ and $\psi_1, \psi_2,\ldots$ be two acceptable numberings. There exists a constant $c$ such that $\left| K^\phi(f) - K^\psi(f)\right | \leq c$ for all $f$. \label{lemma:invariance}
\end{lemma}
\begin{proof}
Let $g(i)$ be the function which which translates from $\psi$ to $\phi$.
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(i) : \phi_i= f\right\} \\
&\geq \min\left\{ K^\psi(i) : \phi_i= f\right\} - c\\
&= \min\left\{ K^\psi(i) : \psi_{g(i)}= f\right\} - c\\
&\geq \min\left\{ K^\psi(i) : \psi_i= f\right\} - c\\
&= K^\psi(f).
\end{align*}
We can reverse $\phi$ and $\psi$ without loss of generality to achieve the same inequality the other way around, completing the proof.
\end{proof}

This lemma shows that the complexity of a function depends on the choice of
enumeration by no more than a constant term. Note that this implies
that for any enumeration $\psi_1,\psi_2,\ldots$, the complexity $K(\psi_i)$
is less than the literal description length of the index
$|\bar\imath|$ up to a constant. For some numberings, there is a gap
between these two description methods; in those cases, the index is an
inefficient representation of the corresponding function. However it
is possible to construct numberings that are \emph{faithful} in the
sense that no such gap exists:

\begin{definition}[Faithful Numbering]\label{def:faithful}
  A numbering $\psi_1,\psi_2,\ldots$ of the partial recursive
  functions is \emph{faithful} if there is a constant $c$ such that
  for all indices $i$ there is a $j$ such that $\psi_i=\psi_j$ and
  $|j|\le C(\psi_j)+c$.
\end{definition}

\begin{lemma}
  There is a faithful acceptable numbering.
\end{lemma}
\begin{proof}
Let $i_\tn{div}(y)$ be an index such that $\phi_{i_\tn{div}}(y)=\infty$ for all $y$. Define
  \[\psi_q=\begin{cases}
    \phi_{\phi_i(p)}&\tn{if $q$ can be written as $\bar\imath p$ and $\phi_i(p)<\infty$,}\\
    \phi_{i_\tn{div}}&\tn{otherwise.}\end{cases}
  \]
  To show that $\psi$ is faithful, pick any function $f$. Then
\[\begin{split}
C(f)&=\min\{C(i):\phi_i=f\}\\
&=\min\{\min\{|\bar a b|:\phi_a(b)=i\}:\phi_i=f\}\\
&=\min\{|\bar a b|:\phi_{\phi_a(b)}=f\}\\
&=\min\{|\bar a b|:\psi_{\bar a b}=f\}.
\end{split}\]
This shows there is a sufficiently small $\psi$ index.

To show that $\psi$ is acceptable, let $\phi_j$ denote the identity
function. Then a $\phi$-index $i$ can be mapped to a $\psi$-index
using the computable function $r(i)=\bar\jmath i$, so that
$\psi_{r(i)}(y)=\psi_{\bar\jmath i}(y)=\phi_i(y)$. For the reverse,
define $\phi_v(\bar\imath p, y)=\phi_{\phi_i(p)}(y)$. For fixed
$\bar\imath p$, the 
$s^n_m$-theorem (see~\cite{TODO}) states that we can compute the $h$
such that $\phi_h(y)=\phi_v(\bar\imath p,y)$. Let $h(\bar\imath p)$
denote this index as a function of the program; further define
$h(q)=i_\tn{div}$ if $q$ cannot be expressed as $\bar\imath p$. By
construction $h$ is total and computable. To check that the mapping
returns the correct function, rewrite $\phi_{h(\bar\imath
  p)}(y)=\phi_v(\bar\imath p,y)=\phi_{\phi_i(p)}(y)=\psi_{\bar\imath p}(y)$.
\end{proof}

\subsection{Antunes's Coarse Sophistication}

\begin{itemize}
\item Model class is $\T$
\item Description method $L(m)$ for models is unclear, but appears to be
  taken from Koppel, i.e. arbitrary but self-delimiting.
\item Sophistication is defined slightly differently: $S^\T(x)=L(m^*)$ where $m^*=\argmin_m L(m)+L^m(x)-C(x)$. Thus, there is no slack.
\end{itemize}


\subsection{Adriaans' Facticity}

\begin{itemize}
\item Model class is $\C$
\item Description method for models is $L(m)=|\bar\imath|$ where
  $\phi_i=m$. The index function is assumed to be \emph{faithful}, see
  Definition~\ref{def:faithful}
\item Threshold function is $\tau(x)=L^\M(x)$, the constant in the candidate set is $0$.
\item Facticity is actually defined as index length without prefix
  encoding:  $S^\C_0(x):=\min\{|i|:\phi_i\in C^\C_0(x)\}$. But this
  difference is not important for the discussion.
\end{itemize}


\begin{lemma}
  If $\min\{i\ge i_0:|\bar\imath|-K(i)\}$ is an unbounded function of $i_0$, then facticity is bounded.
\end{lemma}
\begin{proof}
Let $\bar\imath p$ be any two-part representation for the data $x$, i.e. $\phi_i(p)=x$. Then construct an alternative two-part representation $\bar vi^* p$, where $i^*$ is the shortest $\psi_u$-program for $i$ such that $\phi_v(i^* p)=\phi_{\psi_u(i^*)}(p) = \phi_i(p)=x$. We compare the lengths of these two representations. Note that $|i^*|=K^\psi(i)$. Therefore,
\[
|\bar\imath p|-|\bar v i^* p| = |\bar\imath|-|\bar v| - |i^*| = |\bar\imath|-|\bar v|-K(i).
\]
By assumption there must be an $i_0$ such that the above expression is positive for all $i>i_0$. From this $i$ onwards, the second representation (using $v$) will have a shorter code length, so $\bar\imath p$ cannot achieve the minimum in the definition of the facticity. Consequently, the facticity is bounded by $F^\phi(x)<|\overline{\imath_0}|$ for all $x$. 
\end{proof}
Note that the condition for this lemma applies to all known partial recursive prefix functions:

\begin{conjecture}
For any partial recursive prefix code length function $L$, the function
\[
\min\{L(i)-K(i):i\ge i_0\}
\]
is unbounded in $i_0$.
\end{conjecture}

\subsection{Effective complexity}

\begin{itemize}
\item Model class is \emph{ensembles}. Equivalent to $\K$. Subsequent literature suggests that ensembles have finite support, which would mean that effective complexity reduces to the structure function.
\item Description method for models is $L(m) = K(m)$ (the minimization is not mentioned explicitly, but we'll give them the benefit of the doubt).
\item The threshold function is equal to the minimum two-part code: $\tau(x)=L^m(x)$.
\item Gell-mann and Lloyd acknowledge that representations exist with small and large models. The choice from the candidate set is made by applying a time-bound to the computation from the string to the model, but not to the decoding of the model itself. The idea is that this forces `deep' information into the model, and shallow information into the data. Presumably, the representation with the smallest model code under this constraint is chosen.   
\end{itemize} 

\pb{\cite{ay2010effective} provides a more formal definition.}
\begin{theorem}
There exists a UTM so that for all $x$, $\s_\text{EC}(x) = K(x)$, up to a constant. 
\end{theorem}

Subsequent publications suggest that all ensemebke  

\begin{theorem}[Reduction of Effective Complexity to Structure
  Function]
  There is a constant $c$ such that, for every representation of $x$
  consisting of an ensemble $E$ with $x\in E$ with entropy $H$,
  yielding a total information of $K(E)+H$, there is a finite set $S$
  with $x\in S$ such that $K(S)\le K(E)+c$ and $\log|S|\le H$.
\end{theorem}
\begin{proof}
  First note that although the ensemble $E$ may itself be infinite, it
  is ``typical'', and therefore a valid candidate representation, for
  only a finite number of its elements, namely the set $S=\{y\in E:-\log
  P(y)\le H\} = \{y:P(y)\ge 2^{-H}\}$. Since the sum of probabilities must
  converge, this set cannot be infinite: since $1\ge\sum_{y\in S} P(y)\ge
  \sum_{y\in S}2^{-H} = |S|2^{-H}$ so $|S|\le 2^H$.


\end{proof}



\subsection{Roll your own}


\begin{itemize}
\item Model class is either $\C$ or $\K$
\item Description method for models is $L(m)=K(m)$.
\item Threshold function is $\tau(x)=L^m(x)+c$, which matches $C(x)$
  resp. $K(x)$ up to a constant.
\end{itemize}


\begin{lemma}\label{lem:thecoolone}
  Let $\psi_1,\psi_2,\ldots$ be any acceptable enumeration of the partial recursive functions.
  Let $\M$ be any model class, let $\X$ be any set of binary sequences and let $D:\{0,1\}^*\to\N$ be a computable decoding function with a prefix-free domain that maps function descriptions to their indices in $\psi$, i.e. if $f=\psi_{D(p)}$ then $p$ is a $D$-description of $f$. Let $\M'=\{\psi_i:i\in\tn{range}(D)\}$. Further assume there is a constant $c$ such that
\begin{enumerate}
  \item $\forall_{f\in\M'}:\min\{|p|:\psi_{D(p)}=f\}\le K^\psi(f)+c$
  \item $\forall_{x\in\X}:L^{\M',\psi}(x)-L^{\M,\psi}(x)\le c$.
\end{enumerate}
Then is an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions such that $S^\phi_{\M}(x) = |\bar 0|+S^\psi_{\M'}(x)$ for all $x\in\X$.
\end{lemma}
\begin{proof}
We define the numbering $\phi$ as follows:
\[\begin{cases}
\phi_0(p) = 1^r 0 D(p) \\
\phi_{1^r0i}(p) = \psi_i(p) \\
\phi_j(\cdot) = \infty &\text{if $j$ contains no zeroes.}
\end{cases}\]
We will show that under the $\phi$-enumeration, the best representation for $x$ using a model $f\in\M'$ is always better than the best representation using some $f\not\in\M'$.

First suppose $f\in\M'$. Then
\[\begin{split}
K^\phi(f)&=\min\{K^\phi(i):\phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_j(q)=i, \phi_i=f\}\\
&=\min\{|\bar\jmath q|:\phi_{\phi_j(q)}=f\}\\
&\le\min\{|\bar0 q|:\phi_{\phi_0(q)}=f\}~\text{(using $j=0$)}\\
&=\min\{|\bar0 q|:\phi_{1^r0 D(q)}=f\}\\
&=|\bar 0|+\min\{|q|:\psi_{D(q)}=f\}\\
&\le K^\psi(f)+c+|\bar 0|,
\end{split}
\]
where the last inequality uses the first assumption.


Now assume that the best model $f$ for $x$ is not in $\M'$. 
Let $i$ be the index of $f$ with the shortest description, i.e. it achieves the minimum in
\[
K^\phi(f)=\min\{K^\phi(i):\phi_i=f\}.
\]
There are two possibilities. Either $i=0$, in which case we have $K^\phi(f)\ge r$ because $\phi_0$ cannot output zero and all other $\phi$-programs are at least $r$ bits long.

Otherwise, we can bound
\[\begin{split}
K^\phi(f)&=K^\phi(i)=K^\phi(1^r0j)\\
&\ge K^\phi(j)-c'\\
&\ge K^\psi(j)+r+1-c'\\
&\ge\min\{K^\psi(j):\phi_j=f\}+r+1-c'\\
&=K^\psi(f)+r+1-c'.
\end{split}
\]
Now choose $r=c''+\max\{K^\psi(\phi_0),c'-1\}$. Then substitution yields, for both cases, 
$K^\phi(f)\ge c''+K^\psi(f)$.

Combining the inequalities above, for any model $g\not\in\M'$, there is a model $f\in\M$ such that
\[\begin{split}
K^\phi(g)+C^g(x) &\ge K^\psi(g)+C^g(x)+c''\\
&\ge K^\psi(f)+C^f(x)-c+c''\\
&\ge K^\phi(f)+C^f(x) -2c+c'' -|\bar0|.
\end{split}\]

By choosing $c''$ sufficiently large, we can ensure that the best representation is in $\M'$ for all $x\in\X$, which completes the proof.
\end{proof}

\paragraph{TODO: instantiate for UTM}

\begin{itemize}
\item Model class is $\K$
\item Description method for models is $L(m)=K(m)$.
\item Threshold function is $\tau(x)=L^m(x)+c$, which matches $K(x)$
  up to a constant.
\end{itemize}


\begin{lemma}
Let $\M$ be a prefix model class where for every $x\in\X$ there is a singleton model $f\in\M$ with $f(\epsilon)=x$. Then there is an enumeration $\phi_1,\phi_2,\ldots$ of the prefix partial recursive functions, and a constant $c$, such that
\[
K(x)-S^{\M,\phi}(x)\le c
\]
for all $x\in\X$.
\end{lemma}
\begin{proof}
Let $\psi_1,\psi_2,\ldots$ be any default enumeration of the partial recursive prefix functions. Note that since $f$ is a prefix function, if $f$ is defined for input $\epsilon$ then it cannot be defined for any other input. Pick $f,x$ with $f(\epsilon)=x$. Note that $x$ can be computed from $f$ and a fixed program, so there is a $c$ such that $K(x)\le K(f)+c$. Vice versa, given any $x$ we can construct an index of $f$, since $\psi$ is an acceptable numbering. Therefore $|K(f)-K(x)|\le c$.

We now define a computable function $D$ by $D(\bar\imath p)=j$ where $\psi_j(\epsilon) = \psi_i(p)$.  We will show that the two conditions of Lemma~\ref{lem:thecoolone} hold for the prefix function $D$.

\begin{enumerate}
\item Let $f$ be any function in the range of $D$, and $x$ its output. Then $\min\{|p|:\psi_{D(p)}=f\}=\min\{|\bar\imath q|:\psi_i(q)=x\}=K(x)\le K(f)+c$.
\item On the one hand $L^{\M',\psi}(x)\le K(f)+|\epsilon|\le K(x)+c$. On the other hand, $L^{\M,\psi}(x)$ is an effective description of $x$, so $K(x)$ is at most a constant larger.
Together, these inequalities establish the second condition.
\end{enumerate}

Then by Lemma~\ref{lem:thecoolone} there is an enumeration $\phi_1,\phi_2,\ldots$ such that $S^{\M,\phi}(x)=|\bar 0|+S^{\M',\psi}(x)$. We observed that $|K(f)-K(x)|\le c$ for any $f\in\M'$, so $S^{\M',\psi}(x)\ge K(x)+c$. This proves the lemma.
\end{proof}

\subsection{Kolmogorov's Structure Function}

\begin{itemize}
\item Model class is $\F$
\item Description method for models $L(m)=K(S)$ where $S$ is the range
  of $m$. There is some ambiguity as to the definition of prefix
  complexity for finite sets; discuss and refer to Shen \cite{TODO}.
\end{itemize}

\begin{theorem}
There are infinitely many $x$ such that $S^\F_K(x) \geq K(x)/5$. \label{theorem:structure-function-is-not-bounded}
\end{theorem}
\begin{proof}
From \cite[Proposition~I.3 (b)]{gacs2001algorithmic} we know that for sufficiently large $i$ there are strings of length less than $i$ that are not $(i/4, i/4)$-stochastic. For each $i$, let $x_i$ be the smallest binary string that is not $(i/4, i/4)$-stochastic. Since every string is $(\alpha,\alpha)$-stochastic for sufficiently large $\alpha$, this sequence contains infinitely many distinct binary strings. Now pick any $i$ that is large enough that (a) $|x_i|<i$ and (b) $K(x_i)/5 < i/4$. By definition of $(\alpha,\beta)$-stochasticity, if a string is not $(i/4,i/4)$-stochastic, then for every finite set $S$ containing $x$, either (a) $\log|S|\ge K(x_i)+i/4$, in which case $S$ cannot be a candidate representation for $x$ in $C^\F_K$, or (b) $K(S)>i/4>K(x_i)/5$. This proves the theorem.
\end{proof}

\subsection{Vit\'anyi's Sophistication / Meaningful Information}

\begin{itemize}
\item Model class is $\T$
\item Description method for models is $L(m)=K(m)$
\item Candidate set is defined differently: $C_c=\{m\in\T:|L^m(x)-K(x)|\le
  c\}$. There appears to be a problem with this definition, because
  the two-part descriptions of $x$ may be shorter than $K(x)$ by
  exploiting the fact that the models do not need to be prefix functions.
\end{itemize}

% stru fu: model class is all finite sets
% code length L(m)+L_m(x)
% acceptability criterion acc(m) = 2L(m)+L_m(x)-C(x)
% 

\begin{lemma}
Let $f$ be a total, computable function, such that for some $d$, $f(d) = x$ and let $k = K(f) + |d|$. Then there exists a finite set $S$ containing $x$ and a constant $c$, such that $K(S) \leq K(f) + K(|d|) + c$ and $\log |S| \leq |d|$.\label{lemma:total-to-sets}
\end{lemma}
\begin{proof}
Let $S = f\left(\{0,1\}^{|d|}\right)$. Since $f$ is total, this set can be explicity computed from a description of $f$ and a description of $|d|$, which tells us that there is a constant $c$ such that $K(S) \leq K(f) + K(|d|) + c$. 

Since $S$ is the image of a set of size $2^{|d|}$ under $f$, we have $\log |S| \leq |d|$.
\end{proof}
This lemma is a variation on \cite[Lemma~7.2]{vitanyi2004meaningful}.

We use this lemma to transport the result of Theorem~\ref{theorem:structure-function-is-not-bounded} to the model class of total functions:

\begin{theorem}
There are infinitely many $x$ with 
\begin{enumerate}
  \item $S^\T_K(x) \geq K(x)/6$, {and}\label{eq:poezenvoer}
  \item $S^\T_{\tau^\circ}(x) \geq K(x)/6$. \label{eq:hondevoer}
\end{enumerate}
\end{theorem}
\begin{proof}
We continue from the proof of Theorem~\ref{theorem:structure-function-is-not-bounded}. Suppose towards contradiction that either $C^\T_K(x_i)$ or $C^T_{\tau^\circ}(x_i)$ contains a candidate $f$ with $K(f)\le i/5$. Then by Lemma~\ref{lemma:total-to-sets}, there is a set $S$ with $\log|S|\le|d|\le\tau^\circ(x)\le K(x)$, so (a) in the proof of Theorem~\ref{theorem:structure-function-is-not-bounded} above cannot be the case, and $K(S)\le K(f)+K(|d|)+c\le i/5+K(|d|)+c$, which contradicts (b) in the same proof for large enough $i$. Therefore such candidates $f$ cannot exist and all candidates must satisfy $K(f)>i/5>K(x_i)/6$.
\end{proof}
Note that we favoured simplicity of the proof over the strength of the bounds: the multiplicative constant $1/6$ can be improved with a more detailed treatment.
\section{A world without sophistication}

We have conjectured that sophistication is a lost cause, and that the notion is impossible to formalize in a satisfying manner. 

\pb{Procedural view: we cannot between dominating models. This may not be the aim of algorithmic statistics, but it suggests that it's kind of unreasonable to ask for model selection without repeated samples, or assumptions on the model class.} 
\pb{The problem with making a fixed choice.}
\pb{The existence of balanced strings.}
\pb{Total functions. They ruin efficiency and push sophistication into a land of unprovable claims.}

However, even if sophistication must be abandoned, that does not mean that the efforts surrounding it are lost. The are many elements of algorithmic statistics that do not rely 
\pb{Non-stochastic strings are in some sense invariant}
\pb{We can eliminate almost all models. Complexity is countevidence.No difficult business with total functions.}

Finally, the question remains how we can solve the matter of meaningful information without the ability to perform model selection. \pb{All candidates likely Take the finite set case. }

\subsubsection*{\ackname}

This publication was supported by the Dutch national program COMMIT and by the the Netherlands eScience center.

\bibliographystyle{plain}
\bibliography{facticity}

\appendix

\section{Stuff to move up or delete}

\hide{

\subsection{Plan}

Perhaps surprisingly, it is also reinforced by Vit\'anyi, when he writes : (From Vereshchagin and Vit\'anyi, ``Kolmogorov’s Structure Functions and Model Selection'':)
\begin{quotation}
  This expression emphasizes the two-part code nature of Kolmogorov complexity. In the example
\[x=10101010101010101010101010\]
we can encode $x$ by a small Turing machine printing a specified number of copies of the pattern `10' which computes $x$ from the program `13'.” This way, $K(x)$ is viewed as the shortest length of a two-part code for $x$, one part describing a Turing machine, or model, for the regular aspects of $x$, and the second part describing the irregular aspects of $x$ in the form of a program to be interpreted by $T$. The regular, or “valuable,” information in  is constituted by the bits in the “model” while the random or ``useless'' information of $x$ constitutes the remainder.
\end{quotation}

\subsection{Conclusion}
Stel onze conjecture is waar, is het dan helemaal nutteloos? Nee, want lengte van two-part representations is nog steeds een redelijke maat voor counterevidence against a model. No-hypercompression, $p$-values, randomness deficiency. Probleem zit in harde cutoffs zoals minimalisatie en een vaste constante waarboven representaties niet meer meedoen. Daar gaat invariantie van kapot.

Onze mening: als onze conjecture waar is, dan moet je constraints weglaten: maak het makkelijk en doe prefix functies en alle partial recursive functions, en accepteer de collapse gewoon. Maar dat betekent weer dat je een hele wolk van two-part optimale representaties hebt waar je niet tussen moet willen kiezen.


Er is een gat tussen $K(f)+|y|-K(x)$ en $\delta(x\mid f)$. De laatste is de juiste maat voor de counterevidence tegen $f$.

\subsection{Thoughts and questions}

\begin{itemize}
\item Paul argues bleurg that the randomness deficiency of a model is a measure of the amount of structure left in the noise part. I feel that it is also a measure of the amount of counterevidence against a model in the statistical sense, as follows: the randomness deficiency of $x$ given model $M$ is $\delta(x|M)=L_M(x)-K(x|M)$. Suppose that model $M$ is true. We can then use the no hypercompression inequality to rewrite:
%
\[
P_{x\sim M}(\delta(x|M)\ge c) = P_{x\sim M}(L_M(x) - K(x|M) \ge c)\le 2^{-c}
\]
Thus, if $M$ is true the probability of a large randomness deficiency is exponentially small! This amount of evidence is robust in the sense that changing the UTM around will change the amount of evidence by at most a constant. If the structure function is going to give us a \emph{cloud} of candidate models instead of just a single one, I think this is a really nice interpretation of the meaning of that cloud: we simply measure the available evidence against all individual models.  

\item Shen explains the collapse of the structure function if you use enumerating complexity rather than listing complexity. How can we most cleanly explain that this collapse does not vanish for weaker model classes?

\item Properties of representations on $K(M)$ vs $L_M(x)$ graph: diagonals identify representations with the same two-part codelength. The randomness deficiency is the distance to the minimum, which is the $K(x)$ diagonal:
\[L(x;M)=K(M)+L_M(x) =K(M)+K(x|M)+L_M(x)-K(x|M)  = K(x)+\delta(x|M).\]
 Models move horizontally by a constant when the ordering of TMs is changed. Thus the randomness deficiency and two part code are also changed by at most a constant. In contrast the \emph{minimum} can now be achieved by a very different representation.

\item ``Power and peril of MDL'': bespreken ook de ``collapse of the structure function'' als de model class te sterk is, dwz sets worden te efficient gecodeerd (wsch equivalent met Shen's enumerating complexity).

\end{itemize}

\subsection{Dead darlings}

\begin{definition}
Given an enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive functions, and an enumeration $\psi_1,\psi_2,\ldots$ of the partial recursive prefix functions. Let $\psi_u$ be a universal element with property $\psi_u(\bar\imath p)=\psi_i(p)$. Then define $v$ by
\[
v(a p)=\begin{cases}\phi_{\psi_u(a)}(p)&\tn{if $a$ is in the domain of $\psi_u$,}\\
\infty&\tn{otherwise}.\end{cases}
\]
\end{definition}
\begin{lemma}
The function $v=\phi_v$ as defined above is partial recursive.
\end{lemma}
\begin{proof}
Let $r$ be the input of $v$. Simulate $\psi_u$ using a TM with a one-way read-only input tape. If the TM does not halt, then enter an infinite loop. Otherwise, let $r=ap$ where $a$ is the part of the input tape that has been read when the TM halts, and let $i$ be the output of the TM. Now simulate $\phi_i$ on input $p$.
\end{proof}

\begin{lemma}
Assume the prefix function $\bar\cdot$ used in the definition of complexity is such that $\bar x-x$ is monotonically increasing. Let $\psi_1,\psi_2,\ldots$ be any acceptable numbering. Then for every $c$ there is another acceptable numbering $\phi_1,\phi_2,\ldots$ such that for all partial recursive functions $f$ we have
\[
K^\phi(f)-K^\psi(f) \ge c.
\] \label{lemma:building-block}
\end{lemma}
\begin{proof}
First note that there is a $c'$ such that 
\begin{equation}
	K(1^r 0  x) + c' \geq K(x) \label{eq:prelim1}
\end{equation}
for all $x$, since we can create a program that first runs the shortest program for $1^r 0 x$ and then strips the prefix $1^r 0$ from the result.

Define $\phi_{1^r0i}(x) = \psi_{i}(x)$, with $\phi_{j}(x) = \infty$ if $j$ does not contain a zero. Intuitively, any $\phi$-program is $r+1$ bits longer than the corresponding $\psi$-program. However, the used prefix function complicates this slightly. But all we need is the bound
\begin{equation}\label{eq:prelim2}\begin{split}
K^\phi(x) &= \min \{ |\overline{1^r 0 \imath}| y : \phi_{1^r 0 i}(y) = x \}\\
&  \ge \min\{|1^r0\bar\imath y| : \phi_{1^r 0 i}(y) = x \}\qquad\tn{(using assumption on $\bar\cdot$)}\\
& = r+1 +\min\{|\bar\imath y| : \psi_i(y) = x \}\\
& = r+1+K^\psi(x).
\end{split}\end{equation}

We can now relate the complexity of $f$ under the two enumerations as follows:
\begin{align*}
K^\phi(f) &= \min\left\{ K^\phi(j) : \phi_j = f \right\}& \\
       &= \min\left\{ K^\phi(1^r0i) : \phi_{1^r0i} = f \right\}& \\
       &\ge  \min\left\{ K^\phi(i) : \phi_{1^r0i} = f \right\}-c'&\text{by~\eqref{eq:prelim1}}\\
       &\geq \min\left\{ K^\psi(i)  : \phi_{1^r0i} = f \right\}+r+1-c'& \text{by \eqref{eq:prelim2}}\\
       &= \min\left\{ K^\psi(i) : \psi_{i} = f \right\} + r+1 -c'& \\
       &= K^\psi(f) + r + 1 - c'.
\end{align*}
The proof is completed by choosing $r=c+c'-1$.
\end{proof}

\begin{definition}
  An enumeration $\phi_1,\phi_2,\ldots$ of the partial recursive
  functions is called \emph{compact} if for all $i,y$ there is a $j$
  such that $|j|\le|i y|$ and $\phi_i(y z)=\phi_j(z)$ for all $z$.
\end{definition}

\begin{lemma}
There is a compact acceptable numbering.
\end{lemma}
\begin{proof}
  Let $\phi_1,\phi_2,\ldots$ be any acceptable numbering. Let $\psi_1,\psi_2,\ldots$ be a new acceptable numbering defined by $\psi_j(x)=\phi_i(y x)$ if $j$ is a pair satisfying $j=\bar\imath y$, and $\psi_j(x)=\infty$ otherwise.  We will show that $\psi$ is both $0$-compact and acceptable. 

To show compactness, pick any index $i$ and binary string $y$. First assume $i$ is a valid pair $i=\bar a b$. Then  $\psi_i(y x) = \psi_{\bar a b}(y x)=\phi_a(b y x)$, but also $\psi_{\bar a b y}(x)=\phi_a(b y x)$, so $\psi_i(y x) = \psi_j(x)$ for $j=\bar a b y$; so $|j|=|iy|+0$. On the other hand, if $i$ is not a valid pair, we can simply use $j=i$ so that $\psi_i(y x)=\psi_j(x)=\infty$ for all $x$.

To show that $\psi_1,\psi_2,\ldots$ is acceptable, we define total
computable functions $f$ and $g$ such that $\psi_i(x)=\phi_{f(i)}(x)$
and $\phi_j(x)=\psi_{g(j)}(x)$. The easy direction is
$g(j)=\bar\jmath\epsilon$. The definition of the mapping in the other
direction $f(i)$ depends on whether $i$ is a valid pair. If it is not,
then $f(i)=h$ for some $h$ such that $\phi_h(x)=\infty$ for all $x$. On the other hand if $i=\bar\jmath y$, then $f(i)$ is the index of a partial recursive function $\phi_{f(i)}(x)$ defined by a Turing machine that simulates the machine for $\phi_j$ but with $yx$ on the input tape.
\end{proof}


\begin{lemma}
Compact numberings are sort of faithful.
\end{lemma}
\begin{proof}
First, let $v$ be the index of the partial recursive function given by $\phi_v(\bar k p x)=\phi_{\phi_k(p)}(x)$. Let $\bar k p$ be a shortest program for $i$, that is $|\bar k p| = K(i)$ and $\phi_k(p)=i$. Then $\phi_i(x)=\phi_v(\bar k \bar p x)$. We now use compactness to find an index $j$ with $\phi_v(\bar k \bar p x)=\phi_j(x)$ and length
\[|j|\le |v\bar k\bar p|\le|v|+|\bar k p|+2\log|\bar k p|=K(i)+|v|+2\log K(i),\]
as required.
\end{proof}
}

\end{document}

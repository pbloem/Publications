\documentclass{style/llncs}

\title{The problem of Sophistication}

\author{Peter Bloem and Steven de Rooij}

\institute{
  System and Network Engineering Group, \\University of Amsterdam, the Netherlands\\
  \email{uva@peterbloem.nl, steven.de.rooij@gmail.com}
}

\begin{document} 
\maketitle

\begin{abstract}
Kolmogorov complexity formalizes the idea of information content, but clashes with intuitive ideas of complexity: data seems most complex to us when it contains structure, like language or interesting visual information, not when it contains only random noise.

\hspace{0.05\textwidth}A popular approach is to assume that an object consists of \emph{structural} and \emph{stochastic} information, and that only the former determines the complexity. A dataset $x$ is separated into structural and stochastic parts by representing it as a two-part code: a computable function $\phi$, the \emph{model}, and a string $y$, the \emph{input}, such that $\phi(y) = x$. The aim is that for efficient representations, $\phi$ contains all structure, and $y$ contains all stochastic information. Many proposals, including the structure function, the minimal sufficient algorithmic statistic and sophistication fit this mold. We use \emph{sophistication} as a catch-all term. 

\hspace{0.05\textwidth} We highlight two issues. First, it is crucial to correctly measure the information content of the model. All treatments rely on a \emph{numbering}; a canonical description of the computable functions. Kolmogorov complexity is not affected by the choice of numbering by more than a constant, but for sophistication the impact can be dramatic when the length of the plain description of $\phi$ is used to measure information content.

\hspace{0.05\textwidth} Second, we discuss the problems of over- and underfitting. To give an extreme example of underfitting: we can use a universal Turing machine as a model. This representation has all information in the input, yet is maximally efficient, up to a constant. We show that for some numberings, such models are always preferred, leading to a bounded sophistication. 

\hspace{0.05\textwidth} Underfitting is a known issue which many treatments seek to circumvent by restricting the class of models to total functions. This leads to the opposite problem: for some numberings, all information ends up inside the model. Only one formalization escapes this problem by allowing non-self-delimiting descriptions. While the question whether this form is invariant remains open, we show that numberings exist that cause it to be bounded for all but extremely deep strings. Thus, it certainly does not fit the original idea behind sophistication.

\hspace{0.05\textwidth} We draw three conclusions. First, sophistication is an interesting problem that leads to diverse questions about statistics, computability and compression. Second, we show that all currently published approaches are either incorrect, or do not behave as desired. Finally we propose that a sound definition of sophistication, capturing the right intuition may be impossible. We ask what we can truly discover about individual objects if our only assumption is that the model should be computable.
\end{abstract}

\end{document}
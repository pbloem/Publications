\documentclass{article}

\usepackage{charter}
\usepackage{eulervm}
\usepackage{amsmath, amsthm, amssymb}

\theoremstyle{definition}
\newtheorem*{thm}{Theorem}
\newtheorem{lma}{Lemma}
\newtheorem*{dfn}{Definition}
\newtheorem*{exm}{Example}

\title{Review: {\em Lossless compressors: Degree of compression and optimality}}
\date{\today}

% non-indented, spaced paragraphs
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

\begin{document}

\maketitle

\section{Global review}

This paper aims to analyse compression from an algorithmic information point of view. Its three main contributions are:
\begin{itemize}
  \item A definition of a normalization procedure for compression algorithms, which ensures that the compressed string is always shorter than or equal to the length of the original string (plus one bit), at the cost of one bit. (section 2)
  \item An analysis, mostly by counting arguments, of the expected compression under the uniform distribution. (section 3)
  \item An application of Levin's theorem to compression, suggesting that there is an algorithm which can match the performance of any compressor to within a multiplicative constant. (section 4)
\end{itemize}

Overall, the paper is clearly written, and easy to understand for readers with a reasonable background.

It is difficult for me to asses to what extent the paper presents novel or relevant results. The results discussed in section 3, for example, have clear analogues in Kolmogorov complexity, as does the result of section 4. Specifically the fact that the authors have presented their results almost, but not quite in the framework of algorithmic information theory makes it difficult to place the paper in a greater context.

To remedy this, the paper would benefit from an explicit explanation of how these results are relevant, how they differ from existing research, and why they are beneficial to ongoing efforts. Preferably, this should be done in bookending fashion, by extending the introduction and finishing with a \emph{conclusions} section.

The existing section ``Relationship with algorithmic information theory and with Koppel's sophistication'' underplays the relationship of these results to algorithmic information theory.

My intuition is that these results, if they haven't been published explicitly yet, are at least part of the `folklore' of the field of Kolmogorov complexity. If this is the case, it wouldn't hurt to make them explicit, but by themselves they lack sufficient impact for a publication of this kind.

My overall impression is that the authors are on the road to interesting results for which these are lemmas, but that in its current form the paper only presents reformulations of well known properties. If this impression is unjust, the paper would be improved by making it explicit why, to prevent other readers from forming the same impression.

\section{Specific review}
\subsection*{Section 1}

The introduction is clear and well written.

\subsection*{Section 2}

The idea that compressors may actually inflate, rather than compress, seems like an important theoretical detail. The method of normalization is helpful for theoretical purposes.

The normalization of the universal Turing Machine on page 2 can also be achieved by setting an empty Turing Machine $T(x) = x$ at index 0 in a prefix coded enumeration. This method does not have the drawback of adding a bit of complexity to all compressible strings as well. Both methods, however, trade universality for specificity. $C(x)$ becomes more well-behaved, but holds for fewer computational metaphors. This is the reason that it is generally asserted that $C(x) \leq |x| + O(1)$ instead of replacing the constant order with a $1$. 

The phrasing in the abstract that the authors ``reformulate a particular result of Algorithmic Information theory'' attaches too much importance to this method.

\pagebreak[4]

\subsection*{Section 3}
My main issue with this section is that (as the authors themselves assert at the top of page 8) compression over the uniform distribution is largely useless, as the uncompressible strings will quickly grow towards density one. Thus the expected compression under this distribution will be 0 for any computable method.

When quantifiying over all compressors (like in section 3.1 definition 1), the set over which this quantification happens is never defined.

While Theorem 1 is quite clear, its proof is difficult to follow. In particular the step that starts with ``there must be some maximum value \ldots'' could do with greater explanation.

The last line of the derivation in the middle of the proof to Theorem 2 should read $= 2^n(n-2) + 2 + n$ rather than $= 2^n(n-2) + 2$, I believe.

The relevance of Theorems 3 and 5 is not clear to me. If we are free to choose any values for $\{\epsilon_i\}$, the result does not appear that remarkable. Perhaps an example of the values of $\{\epsilon_i\}$ would make things more clear.

Overall this section seems to present some refinements of the incompressibility results as presented (for instance) in  Li \& Vitanyi, section 2.2. If they were presented as such (without the reliance on probability distributions and the compressor/decompressor framework), I would personally consider them of greater value. To clarify, they could be presented as results about the number of compressible strings, and the effect of the universal Turing machine chosen. The results for specific (de)compressors could then be stated as a consequence of this.

\subsection*{Section 4}

The conclusion that Levin's universal search applies to compression algorithms seems a natural one. This seems like a well-established result, although I cannot find any statements of the theorem explicitly in this form. While such a statement is useful to have, by itself it does not seem worthy of publication.

The statement ``typically, the time complexity of the expansion is smaller than the time complexity of the compression'' requires qualification.

\pagebreak[4]

\section{Minor issues}

Here I present some specific stylistic errors and remarks for the consideration of the authors. Overall the paper did not suffer from any great number of such problems. They are included here only in the hope that they may be of some use.

\begin{itemize}
  \item The document lacks page numbers. I number the title page as 1.
  \item The notation is a little confusing with respect to the standard notations of Kolmogorov Complexity. $C(x)$ of course has a different meaning from its normal use (which is not diffcult to overcome), but it is also defined to return the compressed program rather than the length of that program, as the definition is in Kolmogorov Complexity. I would recommend using lowercase letters $c(x)$ and $d(x)$ to avoid confusion.
  \item On page 2: ``The set of words with length $n$ is $\Sigma^n$ and will be denoted by $S_n$''. It seems to me that not much is won by referring to this set as $S_n$ rather than $\Sigma^n$.
  \item ``Relatively to an optimal compressor \ldots'' should be ``Relative to an optimal compressor \ldots''
  \item ``\ldots{ }has the same time order than \ldots'' should be ``\ldots{ }has the same time order as \ldots''
\end{itemize}


\end{document}

\documentclass{thesis}

%\usepackage{amsthm}
%\usepackage{charter}
%\usepackage{eulervm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{cancel}
\usepackage{hyperref}
\usepackage{makeidx}

%\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{crl}{Corollary}
\newtheorem{lma}{Lemma}
\newtheorem{dfn}{Definition}
\newtheorem{exm}{Example}

\newenvironment{out}
 {\emph{Proof outline.\;}}
 {}

%\let\doendproof\endproof
%\renewcommand\endproof{~\hfill\qed\doendproof}

\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\DeclareMathOperator*{\argmin}{\arg\,\min}
\DeclareMathOperator*{\argmax}{\arg\,\max}
\DeclareMathOperator*{\nid}{NID}
\DeclareMathOperator*{\id}{ID}

\newcommand{\sdr}[1]{\textcolor{blue}{\small #1\textsuperscript{[sdr]} }}
\newcommand{\pb}[1]{\textcolor{OliveGreen}{\small #1 \textsuperscript{[pb]} }}

\newcommand{\p}{\mbox{\,.}}
\newcommand{\fl}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\g}[1]{\color{gray} #1 \color{black}}
\newcommand{\B}{{\mathbb B}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\m}{{\overline{m}}}
\newcommand{\ok}{{\overline{\kappa}}}
\newcommand{\mmid}{\;\middle|\;}

\newcommand{\hide}[1]{}

\title{Single sample statistics}
\date{\today}

% non-indented, spaced paragraphs
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

\makeindex

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

Imagine a mysterious signal arriving from outer space: for a brief period, radio-astronomers receive a sequence of long and short pulses from a largely empty section of the sky. Then, the sky goes quiet again.

This scenario represents the ultimate challenge to statistics. We have a finite amount of data, and nothing more. We cannot take another sample, and no assumptions are justified. We don't know the source distribution, we don't know whether the data can be cut into meaningful parts. All we have is one finite piece of data.

Yet, even in this bleakest situation, we are not totally lost. If the signal is entirely without structure, we feel we can dismiss it as white noise. If the signal is highly regular, like a periodic pulse, some natural phenomenon might be responsible for it, perhaps a planet in close orbit of a star. If the signal contains many complex structures, and yet remains unpredictable, like a binary encoding of a novel, we would suspect an intelligent origin.

This story describes the setting we will analyse in this dissertation. We, the experimenter, are presented with a single, finite sample of data, described in some discrete manner, perhaps as a string of bits. We are given no extra samples, and no basis for any further assumptions. The data may represent a sequence of iid. samples from some distribution, or it may describe a single, complex object. If we wish to see it as such, we must find our motivation for doing so within the data.   

The thesis behind this dissertation is that this settings, which we'll call \emph{single sample statistics}, is the most generic view of statistical practice that still allows us to make useful inference. All other settings, repeated IID samples, time-series analysis, can be seen as  version of this setting with additional assumptions or constraints. If we want to establish a simple, comprehensive framework for statistical practice from first principles, this setting should be our foundation. A single, finite datum, with no further assumptions or restrictions.

\textbf{We will show that statistical inference is feasible in the single sample setting setting}. We can determine the likelihood of the signal in a general sense, and we can compare different explanations for the data. However, there are also common, uncontroversial practices that are unlikely to be possible in the setting of single sample statistics without further assumptions and restrictions. Model selection, for instance, seems to be a lost cause. It's not just that we can't find the data's inherent model, we cannot define what it should be in an unambiguous, objective manner. Both are interesting results. By studying what works and what doesn't for single sample statistics we show both the value and the danger of additional constraints. Assuming that our data is sampled IID from a Bernoulli distribution will allow us to perform model selection, but if this assumption is unjustified 


\chapter{Kolmogorov complexity: a universal probability}

\begin{summary}If we make no assumptions on the source of our data, we are, in essence taking the model class of all computable probability distributions: if our data is produced by a model outside of this class, we will never be able to discover that fact anyway. Following this line of reasoning, we end up naturally with Algorithmic complexity, and Algorithmic statistics. In this chapter we make this reasoning explicit. While Kolmogorov complexity is itself incomputable, the surrounding framework does allow us to perform practical statistical inference, even in the single sample setting. We also explore what further practical approaches are possible if we allow a model assumption.
\end{summary}

\pb{A safe approximation for Kolmogorov Complexity}

\chapter{Model selection and sophistication}

\begin{summary}In this chapter we investigate the limits of the single sample setting. We show that in a  straightforward approach to Algorithmic Statistics, model selection cannot be performed in an objective manner. This tells us that in the single sample setting, the data does not have an inherent model. It may have inherent patterns, as witnessed by the Kolmogorov complexity, but an objective separation into model and noise is not feasible. 
\end{summary}

\pb{A critical review of Algorithmic Model Selection and Sophistication}

\chapter{Graph modeling}

\begin{summary} Analysis of graphs is a perfect exemplar of the single sample setting. Take the world wide web, for instance. It is a single datum, and we cannot sample another. Yet it is large enough that it approaches the size of datasets that we imagine when talking about algorithmic statistics. It is certainly highly structures, and highly unpredictable. It cannot be partitioned into logical units, but it can be sub-sampled. 
\end{summary}

\section{Complex graphs, an MDL perspective}
\pb{MDL models for graphs}


\subsection{Motif extraction}

\pb{Motifs}

\section{Inference on the nodes of a graph}

\pb{Simplifying RDF data for graph-based machine learning, Machine Learning on Linked Data, a Position Paper}

\chapter{Exploiting self-similarity}
\begin{summary}
In the single sample setting, the only thing we have to work with is the internal structure of the data. A particularly useful structure to exploit is that of self-similarity. In this chapter, we investigate the possibilities. 
\end{summary}

If all we have is the data, we must find compressible structure. The most common form of structure is symmetry: transforming the data in some way, leaves it in some way invariant. For example: if we take a text, and chuck it into words, we have a kind of translational symmetry. Shifting the data one chunk left or right exposes a different word, but from the same distribution. Of course, this assumes that the words are sampled IID, which is far from true, but this translational symmetry alone is enough to compress the data.

All such symmetries, are commonly exploited in many different statistical settings. Most practitioners are unlikely to take this perspective, of course: a machine learning with a table of instances is unlikely to think of his dataset as a single datum with internal translational symmetry, but from the single sample perspective, that is what is happening.\index{Symmetry}

While it may be awkward to think of classification from the single sample perspective (sometimes we must allow ourselves to forget our first principles), the notion does suggest a new kind of symmetry to analyse: self similarity.

Self similarity if symmetry of scale: when the data is shrunk down, it remains the same. The most exact and literal form of self similarity can be found in fractal figures, like the Sierpinski gasket. In this chapter we investigate whether we can model self similarity: we will attempt to tackle what is known in fractal geometry as \emph{the inverse problem}. 

\pb{An Expectation Maximization Algorithm for the Fractal Inverse Problem}

\chapter{Single sample statistics}

\pb{Review and Conclusion}

\appendix
\chapter{Tutorials}
\begin{summary}
This appendix provides brief introductions to the subjects that are required to understand this thesis. They illustrate the use of our notation and provide a concise basis for readers who are well-versed in mathematics and computer science, but lack knowledge of these specific subjects. 
\end{summary}

\section{Kolmogorov complexity}
\section{Recommended literature}

\chapter{Symbols and Terms}

\begin{description}
 \item[$K(\cdot)$]
 \item[$k(\cdot)$]
 \item[Shannon Entropy]
\end{description}

\chapter{Algorithms and techniques}
\chapter{Proofs}
\chapter{index}
\printindex

\end{document}

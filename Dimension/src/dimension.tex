\documentclass[11pt]{article}

\usepackage{charter}
\usepackage{eulervm}
\usepackage{amsmath, amsthm, amssymb}

\newtheorem{thm}{Theorem}
\newtheorem{lma}{Lemma}
\newtheorem{dfn}{Definition}
\newtheorem{exm}{Example}
\usepackage{lscape}

\usepackage{float}

\title{Measuring dimension}
\date{\today}

% non-indented, spaced paragraphs
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

% pseudocode 
\floatstyle{ruled}
\newfloat{pseudo}{h}{lop}
\floatname{pseudo}{Algorithm}

\begin{document}

\maketitle

\begin{abstract}
\noindent The intrinsic dimension of a dataset is one of the most telling statistics that can be calculated, and one that has received a great deal of attentation over the past decades. Most methods for estimating dimension rely on at least some subjective judgment to make a selection in the data. Adapting methods recently described in \cite{clauset2007power} for the statistical analysis of power laws, we revisit an existing MLE estimator and present a simple, objective procedure to estimate the dimension of a dataset. The procedure runs without human intervention, and produces a value for the dimension, uncertainty bounds and a significance value. We also extend the procedure to extract multiple characteristic dimensions from a dataset and revisit the concept of \textit{lacunarity} in the context of these methods.\end{abstract}
\section{Introduction}
The \textit{intrinsic dimension} of a dataset does not refer to how many features are used to represent it, but---loosely speaking---to how few features could be used to represent it. This is expressed in various power laws that determine how attributes of the datasets scale over varying length scales. For many datasets, in particular those with coarse ``fractal'' features or those generated by deterministic chaos, dimension is one of the few well defined statistics available, and usually the most expressive.

In the natural sciences, the intrinsic dimension takes the role of the observable: predicted by theory, and derived from experiment. Fractal phenomena do not respond well to traditional geometric measures. We cannot measure the length of a coast line, or the surface area of our lungs. In the words of Eugene Stanley \cite{stanley1986growth}:

\begin{quotation}
\small
\noindent If you are an experimentalist, you try to measure the fractal dimension of things in nature. If you are a theorist, you try to calculate the fractal dimension of of models chosen to describe experimental situations; if there is no agreement then you try another model.
\end{quotation}

For this reason, any scientist who deals with fractal phenomena will want to measure dimension, in order to capture those phenomena in meaningful statistics. Scientists encoutenering this problem include biologists studying the emergence of structure, geologists studying the fractal surfaces and outlines of land masses and physicists studying the measures of strange attractors.

The fields of machine learning and statistics have their own interest in the notion of dimension. Intrinsic dimension has been shown to be an effective lower bound to the number of features to which a dataset can be reduced without losing information.\cite{kumaraswamy2008fractal} Since many methods and algorithms work faster and more accurately on low dimensional data, dimensionality reduction is an important factor in bringing high dimensional data into the domain of machine learning. The field of manifold learning in particular has succeeded in mapping data in many hundreds of dimensions to low dimensional euclidean spaces\cite{roweis2000nonlinear} and often relies on acurate estimates of intrinsic dimension.\cite{levina2004maximum} 

Most methods for measuring dimension have relied at least in some part on subjective judgements and processes that make it difficult to capture experimental error. A similar situation was present in the study of power law distributions until recently Clauset, Shalizi and Newman published \cite{clauset2007power}, in which they dealt with various problems, and described a simple objective approach for estimating the exponent of such distributions with no subjective elements and a clear indications of uncertainty and significance with respect to the assumptions.

The methods in this paper come from the realization that the relatively unknown Takens estimator for dimension is also based on the assumption of a power law distribution (albeit of a different form than the ones studied by Clauset et al.), allowing these methods to be adapted to the estimation of dimension. This process also gives us a means to determine multiple dimensions for datasets that can be considered to have different dimensions for different ranges of scales, and we can give a new measure for the lacunarity, a measure of the `gapiness' or texture of a dataset that is often used in combination with dimension. 

There is a wealth of research on the subject of dimension and related subjects. For a good overview on definitions of dimension and proposed estimators, we recommend \cite{theiler1990estimating}. For a derivation of dimension from basic mathematics (topology and metric spaces) we refer the reader to \cite{edgar2008measure}.

\section{Dimension}

[@! Discuss the range of D for which estimation is reasonable]

Dimension is usually introduced in layman's terms as the ``amount of numbers required to define a single point on a surface, in a volume or in some other set''. This definition, while it explains why a line has lower dimension than a square or the surface of a sphere, breaks down on closer inspection. There are may ways to encode three numbers into one (for instance with the use of space-filling curves, or simply by alternating digits) so that we can, theoretically, locate a single point in a volume by only one coordinate. 

Nevertheless, something fundamental seems to separate basic curves from figures in the plane from figures in space. Some property that we have come to call dimension, before defining it rigorously. In fact, investigations show that once notions of dimension become well defined they can be naturally extends to metric spaces, or even topological spaces. 

Topology has several (equivalent definitions) defined inductively. For instance the covering dimension is based on the idea that a set of dimension $d$ can be covered with open sets so that no more than $d-1$ will overlap anywhere. Starting with the convention that the empty set has dimension $-1$, this gives us a complete notion of dimension.

A spanner was thrown in the works with the investigation of \textit{fractals}, Coarse, non-smooth shapes that reveal ever greater detail as one `zooms in'. The problem with these figures was that they did not seem to obey the Euclidean scaling laws. An example of such a law is that scaling a square so that its sides increase by $s$, will increase its surface by $s^2$. Similarly, a cube scaled in the same way will see its volume increased by $s^3$ and its surface area will increase by $s^2$. By the seventeenth century, the scaling laws had been generalized to all curves, surfaces and solids. Except fractals.

To illustrate how fractal figures seem to break the scaling laws consider figure @!.a (adapted from [@! Cite Manfred Schroeder]). This figure contains seven hexagon tiles arranged into a larger shape. We can now replace each line segment with three smaller line segments as shown in the figure. We note that this operation does not change the surface area of the figure. Afterwards we are left with another collection of line segments, so we can apply the operation again. In the limit we get the figure shows in @!.2, a fractal. It can be shown that this fractal shape is invariant to the transformation.

The issue arises when we recognize that the outline of this figure is exactly a scaled up version of one of its tiles. This is possible because the outline we started with was a scaled up (and slightly rotated) version of the tiles after one transformation. Thus, the outline is always one transformation ahead of the tiles, a difference which disappears in the limit. The length scaling between the outline at step $n$ and a tile at step $n+1$ is $3$. $3^2 \neq 7$ so it seems like this figure violates the scaling laws. 

The situation is resolved when we re-evaluate our notion of dimension. Consider the outline of the figure, traditionally seen as a 1 dimensional curve. If we measure its length (a 1 dimensional measure) we see that it goes to infinity in the limit of our process. If we take from this the hypothesis that its dimension is not $1$, since the 1-dimensional measure fails, but greater than one, we might try to measure its surface area by covering it with ever smaller squares and summing their areas. This measure goes to zero. We are left with the impression that this shape has a dimension above $1$ and below $2$.

In fact, if we decide to hold to the scaling law and release our grip on the notion of dimension qwe see that the value that makes the scaling law work will satisfy $3^d = 7$, which gives us  $d = \log_3 7 \approx 1.77$. A non-integer dimension. Upon investigation, the choice was easy, the fractal dimension (properly defined) were consistent and well behaved. Our notion of dimension was relaxed, and the scaling laws were upheld.

Not only were these pathological counterexamples given a place in Euclidean geometry, they were connected to many natural phenomena. The coast-line of Britain, for example, behaves the same way as the side of our tiles. If we try to measure its length, our values for increasing accuracy of measurement will go to infinity. The same goes for the surface area of a human lung or a mountain range. Luckily, the rate with which conventional these measures fail allows us to get a hint of the intrinsic dimension.

\subsection{A theoretical note}

All notions and estimators of dimension described in this paper (and almost all such notions in general) require the data to follow a particular power law, usually between osme length scale and some property of the data. Often this power law can be proved to hold in the limit as the length scale goes to zero, or the power law is show to hold approximately (perhaps for a certain class of sets). 

This approach to dimension has two main drawbacks. First, in the limit towards zero for length scales data becomes increasingly sparse . If we draw two points uniformly and measure their distance, it will likely be in the larger part of the range, with probability shrinking rapidly for lower distances. Secondly, we will often also be interested in datasets that seem to have one dimension at some range of scales and another dimension over another range of scales (imagine, for instance, points sampled from a curved line with some small, two-dimensional noise added). In the limit to zero we are likely only measureing the properties of the experimental noise. In the larger distance scales we can measure dimensions that actually reveal information about the processes that shaped our data.

Our solution is not to reduce the meaning of our statistics down to a mathematical definition of dimension, but to take as an assumption that a certain scaling law holds for some range in our data. Data sets, by virtue of filling a geometrical space will generally follow various power laws but they can also deviate from them slightly (see section \ref{lacunarity}). If we simply assume that some scaling law holds exactly, we can take a maximum likelihood estimate of the dimension and use a significance measure to test to what extent our assumption is justified. We can also say that our curved line with added noise, while it technically has dimension 2, behaves as a clean curved line over certain length scales so that in that regime we can expect a 1 dimensional scaling law to hold.

The distinction is a subtle one, and in most cases people will be satisfied to claim simply that they are measuring dimension, but in some situation the distinction may be important.

\subsection{Box counting dimension}

The box counting dimension is one of the earlier general-purpose definitions of dimension, and one of the first to be used to define an estimator. Let $X \subseteq {\mathbb R}^d$. We can cover the ${\mathbb R}^d$ with a grid of $d$-dimensional hypercubes with sides of length $\epsilon$. We then count how many of these hypercubes contain elements of $X$ and call this value $N_\epsilon$. We assume the following relation:
\begin{align}
\label{box-counting-assumption}
N_\epsilon = \left (\frac{1}{\epsilon}\right) ^ d
\end{align}

If we take the logarithm on both sides, we get

\begin{align*}
 - d \ln \epsilon = \ln N_\epsilon
 d = - \frac{\ln \epsilon}{\ln N_\epsilon}
\end{align*}

In other words a linear relationship in log-log axes. A common approach, especially in the early days of measuring dimension was to use a log-log plot of many values of $N_\epsilon$ and $\epsilon$ and fit a line over a range of $\epsilon$ where the data seems reasonably linear. A more refined approach is to find a range of scales in which the log-log plot looks linear and to perform linear regression on the logarithmes of these values. apart from the subjective element in this approach, linear regression if logarithmic axes has undesirable statistica properties as detailed in \cite{clauset}. [@! Review]

There is no guarantee that this relation holds. A common tactic is to reduce the assumed power law to  
\begin{align}
N_\epsilon = \Phi\left(\frac{1}{\epsilon}\right)\left (\frac{1}{\epsilon}\right) ^ d
\end{align}

where $\Phi\left(\epsilon\right)$ is called the \textit{pre-factor}. If we say that the pre-factor satisfies
\[
\lim_{\epsilon \rightarrow 0} \frac{\ln \Phi(\frac{1}{\epsilon})}{\ln \frac{1}{\epsilon}} = 0
\]
We can estimate the dimension using 
\[
D_B = \lim_{\epsilon \rightarrow 0} \frac{\ln N_\epsilon}{ \ln \epsilon}
\]
This allows many deviations from the power law (such as oscilation) but requires the dimension to be measured in the lower ranges of $\epsilon$ (note that we will need to increse the volume of data for decreasing values of $\epsilon$ if we want to keep getting meaningful measurements).

\subsection{Dimension of measures}

The dimension of a finite set of points, by any definition, is zero. Thus the dimension of a dataset will always be zero. When we talk about applying some estimator to a dataset we are not measuring the dimension of the dataset, but rather the dimension of the underlying probability distribution. We will usually assume that there is some probability measure over our instance space from which the elements in our dataset were independently drawn. It is the dimension of this measure that we are estimating. But before we do so, we must have some idea of how the dimension of a measure is defined.

Most commonly, this is done by extension of the box counting dimension. If we take the approach to the box counting dimension for sets and instead of counting the boxes which contain points of a set, we count the boxes with non-zero probability under our measure, we are in essence measuring the dimension of the support of the measure. However, the dimension of the support will not always capture the scaling we are interested in. Consider, for instance, a uniform distribution $U_0$ over the interval $[0, 1]$. We can cut this interval in two, and define a new distribution $U_1$ which combines a uniform distribution over the top half of the inteval with three quarters of the probability mass and a uniform distribution over the bottom half with the remaining probability mass. We repeat this process so that $U_{n+1}$ takes the uniform segments of $U_n$ and cuts them in half, distributing their probability mass over the new segements with the same ratio as before. The limiting distribution of this process ($\lim_{n \rightarrow \infty} U_n$) has the interval $[0, 1]$ as its support, and thus dimension $1$ by the definition above. The fractal shape, however, suggests that there are many interesting scaling laws that will give us an insight into its structure.

We can change the definition of dimension by introducing a parameter $q$ that determines how much more we care about regions with high probability than about regions with low probability. We define the quantity $I_q$ as an analgoue of the box counting dimension:
\[
I_q(p, \epsilon) = \sum_{c \in C_\epsilon}p(c)^q 
\]
Where $C$ is the covering of the probability distribution in boxes of side length $\epsilon$. The \textit{generalized dimension} of $p$ is then defined as
\[
D_q(p) = \frac{1}{q-1} \lim_{\epsilon \rightarrow 0} \frac{\ln I_q(X, \epsilon)}{\ln \epsilon}
\]

For $D_0$ reduces to the box counting dimension of the support. $D_1$ causes a division by zero, but letting $q \rightarrow 1$ we can formulate 
\[
D_1(p) = lim_{\epsilon \rightarrow 0} \frac{1}{\ln \epsilon} \sum_{c \in C_\epsilon} p(c) \ln p(\epsilon)
\] 
The right part of this is the negative of the entropy of the distribution over $C$ under $p$. In essence $D_1$ measures how the entropy of a covering scales with the accuracy of that covering. This gives a more rigorous background to the intuitive notion of dimension as the amount of number need to describe a point. If we \ldots @! see Theiler.

For $q = 2$ we get 
\[
D_2(p) = \lim_{\epsilon \rightarrow 0} \frac{1}{\ln \epsilon}\ln \sum_{c \in C_\epsilon} {p(c)^2}
\]
This is called the correlation dimension (\ref{correlation dimension}), and this is the dimension we focus on. It has the advantage that it is easy to estimate. It should be noted that for many distributions $D_q$ does not vary with $q$, so that we can choose $q$ to suit our needs. Where the dimension does depend on $q$ (the so called \textit{multifractals}) $2$ provides reasonable compromise between caprturing the whole support, and capturing the scaling within the probability density function.

The theoretical basis for the generalized dimension is usually based on an analogous treatment of the Hausdorff dimension. We will leave the subject at this brief introduction, noting only that we will technically be estimating $D_2$.

\subsection{Correlation dimension}
\label{correlation-dimension}
In 1983 Grassberger and Procaccia \cite{grassberger1983measuring} introduced the Correlation dimension. The Correlation dimension is based on the probability that two points, drawn independently wil have a distance less than $r$. We call this probability measure over distances the correlation integral $C(r)$ and we assume the following scaling law:
\begin{align*}
C(r) = \Phi(r) r^D
\end{align*}
Where $Phi(r)$ is again the pre-factor.

The correlation estimator $\widehat{C}(r, X)$ for the correlation integral (over dataset $X$) is straightforward\footnote{$[\ldots]$ are Iverson brackets. They are $1$ when the logical formula within them is true, and $0$ otherwise.}:
\begin{align*}
\widehat{C}(r, X) = \frac{\sum_{x\neq y \in X} [d(x, y) < r]}{\sum_{x \neq y \in X} 1}
\end{align*}

@! Mention the statistical properties of the correlation estimator.

We can substitute $\widehat{C}$ for $C$ and follow the same approach as with the box counting estimator. We plot the $\log C$ against $\log r$, and find the slope of the in an appropriate regime.

The great value of the correlation integral is that it turns a dataset of $n$ points into a dataset of $(n^2 - n)/2$ points. While the set of distances is derived (and thus technically contains no new information), we can treat it as a a set of independent data at very low cost to accuracy (see \ref{independence}). The effect of the correlation integral then, is to give us a polynomial increase in data at almost no cost.

Unfortunately, the Grassberger Procaccia estimtor still suffers form the drawbacks of the box counting estimator. A requirement of a subjective judgement and a reliance on a linear fit in logarithmic axes. The Takens estimator described in section \ref{methods}, does not suffer these problems, so that it what we will base our method on. 

\subsection{Non-independent data}

As noted before, we assume that our data is independently sampled from some probability distribution. This may not be the case. For instance time series data can be highly autocorrelated. In some cases, for instance when data is generated by a strange attractor, the whole time series may be predetermined by the initial state of the system, so that we cannot speak of a real probability measure, let alone dependency on one.

The solution to the last problem is the natural measure. Let $\{x_i\} \in S$ be an infinite timeseries generated by some process. Then we define the natural measure of that process as
\begin{align*}
p(S') = \lim_{n \rightarrow \infty} \frac{\sum_{x_i, i < n} [xi \in S']}{n}
\end{align*} 
where $S' \subseteq S$ a similar measure can be defined for continuous $i$. In the case of ergodic strange attractors a slight uncertainty about the initial condition of the system will quickly spread out to coincide with the systems natural measure, so that for such systems the natural measure actually functions as a kind of probability measure representing uncertainty of the state of the system after a number of iterations.

Even when we take the natural measure as the measure for which we are measuring the dimension, we have no guarantee that time series data will be independent. In fac, if it were, our time series wouldn't be very interesting. A solution proposed in \cite{theiler1990estimating} is to use for the numerator of the correlation estimator all distances below $r$ except those for which the points are within $w$ time units. The division is still by the number of all point pairs.

A good example of the dangers of autocorrelation in data is provided by \cite{theiler1996re}. For the rest of this paper we will assume independently drawn data.

We should also caution against bootstrapping. In instances drawn form a distribution with infinite support, the probability that the same instance will occur twice in a finite data set is zero so that the distance 0 should never occur. If we boostrap a dataset by sampling with replacement, we will very likely have recurring elements, and thus a number of zero distances. This can lead to a dimension estimate of 0.0 (as would be correct for independent draws with recurring elements, since it suggests a finite support). For this reason if we want to apply the Takens estimator to a bootstrapped dataset, we recommend bootstrapping over the collection of distances rather than the collection of points. 

\section{Methods}
\label{methods}

\subsection{The Takens estimator}

The Takens estimator is based on two assumptions about a dataset $X$ drawn from probability distribution $p$:
\begin{itemize}
  \item The set of distances $R = \{r : x_i, x_j \in X, i>j, d(x_i, x_j)\}$ can be taken as a set of $(n^2 - n)/2$ independent draws from the probability distribution of te random variable $D = d(X, Y)$ where $X$ and $Y$ are independent random variables distributed according to $p$.
  \item The correlation integral follows a power law with a constant pre-factor $C(r) = c r^D_t$ for values of $r$ below some value $r_{\max}$.
\end{itemize}

Under these two assumptions we can derive the following maximum likelihood estimator for $D_t$:
\begin{align*}
\widehat{D_t} &= \frac{|R|}{\sum{R} - |R| \ln r_{\max}}
\end{align*}

The derivation is presented on section \ref{mle-derivation}. 

This estimator was first described by Floris Takens in 1985 \cite{takens1985numerical} in relation to measuring dimension, although the estimator for power law distributions (and generic distributions with tail) was already treated in depth by Hill in 1975 \cite{hill1975simple}.

The following assumption discusses the independence asumption and test its influences. The second assumption we take as a hypothesis to be validated by a calculation of the significance (see \ref{significance}). 

\subsection{Testing the independence assumption}
\label{independence}
If we use each point for only one pair, (and our dataset is iid) then the distances for those pairs will be iid samples from $p_D$. If we reuse points, we will invalidate the first assumption. Consider for instance, a dataset of three points $a$, $b$ and $c$ drawn independently from some distribution. If we use only $d(a, b)$ we get a single sample from $p_D$. If we use $d(a, b)$ and $d(a, c)$, the two values are dependent on the location of $a$. We are in fact sampling twice independently from the distribution of $d(a, X)$ rather than $d(X, Y)$.
 
If we use all pairs from a large dataset, however, the recurrence of single points will simply mean that we are sampling from $d(a, Y)$ averaged over many different $a$. Thus we expect that for a large dataset, the distances of all pairs do not form a substantially different distribution than the distances over successive pairs, while giving us a squared increase in the volume of data. In \cite{}

We test this assumption by generating both the set of all distances and the set of distances between successive points (for a shuffled dataset) and calculating the Kolmogorov-Smirnov statistic $k$ for the two datasets. To give us a baseline for comparing the values, we also generate (for the datasets based on generating algorothms) two sets of $n/2$ and $(n^2 - n)/2$ values drawn from the true distribution over distances, and report their KS value $k$. For generated datasets experiments were repeated 100 times and the sample standard deviation is reported in parenthesis after the mean.

The results show that the difference between $k$ and $\widehat{k}$ 

\begin{tabular}{l | r r r r r r}
\hline
data set & size & features & $\widehat{k}$  & & $k$&  \\
\hline
Swiss roll & 400 & 3 & 0.065 &(0.02) & 0.059&(0.019) \\
Noisy swiss roll & 400 & 3 & 0.068&(0.022)&  0.062&(0.02)\\ 
Sierpinski & 400 & 2 & 0.065&(0.23)& 0.063&(0.19)\\
Cantor & 400 & 1 & 0.06&(0.18) & 0.58&(0.02)\\
Menger Sponge ! & 400 & 3 & & & & \\
Persistent process ! & 400 & 2 & & & & \\
  
MVN 5 & 400 & 5 & & & & \\ 

Logistic equation & 400 & 1 & & & & \\
Lorenz attractor & 400 & 3 & & & & \\
 
British coast line & 400 & 2 & & & & \\   
Expressions & & & & & & \\
Currency ! & & & & & & \\
Galaxies & & & & & & \\
DNA random walk & & & & & & \\
Turning cup & 481 & 245760 & 0.067 & & & \\ 
Faces & & & & & & \\ 
Sunspots & & & & & & \\

Newsgroup posts & & & & & & \\
Protein sequences  & & & & & & \\
Graphs & & & & & & \\
\hline
\end{tabular}

\subsection{Bias and variance of the MLE}

Refer to Smith.

In \cite{smith1992estimating}, Smith derives the following formula for the number of points $N$ (below $d_{\max}$) required to measure the intrinsic dimension to an (mean squared) error of $\nu$, if the intrinsic dimension is $D$:
\begin{align*}
N = D\left[\frac{4D(D+4)}{(D+2)^2}\right]^{\frac{D}{8}} \left[\frac{D+4}{2}\right]^\frac{1}{2} g^\frac{D}{4} \nu^\frac{-(D+4)}{4}
\end{align*}

Where $g$ is a constant depending on the shape of the distribution.
 
\subsection{Estimating the $d_{\max}$ parameter}

Quick review of the methods described in Clauset. Translation of the method to this estimator. 

\subsection{Significance and Uncertainty}
\label{significance}

Following \cite{clauset2007power}, we provide uncertainty values by bootstrapping over the original data: we sample with replacement a dataset the same size as the original and repeat the experiment. We repeat this procedure 100 times and report the sample standard deviation of the outcome.

Since we assume the scaling law to be present in the data and make no claims that for its existence a priori, we must consider the case that we are fitting a distribution to data that does not support it. Again we turn to \cite{clauset2007power} for our approach.

Having fit a power law to the regime $[0, r_{\max}]$ we can generate a dataset that is like our original given that our assumption is true. For the $n$ points in the data w generate from the fitted distribution with probability $\delta$ and sample from the points above $r_{\max}$ with probability $1 - \delta$, where $\delta$ is the proportion of points in the original data below $r_{\max}$. This way we generate a new dataset for the head of the distribution parametrically and the taiol is bootstrapped so that we do not have to make a assumptions about its shape.

We then assume that our data has a bad fit to the distribution and ask what the probability is that such a bad fit would be seen if the assumptions about distribution were true. We generate $n$ datasets by the methods described above, repeat the experiment on them and calculate the KS statistic (note that we calculate the KS statistic of the generated ddata against the distribution fitted on the generated data). The proportion $p$ of trials for which the KS value is above the KS value measured for our original dataset is the probability that such a bad fit would emerge with correct assumptions. Thus, the closer $p$ is to one, the higher our significance (contrary to more traditional significance measures where low values of $p$ indicate).

If $p$ is low (for instance below $0.01$) then the scaling law does not hold for the regime used and we cannot trust the result of the maximum likelihood estimator. In the case of power law distributions the most likely reason for a low $p$ value is that the data fit a distribution that is similar to a power law distribution (which can then be tested by log-likelihood ratio tests). In our case the most likely reason for a low $p$ is hat the power law has a strong non-constant pre-factor which is throwing off the estimator. The cantor set, for instance shows a basic scaling law in log-log axes, but with an oscilating component. In other words the pre-factor is not constant and does not converge to a constant with zero. In these cases the the Takens estimator will fail to converge  as $r_{\max}$ gos to zero (although it oscillates within 0.05 of the true value).

This deviation from the scaling law is called \textit{lacunarity}, and because of the structural properties that sets and measures must posess to deviate from scaling laws, lacunarity is an interesting statistic in itself. We discuss this interpretation further in section \ref{lacunarity}.

\subsection{Runnning time}

The great benefit of the Takens estimator (and other estimators based on the correlation integral) is that it works in distances between data points rather than data points themselves. This has the effect of squaring the volume of available data (barring some reduction of independence).

The drawback is that using all available data can become very expensive, even for small datasets. In the basic algorithm taken from \cite{clauset2007power}, we must sum over all distances to find the estimate for the dimension, then repeat this process to find the estimate for the $d_max$ parameter and finally we must repeat the process around 1000 times to get a reasonably accurate estimate for the significance.

In some contexts this problem can be overcome. If only a single dimension calculation is required after a long process of generating data, then the experimenter will likely not mind waiting a day or so for the results. Alternatively, one could exploit `embarrasingly parallel` nature of the algorithm and spread the search for a fitting $d_max$ or the iterations of the significance calculation over many cores.

If many calculations of dimension are required, or the dataset is so large that even parallelization and patience are insufficient, we can sample the data. The simplest approach of course, is to take a sample of the data points and run the procedure on that sample. This has two drawbacks. First, of we sample from the set of distances instead we will increase the likelihood that our dataset represents a sample of independent distances. More importantly, we are using our sample to estimate two parameters $D_t$ and $d_{\max}$ some crude tests will show very quickly that if we want to estimate $D_t$ as accurately as possible, we should put all our computational power in the evaluation of the maximum likelihood estimator rather than considering many different values for $d_{\max}$ in other words, if we calculate the MLE with all available distances for a single randomly selected value of $d_{\max}$ we are generally better off than when we use to random values for $d_{\max}$ and half the distances for the MLE. 

Of course, even if the expected accurary is higher, using a single candidate for $d_{\max}$ is still likely to cause a very high variance. We use the following algorithm as a compromise.

We first generate a set of candidates $C$ with sie $c$ for the value of $d_max$. These candidates are generated by taking a subsample $R'$ of $s$ of the distances and running the full estimator on this subsample. When then add the $d_max$ value of this estimator to list of candidates. Since we can make $s$ relatively big (ie. $5000$) without incurring a great computational cost, this allows us to generate a much better set of candidates then a simple random sample would give us, while not making any further assumptions about the data and our estimator than that more data improves accurary on average. \footnote{It may be of interest to note that if we set $s$ to one, we are essentially sampling $c$ candidates at random for $d_{\max}$ while using all data for the MLE as described before. Thus blind subsampling is a special case of this algorithm.}

It is tempting to look toward search or optimization algorithms over the domain of $d_max$ to minimize the KS statistic, but these will often have rndom behaviours and assumptions on smoothness which will reduce the emprical strength of the resulting answer. Only the complete estimator will guarantee that the returned values will optimize their respective measures (maximum likelihood for $D_t$ and minimum KS statistic for $d_{\max}$). However if we must reduce the size of the observed data, we believe our approach strikes a nice balance between using computational resources efficiently and producing a trustworthy result.

For very large datasets, we may run in to the problem that it is not economical to store the set of all distances. We can store the dataset and compute distances on the fly for all elements of the algorithm, except for the KS test, where we require a sorted list of distances. If we are satisfied with a subsample of the set of distances for each run of the KS test, we can then combine this with the method described above to get an approximation to the full estimator with three parameters: the number of candidates, the number of samples per candidate, and the size of the subsample of the data to use for each candidate. 


@! Test all three methods

To consider the impact, we test how the accuracy of the estimator and the measure of its significance changes with a reduction in the number of candidates (keeping the number of samples per candidate fixed). The result is shown in figure @! (averging over 100 repeats of the experiment). We see that the $D_t$ estimator is the least affected. The $d_{\max}$ estimator suffers more, but as it is of secondary importance, this shouldn't bother us too much. The significance clearly becomes unreliable at low values of $c$. This is shown not only in the mean value but more importantly in the standard deviation. When using the full estimator, we can expect a good estimate of the significance with low variance, but even small subsampling begins to introduce massive vriance and error. For this reason we will report results on large datasets (using the method detailed here) but we will always take a subsample of the data to compute the significance. We will then make a further assumption that the significance for the subsample is a good estimate for the significance of the estimate on large data.

\section{A multiscale estimator}

Consider a dataset produced by drawing random points in some 1D curve in 2D Euclidean space, but with a small term of 2D Gaussian noise added. The theoretical dimension of this process, for any definition of dimension, will be 2 since the limit towards the smallest scales determines the dimension. However, the shape that we are interested in (the distribution without the noise term) has dimension 1. 

This is an example of a distribution with multiple intrinsic dimensions. The data is generated by different processes that each create a dimensionality in the data at a particular scale. It's not uncommon to see data with more than two intrinsic dimensions. In the log-log plot produced by the Grassberger-Procaccia estimator, this behavior will produce linear segments for the relevant length scales.

From a natural science perspective, finding these length scales in data can provide great insight. They hint at clearly separable processes producing the data, each with their own intrinsic length scales. However as noted before, the Grassberger-Procaccia estimator relies on subjective judgement to find these scales. Here, we adapt our method for single-sclae dimension measurement to multi-scale measurements.

To start with we assume that our data has some number of intrinsic scales determined by $\{r_i\} \in {\mathbb R}^+$ and $r_0 = 0$ so that we have an intrinsic dimension $D_t^i$ over distances in the half-open interval $[r_{i-1}, r_{i})$. We let the maximum $r_i$ be less than the maximum distance in the dataset, so that as with the single-scale estimator, our distance distributin has a tail that is not covered by the dimensional scaling (and about which we make no assumptions).

We define the correlation integral over an interval simply as $C(r_{i-1}, r_i) = C(r_i) - C(r_{i-1})$. Our assumption that the dimension $D_t^i$ determines scaling in this interval tells us that 
\[
p(d \leq r | d \in [r_{i-1}, r_i)) = \frac{p(d \leq r | d \in [r_{i-1}, r_i))}{p(d \in [r_{i-1}, r_i)])} = \frac{p(d \in [r_{i-1}, r])}{p(d \in [r_{i_1}, r_i))}
\]
\[
 = \frac{C(r) - C(r_{i-1})}{C(r_i) - C(r_{i-1})} = \frac{cr^{D^i_t} - cr_{i-1}^{D^i_t}}{cr^{D^i_t} - cr{i-1}^{D^i_t}}\
\]

We turn this into a probability function by taking the derivative over $r$, and we use this to compute the score (the derivative of the log likelihood of the data). Setting the score to zero gives us a maximum likelihood estimator for $D^t_i$. This process is detailed in appendix @!. Unlike the single scale version, we do not find a closed form version of the MLE. We are left with:

\[
\frac{1}{D^t_i} - \frac{\ln \frac{r_i}{r_{i-1}}}{(\frac{r_i}{r_{i-1}})^{D^t_i}} - \frac{\sum_{r \in R_i}\ln r - |R_i| \ln r_{i-1}}{|R_i|}= 0
\]

Which is unlikely to be expressible in $D^t_1$. However, as proved in the appendix, the function is strictly decreasing in $D^i_t$, for the required values of the constants, so that we can use a simple binary search to pinpoint its root. Call the the left hand side of the equation above $f(x)$. Then the search algorithm works as follows

\begin{pseudo}[h]
	\textbf{starting with $x = 1$ repeat $x \leftarrow 2x$ until $f(x)$ becomes negative}  \\
	\hspace*{5mm}\textit{\# This gives us an upper bound for $f(x) = 0$}\\
	\textbf{return find}$(0, x)$ \\
	
	\textbf{function find}$(x_{\min}, x_{\max})$ \\
	\hspace*{5mm} $x = (x_{\max} + x_{\min})/2$ \\
	\hspace*{5mm} \textbf{if} $x_{\max} - x_{\min} < \epsilon$ \\
	\hspace*{10mm} \textbf{return} $x$\\
	\hspace*{5mm} \textbf{if} $f(x) < 0$
	\textbf{return} $\mbox{find}(x_{\min}, x)$ \\ 
	\hspace*{5mm} \textbf{else} 
 	\textbf{return} $\mbox{find}(x, x_{\max})$ \\	
\end{pseudo}

This algorithm first searches for an $x$ that makes $f(x)$ negative, and then bisects the interval $[0, x]$ recursively until the interval is smaller tha a given $\epsilon$, at which point the middle of the interval is taken as the estimate of $D_i^t$.

To find the interval limits $r_i$ we proced as we did before with $d_{\max}$ and search for a set of values from the data which minimize the Kolmogorov-Smirnov statistic with the filtered data.

To find multiple, non-overlapping intervals, we use the follwing procedure. We first test all pairs of distances $r_a, r_b$ with $r_a < r_b$ and to find the pair with the lowest KS-statistic. Then, for the ranges to the left and right of $[r_a, r_b)$ we repeat the the procedure on only the data in that range. We repeat the procedure resursively until the data runs out or until the significance for the best subrange of any range drops below 0.01.


Results are reported in the next section. 
\section{Experiments}
\subsection{Data sets}

\pagebreak
\begin{landscape}

\begin{tabular}{l | r r r r}
\hline
data set & size & dimension & significance & true dimension \\
\hline
Swiss roll & 500 & & & \\
Noisy swiss roll ! & 1000 & & & \\ 
Sierpinski & 1000 & & & \\
Cantor & & & & \\
Menger Sponge ! & & & & \\
Persistent process ! & & & & \\
  
MVN 50 & 1000 & & & \\ 
MVN 500 & 1000 & & & \\

Logistic equation & & & & \\
Lorenz attractor & & & & \\
 
British coast line & & & & \\   
JAFFE expression database & & & & \\
Currency (Traina/Faloutsos) ! & & & & \\
Galaxies & & & & \\
DNA random walk & & & & \\
Turning cup & & & & \\ 
Faces & & & & \\ 
Sunspots & & & & \\

Newsgroup posts (edit distance, possibly normalized by length) & & & & \\
Protein sequences (mPAM metric) & & & & \\
Graphs (Weisfeiler Lehman) & & & & \\
\hline
\end{tabular}

\end{landscape}
\pagebreak

\subsection{Results}

\section{Lacunarity}
\label{lacunarity}


\section{Conclusion}

We have presented a straightforward, objective method for calculating a maximum likelihood estimate of the intrinsic dimension of a dataset. We have com 

\section{Appendix}

\subsection{Derivation of the MLE}
\label{mle-derivation}

We start with the cumulative distribution function below $d_{\max}$:
\[
p(d \leq r | d \leq r_{\max}) = \frac{p(d \leq r)}{p(d \leq r_{\max})} = \frac{C(r)}{C(r_{\max})} = \frac{c \cdot r^{D_t}}{c \cdot {r_{\max}} ^ {D_t}} = \left ( \frac{r}{r_{\max}} \right )^{D_t}
\]

We take the derivative (over $r$) to get the probability density function:

\[
p(d = r | d \leq r_{\max}) = D_t \frac{r^{D_t-1}}{{r_{\max}}^{D_t}} = D_t \frac{1}{r} \left (\frac{r}{r_{\max}} \right ) ^ {D_t} 
\]

Let $R$ be the set of distances in the dataset less than $r_{\max}$. The the likelihood of this dataset is
\[
{\cal L}(R|D_t) = \prod_{r \in R} D_t \frac{1}{r} \left (\frac{r}{r_{\max}} \right ) ^ {D_t} 
\] 
Which we must maximize. Maximizing the logarithm of the likelihood is equivalent, and easier.
\begin{align*}
\ln {\cal L}(R|D_t) &= -\sum \ln r + D_t \sum \ln \frac{r}{r_{\max}} + |R| \ln D_t
\end{align*}

To maximize, we take the derivate with respect to $D_t$

\begin{align*}
\frac{\partial \ln{\cal L}(R|D_t)}{\partial D_t} = \sum{\ln \frac{r}{r_{\max}}} + |R|\frac{1}{D_t}
\end{align*}

and set it equal to zero and solve for $D_t$

\begin{align*}
\sum{\ln \frac{r}{r_{\max}}} + |R|\frac{1}{D_t} &= 0 \\
|R|\frac{1}{D_t} &= - \sum{\ln \frac{r}{r_{\max}}} \\
D_t &= \frac{|R|}{- \sum{\ln \frac{r}{r_{\max}}}} 
\end{align*}

\subsection{Derivation of the multiscale MLE}

The only difference between the single- and the multiscale MLE is that the latter has a lower bound to its range in addition to an upper bound. Unfortunately this small difference causes the derivation of the MLE to become a great deal less elegant.

We start with the cumulative distribution function as described in the text. We use $r_{i-1} = r_h$ to clear up the notation.

\begin{align*}
p\left(d \leq r \mid r \in \left[r_h, r_i\right)\right) = \frac{r^{D_i} - {r_h}^{D_i}}{{r_i}^{D_i} - {r_{h}}^{D_i}} = \frac{\left(\frac{r}{r_h}\right)^{D_i} - 1}{\left(\frac{r_i}{r_h}\right)^{D_i} - 1}
\end{align*}
We take the derivative with respect to $r$ to get the probability density function.

\begin{align*}
p(d = r \mid \ldots) = {D_i} r^{D_i - 1} r_h^{-D_i} \frac{1}{\left(\frac{r_i}{r_h}\right)^{D_i} - 1}
\end{align*}

This gives us the following log likelihood function:
\begin{align*}
\ln{\cal L}(R_i|D_i) &= \sum \ln D_i +  (D_i-1) \sum \ln r - D_i \sum \ln r_h  - \sum \ln \left(\left(\frac{r_i}{r_h}\right)^{D_i} -1\right)\\
&=  |R_i|\ln D_i + (D_i - 1)\sum \ln r - |R_i| D_i \ln r_h - |R_i| \ln \left(\left(\frac{r_i}{r_h}\right)^{D_i} -1\right) 
\end{align*}

Where $R_i$ is the subset of distances in the dataset that fall within the interval $[r_h, r_i)$. 

Taking the derivative:
\begin{align*}
\frac {\partial \ln{\cal L}(R_i|D_i)}{\partial D_i} &=  \frac{|R_i|}{D_i} + \sum \ln r - |R_i|\ln r_h - |R_i| \frac{\left(\frac{r_i}{r_h}\right)^{D_i} \ln \frac{r_i}{r_h}}{\left(\frac{r_i}{r_h}\right)^{D_i} -1}
\end{align*}

Setting to zero gives us.

\begin{align*}
\frac{1}{D_i}  - \frac{\left(\frac{r_i}{r_h}\right)^{D_i} \ln \frac{r_i}{r_h}}{\left(\frac{r_i}{r_h}\right)^{D_i} -1} &= \frac{|R_i| \ln r_h - \sum \ln r}{|R_i|}  
\end{align*}

We have little hope of solving this for $D_i$, but as shown in lemma \ref{monotonicity}, the function whose root we're looking for is strictly decreasing for the values we are interested in. This allows us to find the value of $D_i$ by binary search as detailed in the main text.

For $r_h = 0$ this equation becomes undefined. However, this is just the single scale case, so we can use the basic Takens estimator derived in the previous section.

In the single-scale case we could normalize the data by scaling the original data, so that the maximum distance would become one. In this case we might be tempted to scale the distances so that we get $r_j' = 0$ and $r_i' = 1$ which would allow us to use the single-scale MLE. Unfortunately this transformation, while linear in the space of the distances does not correspond to an affine transformation of the original data \footnote{Or even a bijective transformation, since it maps all point pairs at distance $r_h$ to points with distance $0$, ie. a single point.}, and does not preserve $D_i$. We must make due with the non-closed form expression of the MLE.

\subsection{Monotonicity of the multi-scale MLE}

@! TODO Prove that it starts at a positive value

The following lemma tells us that the function describing the maximum likelihood estimate for the multiscale case is strictly decreasing over positive values, so that we can search for its root with a simple binary search.

\begin{lma}
\label{monotonicity}
The function
\[
f(x) = \frac{1}{x} - \frac{a^x \ln(a)}{a^x - 1} + c
\]
is strictly decreasing for $ x > 0$ and $ a > 1$.
\end{lma}
\begin{proof}
Taking the derivative gives us. 

\[
f'(x) = -\frac{1}{x^2} + \left [ \frac{\sqrt{a^x}\ln a}{a^x - 1} \right ]^2
\]

$f$ is stricly decreasing in the given domain if $f'(x) < 0$ holds. Assume for a contradiction that
\begin{align*}
-\frac{1}{x^2} + \left [ \frac{\sqrt{a^x}\ln a}{a^x - 1} \right ]^2 \geq 0
\end{align*}
Which rewrites to 
\begin{align*}
\frac{\sqrt{a^x}\ln a}{a^x - 1}  &\geq \frac{1}{x}\\
\frac{a^x - 1}{\sqrt{a^x}\ln a} &\leq x \\
\frac{a^x}{x\sqrt{a^x}\ln a} - \frac{1}{x\sqrt{a^x}\ln a} &\leq 0 \\
a^x &\leq 1
\end{align*}

For $a>1$ and $x>0$ we know that $a^x > 1$ so we have a contradiction.
\end{proof}

\subsection{Hausdorff dimension}
\label{hausdorff-dimension}

For theoretical purposes, the Hausdorff dimension is the most popular definition of dimension. It's not much in measuring dimension, but it can help us understand where the various scaling laws come from that help us to measure notions of dimension that are more suited to estimation from data. Of course, most of the notions of dimension agree with each other for most sets, or have well-understood equalities.

A complete treatment of Hausdorff dimension is beyond the scope of this article (@! Edgar provides a reasonably direct overview of the basic requirements for understanding Hausdorff dimension). We will give some intuition and the basic definitions. 

The basic notion behind Hausdorff dimension is that there are measures that are intrinsic to a given dimension (ie. length is related to dimension 1 and area to dimension 2) and that these measures will be $\infty$ or $0$ when they are applied to sets with dimension below or above the true dimension respectively. A fractal with dimension 1.5 will have length $\infty$ and area $0$.

The Hausdorff dimension is defined over metric spaces. Euclidean spaces and sets in Euclidean space are specific instances of metric spaces. We will first introduce some basic concepts.

\begin{dfn}
A covering $E$ of a metric space $S$ is a sets of sets such that $S \subseteq\bigcup_e \in E e$. 
\end{dfn}

\begin{dfn}
The diameter of a set $X$ in metric space $S$ with metric $\sigma$ is
\[
\mbox{diam}(X) = \sup_{x, y \in X}\sigma(x, y)
\]
\end{dfn}

\begin{dfn}
The quantity $H^d(x, r)$ of a metric space $X$ is the covering of $X$ that minimizes the sum of the diameters raised to $d$:
\[
H^d(X, r) = \inf_E \sum_e \in E \mbox{diam}(e)^d
\]  
\end{dfn}

Where the infimum is taken over all coverings $E$.

If we let $r$ got to zero, we get the measure mentioned above

\begin{dfn}
\[
H^d(X) = \lim_{r \rightarrow 0} H^d(X, r)
\] 
\end{dfn}

This measure has an intrinsic dimensionality $d$. If it is used on sets with the wrong dimension, it produces $0$ or $\infty$. Specifically, it can be shown that for each argument $X$ there is a single unique value $D_H$ such that 
\[
H^d(X) = \left \{ 
\begin{array}{rl}
  \infty &\mbox{if} d < D_H(X) \\
  0 &\mbox{if} d > D_H(X)
\end{array} \right.
\]

This value is the Hausdorff dimension of $X$.

\subsection{The Kolmogorov Smirnov test as an MDL approximation}

\subsection{Generating random points}
\subsection{Algorithms}

\bibliographystyle{siam}
\bibliography{dimension}

\end{document}

\documentclass{article}

\usepackage{charter}
\usepackage{eulervm}
\usepackage{amsmath, amsthm, amssymb}

\newtheorem{thm}{Theorem}
\newtheorem{lma}{Lemma}
\newtheorem{dfn}{Definition}
\newtheorem{exm}{Example}
\usepackage{lscape}

\usepackage{float}

\title{Measuring dimension across scales}
\date{\today}

% non-indented, spaced paragraphs
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

% pseudocode 
\floatstyle{ruled}
\newfloat{pseudo}{h}{lop}
\floatname{pseudo}{Algorithm}

\begin{document}

\maketitle

\begin{abstract}
The intrinsic dimension of a dataset is one of the most telling statistics that can be calculated, and one that has received a great deal of attentation over the past decades. Most methods for estimating dimension relsy on at least some subjective judgment to make a selection in the data. Adapting methods recently described [@! cite] for the statistical analysis of power laws, we revisit an existing MLE estimator and present a simple, objective procedure to estimate the dimension of a dataset. The procedure runs without human intervention, and produces a value for the dimension, uncertainty bounds and a significance value.\end{abstract}

\section{Introduction}

Definitions of dimension, fractal dimension and measuring dimension.

Previous work. Theiler. Manifold learning. Databases (Cite faloutsos)We will not discuss the mathematical backgrounds of dimension (Cite Edgar). We will not focus greatly on the algorithmic complexity.

\section{Dimension}

Scaling laws.

Box counting dimension, Correlation integral, Haussdorff & Packing dimension.

Box counting estimator, Correlation estimator, Takens estimator.

\section{The Takens estimator}

Like the Hill estimator. State assumptions for MLE. 

\subsection{Testing the independence assumption}

If we use each point for only one pair, (and our dataset is iid) then the distances for those pairs will be iid samples from $p_D$. If we reuse points, we will invalidate the first assumption. Consider for instance, a dataset of three points $a$, $b$ and $c$ drawn independently from some distribution. If we use only $d(a, b)$ we get a single sample from $p_D$. If we use $d(a, b)$ and $d(a, c)$, the two values are dependent the location of $a$. we are in fact sampling twice independently from the distribution of $d(a, X)$ rather than $d(X, Y)$.
 
If we use all pairs from a large dataset, however, the recurrence of single points will simply mean that we are sampling from $d(a, Y)$ averaged over many different $a$. Thus we expect that for a large dataset, the distances of all pairs do not form a substantially different distribution than the distances over successive pairs, while giving us a squared increase in the volume of data.

We test this assumption by taking datasets in full (after shuffling the data) and performing the Kolmogorov-Smirnov statistic on them. The results are shown in table @!.

\subsection{Testing the distribution assumption}



\subsection{Physics}
\subsection{Statistics and Machine Learning}

\section{Estimating dimension}

\subsection{Maximum Likelihood Estimator}

Derivation of Takens' MLE and its standard error. Comparison to the Hill estimator and discussion of the differences.
 
\subsection{Estimating the $d_Max$ parameter}

Quick review of the methods described in Clauset. Translation of the method to this estimator. Experiment showing the validity of the method for this MLE.

\subsection{Uncertainty}

Quick review of the approach of Clauset.

\subsection{Significance}

Review of the approach of Clauset, and a description of the method translated to this problem.

\subsection{Metric data}

Discussion of the use of dimension for metric data and the use of the significance measure.

\subsection{Runnning time}

Discussion of the running time an an investigation of the effects of reducing the data size.

\section{A multiscale estimator}

Consider a datset produces by drawing random points in some 1D curve in 2D Euclidean space, but with a small term of 2D Gaussian noise added. The theoretical dimension of this process, for any definition of dimension will be 2, since the limit towards the smallest scales determines the dimension. However, the shape that we are interested in (the distribution without the noise term) has dimension 1. 

This is an example of a distribution with multiple intrinsic dimensions. The data is generated by different processes that each create a dimensionality in the data at a particular scale. It's not uncommon to see data with more than two intrinsic dimensions. In the log-log plot produces by the Grassberger-Procaccia estimator, this behavior will produce linear segments for the relevant length scales.

From a natural science perspective, finding these length scales in data can provide great insight. They hint at clearly separable processes producing the data, each with their own intrinsic length scales. However as noted before, the Grassberger-Procaccia estimator relies on subjective judgement to find these scales. Here, we adapt our method for single-sclae dimension measurement to multi-scale measurements.

To start with we assume that our data has some number of intrinsic scales determined by $\{r_i\} \in {\mathbb R}^+$ and $r_0 = 0$ so that we have an intrinsic dimension $D_t^i$ over distances in the half-open interval $[r_{i-1}, r_{i})$. We let the maximum $r_i$ be less than the maximum distance in the dataset, so that as with the single-scale estimator, our distance distributin has a tail that is not covered by the dimensional scaling (and about which we make no assumptions).

We define the correlation integral over an interval simply as $C(r_{i-1}, r_i) = C(r_i) - C(r_{i-1})$. Our assumption that the dimension $D_t^i$ determines scaling in this interval tells us that 
\[
p(d \leq r | d \in [r_{i-1}, r_i)) = \frac{p(d \leq r | d \in [r_{i-1}, r_i))}{p(d \d \in [r_{i-1}, r_i)])} = \frac{p(d \in [r_{i-1}, r])}{p(d \in [r_{i_1}, r_i))}
\]
\[
 = \frac{C(r) - C(r_{i-1})}{C(r_i) - C(r_{i-1})} = \frac{cr^{D^i_t} - cr_{i-1}^{D^i_t}}{cr^{D^i_t} - cr{i-1}^{D^i_t}}\
\]

We turn this into a probability function by taking the derivative over $r$, and we use this to compute the score (the derivative of the log likelihood of the data). Setting the score to zero gives us a maximum likelihood estimator for $D^t_i$. This process is detailed in appendix @!. Unlike the single scale version, we do not find a closed form version of the MLE. We are left with:
\[
\frac{1}{D^t_i} - \frac{\ln \frac{r_i}{r_{i-1}}}{(\frac{r_i}{r_{i-1}})^{D^t_i}} - \frac{\Sum_{r \in R_i}\ln r - |R_i| \ln r_{i-1}}{|R_i|} = 0
\]

Which is unlikely to be expressible in $D^t_1$. However, as proved in the appendix, the function is strictly decreasing in $D^i_t$, for the required values of the constants, so that we can use a simple binary search to pinpoint its root. Call the the left hand side of the equation above $f(x)$. Then the search algorithm works as follows


\begin{pseudo}[h]
	\textbf{starting with $x = 1$ repeat $x \leftarrow 2x$ until $f(x)$ becomes negative}  \\
	\hspace*{5mm}\textit{\# This gives us an upper bound for $f(x) = 0$}\\
	\textbf{return find}$(0, x)$ \\
	
	\textbf{function find}$(x_{\min}, x_{\max})$ \\
	\hspace*{5mm} $x = (x_{\max} + x_{\min})/2$ \\
	\hspace*{5mm} \textbf{if} $x_{\max} - x_{\min} < \epsilon$ \\
	\hspace*{10mm} \textbf{return} $x$\\
	\hspace*{5mm} \textbf{if} $f(x) < 0$
	\textbf{return} $\mbox{find}(x_{\min}, x)$ \\ 
	\hspace*{5mm} \textbf{else} 
 	\textbf{return} $\mbox{find}(x, x_{\max})$ \\	
\end{pseudo}

This algorithm first searches for an $x$ that makes $f(x)$ negative, and then bisects theinterval $[0, x]$ recursively until the interval is smaller tha a given $\epsilon$, at which point the middle of the interval is taken as the estimate of $D_i^t$.

To find the interval limits $r_i$ we proced as we did before with $d_{\max}$ and search for a set of values from the data which minimize the Kolmogorov-Smirnov statistic with the filtered data. We compare each interval with the data filtered to that interval and take the sum of all intervals for a given set as the value to minimize. As searching a for a single value was already expensive in the single-scale case, here we must give up on the idea of checking all possible values and instead search for a suitable set of ranges.  

@!NOTE: Try contiguous and non-contiguous scaling intervals. 


\section{Experiments}
\subsection{Data sets}

\pagebreak
\begin{landscape}

\begin{tabular}{l | r r r r}
\hline
data set & size & dimension & significance & true dimension \\
\hline
Swiss roll & 1000 & & & \\
Noisy swiss roll ! & 1000 & & & \\ 
Sierpinski & 1000 & & & \\
Cantor & & & & \\
Menger Sponge ! & & & & \\
Persistent process ! & & & & \\
  
MVN 50 & 1000 & & & \\ 
MVN 500 & 1000 & & & \\
 
British coast line & & & & \\
   
EEG & & & & \\
JAFFE expression database & & & & \\
Currency (Traina/Faloutsos) ! & & & & \\
Galaxies & & & & \\
DNA random walk & & & & \\
Turning cup & & & & \\ 
Faces & & & & \\ 

Newsgroup posts (edit distance, possibly normalized by length) & & & & \\
Protein sequences (mPAM metric) & & & & \\
Graphs (Weisfeiler Lehman) & & & & \\
\hline
\end{tabular}

\end{landscape}
\pagebreak

\subsection{Results}

\section{Conclusion}

We have presented a straightforward, objective method for calculating a maximum likelihood estimate of the intrinsic dimension of a dataset. We have com 

\section{Appendix}

\subsection{Derivation of the MLE}

We start with the cumulative distribution function below $d_{\max}$:
\[
p(d \leq r | d \leq r_{\max}) = \frac{p(d \leq r)}{p(d \leq r_{\max})} = \frac{C(r)}{C(r_{\max})} = \frac{c \cdot r^{D_t}}{c \cdot {r_{\max}} ^ {D_t}} = \left ( \frac{r}{r_{\max}} \right )^{D_t}
\]

We take the derivative (over $r$) to get the probability density function:

\[
p(d = r | d \leq r_{\max}) = D_t \frac{r^{D_t-1}}{{r_{\max}}^D_t} = D_t \frac{1}{r} \left (\frac{r}{r_{\max}} \right ) ^ {D_t} 
\]

Let $R$ be the set of distances in the dataset less than $r_{\max}$. The the likelihood of this dataset is
\[
{\cal L}(R|D_t) = \prod_{r \in R} D_t \frac{1}{r} \left (\frac{r}{r_{\max}} \right ) ^ {D_t} 
\] 
Which we must maximize. Maximizing the logarithm of the likelihood is equivalent, and easier.
\begin{align*}
\ln {\cal L}(R|D_t) &= -\sum \ln r + D_t \sum \ln r - D_t |R| \ln r_{\max} + |R| \ln D_t
\end{align*}

To maximize, we take the derivate with respect to $D_t$

\begin{align*}
\frac{\partial \ln{\cal L}(R|D_t)}{\partial D_t} = \sum{\ln r} - |R|\ln r_{\max} + |R|\frac{1}{D_t}
\end{align*}

and set it equal to zero and solve for $D_t$

\begin{align*}
 \sum{\ln r} - |R|\ln r_{\max} + |R|\frac{1}{D_t} &= 0 \\
|R|\frac{1}{D_t} &= - \sum \ln r + |R| \ln r_{\max} \\
D_t &= \frac{|R|}{- \sum \ln r + |R| \ln r_{\max}} 
\end{align*}

Many publications assume that the data $|R|$ are normalized by $r_{\max}$ which allows the cleaner result:
\begin{align*}
D_t &= \frac{|R|}{- \sum \ln r} 
\end{align*}

However, this hides the reliance on $r_{\max}$. As we will be iterating over all possible values for $r_{\max}$, it's easier to include in the estimator explicitly.

\subsection{Derivation of the multiscale MLE}

The only difference between the single- and the multiscale MLE is that the latter has a lower bound to its range in addition to an upper bound. Unfortunately this small difference causes the derivation of the MLE to become a great deal less elegant.

We start with the cumulative distribution function as described in the text. We use $r_{i-1} = r_j$ to clear up the notation.

\begin{align*}
p(d \leq r | d \in [r_{i-1}, r_i)) = \frac{r^{D_i} - {r_{j}}^{D_i}}{{r_i}^{D_i} - {r_{j}}^{D_i}} = \frac{\left(\frac{r}{r_j}\right)^{D_i} - 1}{\left(\frac{r_i}{r_j}\right)^{D_i} - 1}
\end{align*}
We take the derivative with respect to $r$ to get the probability density function.

\begin{align*}
p(d = r | \ldots) = {D_i}r^{D_i-1} \frac{1}{\left(\frac{r_i}{r_j}\right)^{D_i} - 1}
\end{align*}

This gives us the following log likelihood function:
\begin{align*}
\ln{\cal L}(R_i|D_i) &= \sum \ln D_i + (D_i - 1)\sum \ln r - \sum \ln \left(\left(\frac{r_i}{r_j}\right)^{D_i} -1\right)\\
&=  |R_i|\ln D_i + (D_i - 1)\sum \ln r - |R_i| \ln \left(\left(\frac{r_i}{r_j}\right)^{D_i} -1\right) 
\end{align*}

Where $R_i$ is the subset of distances in the dataset that fall within the interval $[r_{i-1}, r_i)$. 

Taking the derivative:
\begin{align*}
\frac {\partial \ln{\cal L}(R_i|D_i)}{\partial D_i} &=  \frac{|R_i|}{D_i} + \sum \ln r - |R_i| \frac{\left(\frac{r_i}{r_j}\right)^{D_i} \ln \frac{r_i}{r_j}}{\left(\frac{r_i}{r_j}\right)^{D_i} -1}
\end{align*}

Setting to zero and solving for $D_t$

\begin{align*}
\frac{|R_i|}{D_i} + \sum \ln r - |R_i| \frac{\left(\frac{r_i}{r_j}\right)^{D_i} \ln \frac{r_i}{r_j}}{\left(\frac{r_i}{r_j}\right)^{D_i} -1} &= 0 \\
\frac{1}{D_i}  - \frac{\left(\frac{r_i}{r_j}\right)^{D_i} \ln \frac{r_i}{r_j}}{\left(\frac{r_i}{r_j}\right)^{D_i} -1} &= \frac{-\sum \ln r}{|R_i|}
\end{align*}

@! If we pre-normalize $R_i$ we may still get a closed form solution!

\subsection{Proof of monotonicity}

The following lemma tells us that the function describing the maximum likelihood estimate for the multiscale case is strictly decreasing over positive values, so that we can search for its root with a simple binary search.

\begin{lma}
The function
\[
f(x) = \frac{1}{x} + \frac{\ln(a)}{a^x - 1} + c
\]
is strictly decreasing for $ x \in [0, \infty)$ and $ a \in [0, 1]$ 
\end{lma}
\subsection{Algorithms}

\end{document}

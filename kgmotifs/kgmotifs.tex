\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\N}{{\mathbb N}}

\title{Detecting Network Motifs in Knowledge Graphs using Compression}

\begin{document}

\maketitle

\begin{abstract}
We introduce a method to detect \emph{network motifs} in knowledge graphs. Network motifs are useful patterns or meaningful subunits of the graph that recur frequently. We introduce a scalable approach for detecting such motifs in large knowledge graphs, inspired by recent work for simple graphs, and show that the motifs returned reflect the basic structure of the graph. Specifically, we show that common motifs reflect graph patterns used in common queries, basic schematic units of the graph and meaningful functional subunits, for various knowledge graphs.
\end{abstract}

The Linked, Open Data cloud contains a wealth of knowledge graphs, and is growing quickly. At the time of writing the average knowledge graph contains overt 65 000 edges, with many datasets containing more than a million \footnotemark. For such large graphs, it can be difficult to see the forest for the trees: how is the graph structured at the lowest level? What kind of things can I ask of what types of objects? What are small recurring patterns that might represent a novel insight into the data?

\footnotetext{\url{http://stats.lod2.eu/stats}}

In the domain of plain (unlabelled graphs), \emph{network motifs} were recently introduced as a tool to provide users of large graphs insight into their data. Network motifs are small subgraphs whose frequency is unexpected with respect to a null model. That is, for a given graph dataset $g$ and small graph $m$, we count the frequency $F(m, g)$: how often $m$ occurs in $g$ as a subgraph. We define a probability distribution over graphs $p^\text{null}(g)$, and estimate the probability that a graph sampled from $p^\text{null}$ contains more instances of $m$ than observed in our data: $p^\text{null}(F(m, G) \geq F(m, g))$, where $G$ is a random variable representing a graph. If this probability is low (commonly, below 0.05), we consider $m$ a motif. \footnotemark

\footnotetext{This procedure has the structure of a hypothesis test, but it is important not to interpret it as statistical evidence for the meaningfulness of the motif. The only thing it proves (in a frequentist statistical sense) is that $p^\text{null}$ is not the true source of the data. This is usually not a surprise: we are rarely able to model all aspects of a realistic data-generating process in a single distribution. The $p$-values used in motif analysis should be interpreted strictly as \emph{heuristics}. See also \cite{bloem2017large}.}

In \cite{bloem2017large}, an alternative method is presented that uses compression as a heuristic for motif value: the better a motif compresses the data, the more likely it is to be meaningful. It is shown that this principle can be implemented in a similar sort of hypothesis test for a null model, allowing a comparable workflow to classical motif analysis. 
In this paper, we extend the compression-based motif analysis to Knowledge Graphs. For the purposes of this research we define Knowledge graphs as labeled multigraphs. Nodes are labeled with entity names, and links are labeled with relations. We extend the definition of a motif to that of a basic graph pattern: that is, a motif is a small graph with some of its nodes and links labeled. The motif occurs in the graph where the graph structure matches, and the labels. The unlabeled nodes and links are free to contain whatever label when the motif is mapped to the data.
To maintain the connection to graph pattern queries, we do not require the motifs to be\emph{induced} subgraphs: that is, when a motif is mapped to a graph, the graph can contain additional links that are not specified by the motif. 
We perform several experiments to show that our method returns meaningful subgraphs. First, we look at several graphs for which schema information is provided. That is, we know the basic properties available for each entity. We show that the motifs found by our method correspond well to the schema, and compare performance against several basic benchmarks. Second, we run motif analysis on several datasets for which a list of real SPARQL queries entered by users is available. From these, we extract the basic graph patterns, and show that (a) these correspond to good motifs using our criterion and (b) our method can return such motifs with good precision and recall.
All code and benchmark datasets used in the paper are available, under open licenses.

% TODO: Make illustration

\subsection{Related Work}

\subsection{Preliminaries}

\section{Method}

We will first describe our method under the assumption that a good null model $p^\text{null}$ is available, and that $- \log p^\text{null}(G)$ can be efficiently computed. In Section~\ref{section:null-model}, we explain which null model we use for our experiments, and how it is implemented.
We will start with some formal definitions of our basic ingredients. To help with notation, we make the simplifying assumption that all names (for entities and relations) have been mapped to the natural numbers in some arbitrary way.  We can then define a \textbf{knowledge graph} $G$ as a finite set of triples of integers: $G \subseteq \N \times\N \times \N$. If $(s, p, r) \in G$ with $s, p, r \in \N$, then the entities mapped to integers $s$ and $o$ are related by relation $r$.
This representation means that when we used codes for all integers to represent these triples, that those nodes and relations assigned to small natural numbers will receive smaller codes. In practice, however we will always remap these integers to other symbols based on their frequency in the data, so this mapping will not affect performance. We may also worry that the names contain structure \emph{internally} that could be used for compression (since the labels of knowledge graphs are usually URLs), as was shown to be true in \cite{}. For our purposes, however we would like our analysis to use only the graph structure itself, and not the internal structure, so this simplified definition is a good way to ensure that we are blind to the internal structure of the labels.

\subsection{Motif search}

\subsection{Relevance test}

\subsection{Motif code}



\section{Null model}

\label{section:null-model}


\section{Experiments}

\subsection{Schema reconstruction}

\subsection{Query induction}

\subsection{Scale}

To show the scale of our method, we show its performance on some very large graphs, culminating with adump of the complete LOD cloud, from \cite{}.

\subsection{Various}

% Note: only if there's time. If not, save for later.

In this section, we show some experiments that are not intended to substantiate any claims about our method, but rather to illustrate the variety of uses for succesfull motif detection. 

\subsubsection{Analogical reasoning}

\subsubsection{Visualization}

\subsubsection{Feature extraction}

\subsubsection{Node embedding}

\bibliography{kgmotifs}
\end{document}

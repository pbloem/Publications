\documentclass[letterpaper]{article} %DO NOT CHANGE THIS

\usepackage{pdflscape}
\usepackage{times}     % Required
\usepackage{helvet}    % Required
\usepackage{courier}   % Required
\usepackage{url}       % Required
\usepackage{graphicx}  % Required
\frenchspacing         % Required
\setlength{\pdfpagewidth}{8.5in}  % Required
\setlength{\pdfpageheight}{11in}  % Required
\usepackage{longtable}
\usepackage{makecell}


\setcounter{secnumdepth}{0}    

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{float}

\usepackage{easylist}
\usepackage{soul}

\newcommand{\N}{{\mathbb N}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Z}{{\mathbb Z}}

\newcommand{\X}{{\cal X}}
\newcommand{\B}{{\mathbb B}}
\newcommand{\G}{{\cal G}}
\newcommand{\cS}{{\cal S}}

\newcommand{\I}{{\cal I}}
\newcommand{\tab}{\hspace*{5mm}}
\newcommand{\from}{\leftarrow}

\renewcommand\cellalign{tl}

\floatstyle{ruled}
\newfloat{pseudo}{h}{lop}
\floatname{pseudo}{Algorithm}

\title{Supplement: Detecting Motifs in Knowledge Graphs using Compression}

\begin{document}
\maketitle 
%\section{The canonical isomorphism algorithm for patterns}
%
%During search, we want to re-order the nodes and links in an encountered pattern to some canonical ordering. This way, when we later return an isomorphic pattern, we recognize it, and will not need to compute its logfactor again. 
%The \emph{nauty} algorithm is the standard solution for findings such \emph{canonical isomorphs}. As we are sampling partially labeled, directed graphs, we have a lot of information that can speed up canonicalization. Since this step is usually the bottleneck in sampling, it's important to perform it as efficiently as possible. Below, we provide a very succinct description of how nauty works, when applied to our patterns.
%
%Let $V'_M$ and $R'_M$ represent the labels of the variable nodes and links in $M$. Pattern $M$ is an isomorphism of pattern $M'$ if there exist bijections $\sigma_v : V'_M \times V'M$ and $\sigma_r: \R'_M \times \R'_M$ such that applying these to the variable elements of $M$ yields $M'$ (under graph equivalence). The set of all patterns isomorphic to $G$ is the isomorphism class $[G]$. A function $C(P)$ is a canonicalization function if it produces for all any pattern $M$ in $[M]$ the same, canonical member of $M$.
%
%Note that a very simple canonicalization algorithm would be to pass the unlabeled graph to the Nauty algorithm, and use its canonical ordering on the labeled graph. However, the labels can make the canonicalization procedure significantly faster.
%
%The key principle behind the Nauty algorithm is the use of a \emph{colouring}: a partition of the nodes of a graph. 
% A \emph{safe colouring} is defined as a partition $\pi$ of the nodes of the graph such that any permutation that maps nodes from different partitions to one another is guaranteed \emph{not} to be an automorphism. (Two nodes with the same node cannot be mapped to one another to create an automorphism.) No guarantees are made about nodes in the same cell. For instance, the partition that puts all nodes in the same cell is always safe. If two nodes have different degrees, we can put them in different cells and be sure that we have a safe colouring. Given one safe colouring $\pi$, if node $a$ from partition $i$ has a neighbor in partition $j$ and node $b$ from partition $j$ has no such node, we can \emph{refine} the partition $\pi$ by separating $i$ into a new partition (one contain $a$ and one containing $b$). 
%
%The Nauty algorithm operates on a graph with a given colouring (with the unit colouring used by default). It first refines the colouring as much as possible, using degree information, and propagating the distinguishing features through the graph. Once the most refined colouring has been achieved, it searches through all autorphisms of the graph to produce a canonical example (using various optimizations to prune the search tree).
%
%To adapt Nauty to our situation we make two major modifications. Firstly, we have node labels as well as edge labels. We can permute both the node variables and the edge variables to create an automorphism of a given BGP. Secondly, we have much more information with which to create the basic coloring (which Nauty then refines). We can treat labeled nodes as node in a single cellin their partition. This ensures not only that they are not mapped to other nodes by any automorphism, but also that their information propagates to the neighboring nodes during refinement. 

\tiny
\subsection{Dogfood, top 100 by log factor}

\begin{longtable}{ r r p{10cm} }
 log factor & frequency & \\
\hline\endhead
\input{results/dogfood/motifs-byscore.latex}
 \hline
\end{longtable}
\subsection{Dogfood, top 100 by frequency}

\begin{longtable}{ r r p{10cm} }
 log factor & frequency & \\
\hline\endhead
\input{results/dogfood/motifs-byfreq.latex}
 \hline
\end{longtable}

\subsection{AIFB, top 100 by log factor}

\begin{longtable}{ r r p{10cm} }
 log factor & frequency & \\
\hline\endhead
\input{results/aifb/motifs-byscore.latex}
\hline
\end{longtable}

\subsection{AIFB, top 100 by frequency}

\begin{longtable}{ r r p{10cm} }
 log factor & frequency & \\
\hline\endhead
 \input{results/aifb/motifs-byfreq.latex}
\hline
\end{longtable}

\subsection{Mutag, top 100 by log factor}

\begin{longtable}{ r r p{10cm} }
 log factor & frequency & \\
\hline\endhead
\input{results/mutag/motifs-byscore.latex}
\hline
\end{longtable}

\subsection{Mutag, top 100 by frequency}


\begin{longtable}{ r r p{10cm} }
 log factor & frequency & \\
\hline\endhead
 \input{results/mutag/motifs-byfreq.latex} 
\hline
\end{longtable}

\end{document}
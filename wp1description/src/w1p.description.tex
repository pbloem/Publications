\documentclass{article}

\usepackage{charter}

\title{Work Package 1: Description}
\date{\today}
\author{Peter Bloem}

% non-indented, spaced paragraphs
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

\begin{document}

\maketitle

\begin{abstract}
\noindent A concise, non-formal description of the aims of Work~Package~1 of
Project~23 (Data2Semantics) of the {\sc commit} project.
\end{abstract}

This project will ask the following questions:
\begin{itemize}
  \item Can we say, without relying on context, that some datasets are more interesting than others?
  \item Can we describe the notion of a context in the broadest possible terms and define the interestingness or utility of a dataset given such a description?
  \item What happens to the scientific process as we move into an era of
  increasingly large datasets? Will all scientific problems be solved simply by
  fast enough computer programs and large enough datasets, or will we find new
  limits that will defeat supercomputers and petabyte-sized datasets?
\end{itemize}

The first two questions serve to create a framework to pose and answer the third in a rigorous manner. Within context of Project~23, the first two questions are most likely to produce techniques that may benefit the other work packages, while the third question relates more to the enterprise of e-science as a whole.

\section*{Framework}

Consider what we might mean by a `dataset'. A large body of annotated language. An medical scan of a patient's brain. Photographs of the Taj Mahal. A collection of chess positions which end in a draw. To be able to talk about data in the most general sense, we suggest that anything we might call data can be translated to a single string of ones and zeroes. If it can't be, then it cannot be stored on a computer. We then build our theoretical constructs on only this assumption about the structure of the data. That way we can be sure that dataset will be subject to our conclusions.

Any process, from simple operations like sorting a list of numbers, to learning natural languages or discovering scientific knowledge, can be thought of as a procedure on such a string of ones and zeroes. Any procedure that can be described unambiguously and performed independently of any human interpretation or ill-understood processes is called \emph{effective}. It is a deep and open question whether processes that cannot be described by an effective procedure can exist at all. It is less controversial by far to say that science deals only with effective procedures.

Thus, we will investigate effective procedures on strings of ones and zeroes. This field of mathematics is called Computer Science, and it has been thoroughly studied for over a century. Anything that holds true in this framework of an effective procedure on a string of ones and zeroes will be true for all scientific process.

\pagebreak[0]

\section*{{\sc Question 1}: Complexity and Interestingness}

Consider the  following two datasets:
\begin{center}
\indent101010101010101010101010101010101010101010101010101\ldots

\indent101000110111111001110001011111010000001000101000110\ldots

\end{center}
These examples represent extremes on a spectrum of interestingness. They are
sometimes called \emph{the clock} and \emph{the coinflip}. The first (the clock)
shows a high degree of regularity. It's perfectly predictable. The second
(the coinflip) shows the highest possible degree of `chaos'. It's perfectly
unpredictable.

There are many well defined notions of complexity. These measures tend to
consider the coinflip maximally complex and the clock maximally simple. While 
this certainly captures, for instance, the effort required to describe the data,
intuitively, it leaves something to be desired.

Consider an old-fashioned black-and-white television. If we leave the
television off, the screen remains black. This is analogous to the clock above.
If we don't tune the receiver to a specific channel, we get a uniform static. By
most notions, he most complex signal the television is capable of displaying. It
is also commonly said that this signal contains the highest possible level of
information. That is were our intuition starts to confict with the theory.

The static may be complex, or complicated, it is of no use to us. We are
much more interested in the television programs we see when we tune the
television to a specific frequency. These are not entirely predictable and not
entirely unpredictable. We tend to be interested in the phenomena that lie somewhere
between the clock and the coinflip. Maximal information isn't enough, there must
be such a thing as `useful information'.

Intuitively, we guess that any datasets near the extremes will never be
interesting to anyone. How far can we push this? Can we narrow the band of
strings that are likely to be interesting? Can we suggest an optimum of a priori
interestingness somewhere between the clock and the coinflip? How far can we
take this without defining any kind of a context against which to measure
interestingness.

\section*{Question 2: Conditional Complexity}

A priori interestingness can only go so far. A TIFF image of the Mona Lisa
may have a high a priori interestingness, but it's unlikely to to be of any use
to a theoretical physicist. 

Determining interestingness on the basis of a context is easy to do in domain
specific terms. Given a well-formulated research question in molecular biology,
it should be simple enough to evaluate the utility of any dataset in answering
the research question, provided that the dataset is written in the same
`language' as the question. If we take the notion of `well-formed' questions to
its logical extreme, we end up with a computer program to be run on a dataset. A
query.

Can we generalize again? Ignore domain specific coding and programming and
simply consider the landscape of all datasets and all queries? The notions of
\emph{data} and \emph{computer program} have been thoroughly generalized. The
concept of a question, or a query has (to our knowledge) not yet received the
same treatment.

Consider a simple question like ``When does the supermarket close''. This question defines a hierachy of three classes. The class of \emph{replies}, which is simply anything that can be said in response. A subset of that is the class of \emph{answers}, which is anything that fits the question. A subset of answers is the set of \emph{correct answers}.

An reply like ``blue'' can be rejected without any verification based on data. I's a reply, but it's not an answer. The reply ``three o'clock'' is an an answer. It satisfies the expectations defined by the question. To check whether or not it's the correct answer, we need consult the data (ie. the table of closing times on the store's website). Depending on the dataset, the correct answer may be different.

*** Comments:
I don't know if this will pan out, but I can see connections to two-part coding, with the query serving the part of the model (defining a set of answers), and the correct answer serving the part of the data given the model. 

Either way, I think the possiblity of `conditional interestingness' is very relevant, and the connection to queries on data ties it in to the original work package description.
***
\section*{Question 3: Convergence}

Consider a linguist trying to build a model of the English language. This model (a grammar) is basically an effective preocedure for determining whether sentences are grammatically correct. The linguist finds that a simple grammar, taking up 1 kbit on his laptop, will capture about half of the sentences he finds on the internet. He wishes to increase this coverage, so he keeps working on the grammar. By the time it captures 3/4 of sentences correctly, its size has grown to 10 kBit. The linguist finds that to capture an additional 1/8th, the model size needs to increase to 100 kBit. the next 1/16 of sentences requires another ten-fold increase in complexity.

At this point the linguist will likely lose his enthusiasm for the project. It has long been a property of scientific research that models converge. Physics is a prime example. Newtons laws were built on a minimum of data and survived for centuries of additional experimentation. When it did finally show its weaknesses, The Theory of Relativity was discovered and survived the explosion of data-volume that the invention of the computer brought us. 

If the linguist were as lucky as the physicist, his 1 kB grammar would have been more than enough to capture all the language he could record. And if it did eventually fail after he had collected ten times as many sentences as he had started with, a new model of 2 kB would suffice to model the new data as well.

It would seem that not all sciences are created equal. While the linguist struggles with his growing grammars, the biologist next door must come to terms with the fact that when he doubles his sample size, he will see twice as many diseases. For some fields of science, increasing the amount of data will only increase the number of questions. And while the computer revolution has so far given us exponentially increasing power, we are fast approaching the physical limits of computation. 

At some point the computational power required to infer some answer from a dataset will simply not be sustainable in our universe. We hope to build a framework to formalize such statements, and possibly estimate where various datasets fall in the landscape of convergent and non-convergent models.

\subsection*{Power Laws}

Question 3 relates to 1 and 2, mostly through the notion of a \emph{power law}. Power laws relate the probability that some event will occur, to some property of that event. For instance, there is a power law relating the probability of an earthquake to its size. A large magnitude earthquake is more rare than a small magnitude earthquake, and the relation between probability and magnitude follows roughly the following relation 
\[
p = m^{-\alpha}
\] 
where $\alpha$ is the \emph{exponent}, the most important parameter of the distribution. Other phenomena which follow a power law distribution are the size of an individual's wealth, the population size of cities and the size of craters on the moon.

Intuitively, the example of the linguist building progressively more complex grammars calls to mind a power law. The relation here is between probability of seeing a sentence, and the complexity of the model required to capture this sentence. 

If we can formalize this picture, we can relate probability of events to required model complexity, and make suggestions for which fields will benefit from increased data acquisition, and which will will need to manage their expectations. 

\end{document}
